#!/usr/bin/env python

from http.server import BaseHTTPRequestHandler, HTTPServer
from html.parser import HTMLParser
import argparse
import base64
import colorsys
import collections
import datetime
import gzip
import json
import logging
import os
import random
import re
import shutil
import subprocess
import threading
import time
import urllib.error, urllib.parse, urllib.request
import webbrowser

# base
# TODO no-r mode
# TODO help page on how to get cookies.txt
# TODO html extractor: get innerHTML? (do ranking descriptions contain html tags??)
# TODO other websites? (and rename to ja-novels.py)
# TODO chara name db in separate files (one file per each series, and embed on build)
# TODO url like /search/abc/6?compact=... (for better history integration)

# as a cli util
# TODO json api (like &json=1, for cli, also for test automation)


### Configuration

save     = False  # Save visited novels?
color    = False  # colorize character names?
verbose  = False  # Show http logs?
cachedir = "NONE" # Cache directory (NONE to disable)

emoji = { "love": "üíô", "search": "üîç" }


### HTTP Server

def run_threaded_https_server(RequestHandlerClass, host="0.0.0.0", port=8030, https=False, certfile="", keyfile=""):
    """Run http server with threading and ssl support.
if `https` is true, (`certfile`, `keyfile`) is passed to load_cert_chain.
Use `openssl req -new -x509 -keyout CERTFILE -out KEYFILE -days 365 -nodes`."""
    import http.server, ssl
    def ssl_wrap(httpd, certfile, keyfile):
        # https://gist.github.com/DannyHinshaw/a3ac5991d66a2fe6d97a569c6cdac534
        ctx = ssl.SSLContext(protocol=ssl.PROTOCOL_TLS_SERVER)
        ctx.load_cert_chain(certfile=certfile, keyfile=keyfile)
        httpd.socket = ctx.wrap_socket(httpd.socket, server_side=True)
    httpd = http.server.ThreadingHTTPServer((host, port), RequestHandlerClass)
    if https:
        ssl_wrap(httpd, certfile, keyfile)
    httpd.serve_forever()

class MyRequestHandler(BaseHTTPRequestHandler):
    def log_message(self, format, *args):
        cli = self.client_address
        logging.debug(("%s:%s " + format) % (cli[0], cli[1], *args))

    def sendHTML(self, html):
        gz = "gzip" in (self.headers["Accept-Encoding"] or "")
        data = bytes(html, "utf-8")
        if gz: data = gzip.compress(data)
        self.send_response(200)
        self.send_header("Content-type", "text/html")
        if gz: self.send_header("Content-Encoding", "gzip")
        self.send_header("Content-Length", str(len(data)))
        self.end_headers()
        try:
            self.wfile.write(data)
        except BrokenPipeError:
            logging.warning("BrokenPipeError")
            return

    def do_GET(self):

        parsed = urllib.parse.urlparse(self.path)
        paths  = [x for x in parsed.path.split("/") if x]
        param  = { k: v[0] for k, v in urllib.parse.parse_qs(parsed.query).items() }.get

        try:
            html = ""
            err = b"400 Bad Request"
            if len(paths) == 0:
                s = SearchRanking(param("mode", "daily"), param("date", ""))
                html = s.html(param("compact", 1))
            elif len(paths) == 1:
                if paths[0] == "ranking":
                    s = SearchRanking(param("mode", "daily"), param("date", ""))
                    html = s.html(param("compact", 1))
                elif paths[0] == "search":
                    s = Search(param("q", ""), param("bookmarks", 0), param("page", "1"), param("npages", 1))
                    html = s.html(param("compact", 1))
                elif paths[0] == "user":
                    s = SearchUser(param("id", ""), param("bookmarks", 0))
                    html = s.html(param("compact", 1))
                elif paths[0] == "novel":
                    if not (novelID := param("id")):
                        err = b"400 Bad Request: missing id"
                    else:
                        f = Fetch(novelID)
                        if save:
                            f.save()
                        html = f.html()
            if html:
                self.sendHTML(html)
            else:
                self.send_response(400)
                self.send_header("Content-Length", str(len(err)))
                self.end_headers()
                self.wfile.write(err)
        except Exception as e:
            logging.error("Error occured\n" + str(e))


### Search

def searchEntry(title, tags, id, description, xRestrict, bookmarkCount, textCount):
    # Abstracts (1) response json format and (2) difference between websites
    return {
            "title"         : title,
            "tags"          : tags,
            "id"            : id,
            "description"   : description,
            "xRestrict"     : xRestrict,
            "bookmarkCount" : bookmarkCount,
            "textCount"     : textCount,
            }

def html_searchBar(simple=True, query="", compact=False, bookmarks=0, npages=1):
    html = f"""<form style="text-align: right; line-height: 1.5" action=search>
<input type="text" name="q" placeholder="Ê§úÁ¥¢" value="{query}">
<input type="submit" value="{emoji['search']}">
<input type="hidden" name="compact" value="{1 if compact else ''}">
"""
    if not simple:
        html += f"""<br>
<select name="npages">
<option value="1"{" selected" if npages == 1 else ""}>1„Éö„Éº„Ç∏„Åö„Å§
<option value="2"{" selected" if npages == 2 else ""}>2„Éö„Éº„Ç∏„Åö„Å§
<option value="3"{" selected" if npages == 3 else ""}>3„Éö„Éº„Ç∏„Åö„Å§
</select>
{emoji['love']}:<input type="text" name="bookmarks" value="{bookmarks}" size="3">
"""
    html += "</form>"
    return html

class Search():

    # Usage: print(Search("abc",10,1,1).html())

    def __init__(self, query, bookmarkCount, page, npages):
        self._novels = []

        self._query = query
        self._bookmarkCount = int(bookmarkCount)
        self._page = int(page)
        self._npages = int(npages)

        self._doSearch()

    def _doSearch(self):
        dataList = self._getDataList()
        self._novels = [
                searchEntry(x["title"], x["tags"], x["id"], x["description"], x["xRestrict"], x["bookmarkCount"], x["textCount"])
                for x in dataList
                if int(x["bookmarkCount"]) >= self._bookmarkCount
                ]

    def _getDataList(self):
        self._wordEscaped = percentEncode(self._query).replace("+", "%20")
        dataList = []
        for i in range(self._npages):
            resJson = Download.Pixiv.jsonSearch(self._query, i+self._page)
            dataList += resJson["body"]["novel"]["data"]
        return dataList

    def html(self, compact):
        compact = compact == "1"

        # common css
        css = """body { max-width: 750px; margin: 1em auto; padding: 0 .5em; }
#main { margin: 1em 0 }
li p { margin: 1.5em 0 }
td:not(:nth-child(4)) { text-align: center; padding: 0 .35em }
td:nth-child(4)       { padding-left: 2em }"""

        # novels
        novels = "<table>" if compact else "<ul>"
        for x in self._novels:
            href = mkurl("novel", id=x['id'])
            if compact:
                novels += f"<tr><td><a href=\"{href}\">{x['id']}</a></td><td>{getRSign(x['xRestrict'])}</td><td>{x['bookmarkCount']}</td><td>{x['title']}</td></tr>\n"
            else:
                desc = "<br>".join(replaceLinks(x["description"]).split("<br />")[0:5])
                desc = addMissingCloseTags(desc, tags=["b", "s", "u", "strong"])
                tags = ", ".join([f"<a href=\"{mkurl('search', q=t)}\">{t}</a>" for t in x["tags"]])
                novels += f"<li>{x['title']} ({x['textCount']}Â≠ó) <a href=\"{href}\">[{x['id']}]</a><p>{desc}</p>{emoji['love']} {x['bookmarkCount']}<br>{tags}</li><hr>\n"
        novels += "</table>" if compact else "</ul>"

        # final html
        return f"""<!DOCTYPE html>
<html lang="ja">
<head>
{self._html_head()}
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
{css}
</style>
</head>
<body>
{self._html_header(compact)}
{self._html_nav(compact)}
<div id="main">
{novels}
</div>
{self._html_nav(compact)}
</body>
</html>"""

    def _html_head(self):
        return f"<title>Search {self._query}{(f' ({self._page})' if self._page > 1 else '')}</title>"

    def _html_header(self, compact):
        return f"""<h1>Search {self._query}{(f' ({self._page})' if self._page > 1 else '')}</h1>
{html_searchBar(simple=False, query=self._query, compact=compact, bookmarks=self._bookmarkCount, npages=self._npages)}
<hr>"""

    def _html_nav(self, compact):
        # navigation links (on both top and bottom of page)
        common = { "q": self._query, "npages": self._npages, "bookmarks": self._bookmarkCount }
        hrefPrev   = mkurl("search", **common, page=max(1,self._page-self._npages), compact=compact)
        hrefNext   = mkurl("search", **common, page=self._page+self._npages,        compact=compact)
        hrefToggle = mkurl("search", **common, page=self._page,                     compact=int(not compact))
        return f"""<div id="nav" style="display: flex">
<span style="flex: 1">
<a href="{hrefToggle}">{compact and "Ë©≥Á¥∞Ë°®Á§∫" or "„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫"}</a>
</span>
<span style="flex: 1"></span>
<span style="flex: 1; text-align: right">
<a href='{hrefPrev}'>Ââç„Å∏</a>
<a href='{hrefNext}'>Ê¨°„Å∏</a>
</span>
</div>"""

class SearchUser(Search):

    def __init__(self, userID, bookmarkCount):
        self._novels = []

        self._userID = userID
        self._bookmarkCount = int(bookmarkCount)

        self._doSearch()

    def _getDataList(self):
        dataList = []

        # First, response includes all novelIDs of user
        json1 = Download.Pixiv.jsonUserAll(self._userID)
        novelIDs = list(json1["body"]["novels"].keys())

        # Next, get data for each novel (100 novels at once)
        # So, 0 novel = 0 request, 1-100 novels = 1 request, 101-200 = 2 etc.
        dataList = []
        n = 100
        numRequests = 1 + int((len(novelIDs)-1)/n)
        for ids in [novelIDs[n*i:n*(i+1)] for i in range(numRequests)]:
            json2 = Download.Pixiv.jsonUserNovels(self._userID, ids)
            dataList += list(json2["body"]["works"].values())

        return dataList

    def _html_head(self):
        return f"<title>Search {self._userID}</title>"

    def _html_header(self, compact):
        return f"""<h1>Search {self._userID}</h1>
{html_searchBar(simple=True, query="", compact=compact)}
<hr>"""

    def _html_nav(self, compact):
        hrefToggle = mkurl("user", id=self._userID, compact=int(not compact))
        return f"<a href='{hrefToggle}'>{compact and 'Ë©≥Á¥∞Ë°®Á§∫' or '„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫'}</a>"

class SearchRanking(Search):

    _modeNames = {
            "daily":           "„Éá„Ç§„É™„Éº",
            "weekly":          "„Ç¶„Ç£„Éº„ÇØ„É™„Éº",
            "monthly":         "„Éû„É≥„Çπ„É™„Éº",
            "rookie":          "„É´„Éº„Ç≠„Éº",
            "weekly_original": "„Ç™„É™„Ç∏„Éä„É´",
            "male":            "Áî∑Â≠ê„Å´‰∫∫Ê∞ó",
            "female":          "Â•≥Â≠ê„Å´‰∫∫Ê∞ó",
            "daily_r18":       "„Éá„Ç§„É™„Éº R-18",
            "weekly_r18":      "„Ç¶„Ç£„Éº„ÇØ„É™„Éº R-18",
            "male_r18":        "Áî∑Â≠ê„Å´‰∫∫Ê∞ó R-18",
            "female_r18":      "Â•≥Â≠ê„Å´‰∫∫Ê∞ó R-18",
            }

    def __init__(self, mode, date):
        self._novels = []

        self._mode = mode

        # set self._date to a date object (at most yesterday)
        if re.match(r"\d\d\d\d-\d\d-\d\d", date):
            d = datetime.date.fromisoformat(date)
            y = yesterday()
            self._date = d if d <= y else y
        else:
            self._date = yesterday()

        self._doSearch()

    def _doSearch(self):
        dataList = withFileCache(
            f"pixiv-ranking-{self._mode}-{self._date.isoformat().replace('-', '')}",
            self._getDataList, 3600)
        self._novels = [
                searchEntry(x["title"], x["tags"], x["id"], x["description"], x["xRestrict"], x["bookmarkCount"], x["textCount"])
                for x in dataList
                ]

    def _getDataList(self):
        # return self._getDataListFromHTML("".join(open("../r_daily","r").readlines()))
        dataList = []
        for page in [1, 2]:
            res = Download.Pixiv.rankingPhp(self._mode, self._date.isoformat(), page)
            dataList += self._getDataListFromHTML(res)
        return dataList

    def _getDataListFromHTML(self, html):
        # searchEntry(title, tags, id, description, xRestrict, bookmarkCount, textCount):
        data = []
        current = None
        def done():
            nonlocal current, data
            if current:
                data += [searchEntry(**current)]
            current = {}
        def setp(prop, val):
            current[prop] = val
        def onStart(match, attr):
            if match("._ranking-item"):
                done()
                setp("xRestrict", "r18" in self._mode)
                setp("description", "") # some novels don't have description
            elif match(".cover"):
                setp("title", re.sub(r"/.*", "", attr("alt")))
                setp("tags",  attr("data-tags").split())
                setp("id",    attr("data-id"))
        def onData(match, data):
            if match(".bookmark-count"):
                setp("bookmarkCount", re.sub(r"[^\d]", "", data))
            elif match(".chars"):
                setp("textCount", re.sub(r"[^\d]", "", data))
            elif match(".novel-caption"):
                setp("description", data)

        MyHTMLParser(onStart, onData).feed(html)
        done()
        # print(json.dumps([[x["title"]] for x in data], indent=2, sort_keys=True, ensure_ascii=False))
        return data

    def _html_head(self):
        return f"<title>{self._modeNames[self._mode]} „É©„É≥„Ç≠„É≥„Ç∞ {self._date}</title>"

    def _html_header(self, compact):
        # links for other rankings
        hrefBase = mkurl("ranking", compact=compact, date=self._date)
        modeLinks1 = "\n".join([f"""<a href="{hrefBase}&mode={mode}"{ ' class="ranking-selected"' if mode == self._mode else ""}>{self._modeNames[mode]}</a>""" for mode in self._modeNames.keys() if not "r18" in mode])
        if Download.Pixiv.hasCookie():
            modeLinks2 = "<span>R-18:</span>"
            modeLinks2 += "\n".join([f"""<a href="{hrefBase}&mode={mode}"{ ' class="ranking-selected"' if mode == self._mode else ""}>{self._modeNames[mode].replace(" R-18", "")}</a>""" for mode in self._modeNames.keys() if "r18" in mode])
        else:
            modeLinks2 = "R-18 „É©„É≥„Ç≠„É≥„Ç∞„ÇíË¶ã„Çã„Å´„ÅØ cookies.txt „ÅåÂøÖË¶Å„Åß„Åô„ÄÇ"
        modeLinks = """<style>
#ranking-modes { margin: .8em 0; font-size: small; line-height: 1.8 }
#ranking-modes a { margin-right: 1.4em }
#ranking-modes span { margin-right: 1.0em }
.ranking-selected { font-weight: bold }
</style>""" + f"""
<p id="ranking-modes">
{modeLinks1}
<br>
{modeLinks2}
</p>"""

        # construct html
        return f"""<h1>{self._modeNames[self._mode]} „É©„É≥„Ç≠„É≥„Ç∞ {self._date}</h1>
{html_searchBar(simple=True, query="", compact=compact)}
{modeLinks}
<hr>
<form style="text-align: center; margin: .5em 0" action=ranking>
<label for="date">Êó•‰ªò:</label>
<input type="date" id="date" name="date" value="{self._date}" max="{yesterday()}">
<input type="submit" value="{emoji['search']}">
<input type="hidden" name="mode" value="{self._mode}">
<input type="hidden" name="compact" value="{1 if compact else ''}">
</form>"""

    def _html_nav(self, compact):
        hrefToggle = mkurl("ranking", q=self._mode, compact=int(not compact), date=self._date)
        return f"<a href='{hrefToggle}'>{compact and 'Ë©≥Á¥∞Ë°®Á§∫' or '„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫'}</a>"

class MyHTMLParser(HTMLParser):
    # Usage: MyHTMLParser(onStart, onData).feed(html)
    # On each handle_starttag, onStart(match, attr) is called, where match(query) checks if the tag matches the CSS query, and attr(attrName) is like getAttribute(attrName)
    # On each handle_data, onData(match, data) is called, where match(query) is the same above, and data is the same as in handle_data
    def __init__(self, onStart, onData):
        super().__init__()
        self._stack = []
        self._onStart = onStart
        self._onData = onData
    def _attr(self, attr):
        if len(self._stack) == 0: return
        for (k, v) in self._stack[-1]["attrs"]:
            if k == attr:
                return v
    def _match(self, query):
        if not re.match(r"^\.[^\s.]*$", query):
            logging.error("Query must be like .CLASS")
            return
        return re.sub(r"\.", "", query) in (self._attr("class") or "").split()
    def handle_starttag(self, tag, attrs):
        self._stack.append({ "tag": tag, "attrs": attrs })
        self._onStart(self._match, self._attr)
        if tag in ["img"]:
            self._stack.pop()
    def handle_endtag(self, tag):
        self._stack.pop()
    def handle_data(self, data):
        self._onData(self._match, data)


### Fetch

def withFileCache(name, getDefault, expiry=600):
    # note: when cache is expired and getDefault fails, returns old cache
    # expiry is in seconds
    # getDefault should return json-serializable data
    if cachedir == "NONE":
        return getDefault()
    if not re.match(r"^[a-zA-Z0-9-._]*$", name):
        raise Exception("Invalid cache name", name)
    if not os.path.isdir(cachedir):
        os.mkdir(cachedir)
    file = cachedir + os.sep + name
    def updateCache():
        value = getDefault()
        with open(file, "w") as f:
            json.dump(value, f, ensure_ascii=False, separators=(",", ":"))
        return value
    def readCache():
        with open(file, "r") as f:
            return json.load(f)
    try:
        if datetime.datetime.now().timestamp() - os.stat(file).st_mtime > expiry:
            try:
                value = updateCache()
                logging.debug("cache updating " + name)
            except:
                value = readCache()
                logging.debug("cache failed updating, using old " + name)
        else:
            value = readCache()
            logging.debug("cache is used " + name)
    except FileNotFoundError:
        value = updateCache()
        logging.debug("cache new item " + name)
    return value

class Fetch():

    def __init__(self, novelID):
        self._novelID = novelID
        self._data = self._getData()
        self._html = None

    def _getData(self):
        data = withFileCache(
            f"pixiv-showPhp-{self._novelID}",
            lambda: self._extractData(Download.Pixiv.showPhp(self._novelID)),
            expiry=3*86400
        )
        return data

    def html(self):
        if self._html: return self._html

        ## Construct html

        data = self._data

        o_css = """body { max-width: 700px; margin: 1em auto; padding: 0 .5em; }
@media screen and (max-aspect-ratio: .75) and (max-width: 13cm) { /* Mobile */
    body { max-width: 100%; margin: 0.5em 0.5em }
}
#novel { line-height: 1.9; border-bottom: solid #888 1px; margin-bottom: 2em; padding-bottom: 3em; }
#data { display: none }"""

        o_rSign = getRSign(data["xRestrict"])
        o_title = data["title"]
        o_description = replaceLinks(data["description"])
        o_content = data["content"]
        for (regex, replace) in [
                (r"$", "<br>"),
                (r"\[newpage\]", "<hr>\n"),
                (r"\[chapter:(.*?)\]", "<h2>\\1</h2>\n"),
                (r"\[\[rb:(.*?)(>|&gt;)(.*?)\]\]", "<ruby>\\1<rt>\\3</rt></ruby>"),
                ]:
            o_content = re.sub(regex, replace, o_content, flags=re.MULTILINE)

        # uploadedimage
        def getUploadedImageTag(novelImageId):
            url = data["textEmbeddedImages"][novelImageId]["urls"]["original"]
            imgB64 = base64.b64encode(Download.Pixiv.uploadedImage(url)).decode("utf-8")
            return f"""<figure>
<a href="{url}">
<img src=\"data:image/png;base64,{imgB64}\" alt=\"[uploadedimage:{novelImageId}\" style=\"width: 100%\">
</a>
</figure>"""
        o_content = re.sub(r"\[uploadedimage:([0-9]*)(.*?)\]", lambda m: getUploadedImageTag(m.group(1)), o_content)

        # pixivimage (may need cookie to retrieve images)
        def getArtworkImageTag(imageId, subIndex):
            url      = f"https://www.pixiv.net/artworks/{imageId}"
            json1    = Download.Pixiv.artworkPagesJson(imageId)
            imageUrl = json1["body"][int(subIndex or 1)-1]["urls"]["original"]
            imgB64   = base64.b64encode(Download.Pixiv.artworkImage(imageUrl)).decode("utf-8")
            return f"""<figure>
<a href="{url}">
<img src=\"data:image/png;base64,{imgB64}\" alt=\"[pixivimage:{imageId}-{subIndex}\" style=\"width: 100%\">
</a>
</figure>"""
        o_content = re.sub(r"\[pixivimage:([0-9]*)(-([0-9]*))?\]", lambda m: getArtworkImageTag(m.group(1), m.group(3)), o_content)

        # colorize character names
        if color:
            o_content = CharaColor.colorHTML(o_content)

        # embed json
        # e.g. <div data='{"x":10,"y":"„ÅÇ"}'></div>
        # o_json = json.dumps(data, ensure_ascii=False, separators=(',', ':'))
        # for (fromStr, toStr) in [
        #         ("'", "&#39;"), # ("\"", "&quot;"), ("<", "&lt;"), (">", "&gt;")
        #         ]:
        #     o_json = o_json.replace(fromStr, toStr)
        o_json = ""

        o_tags = ",\n".join(map(lambda y: f"<a href='{mkurl('search', q=y)}'>{y}</a>", [x["tag"] for x in data["tags"]["tags"]]))
        o_info = f"""<p>
„Çø„Ç∞:
{o_tags}
</p>
<p>
<a href="https://www.pixiv.net/novel/show.php?id={data["id"]}">Pixiv„ÅßÈñã„Åè</a>
ID:{data["id"]}
U:<a href="{mkurl('user', id=data["userId"])}">{data["userId"]}</a>
B:{data["bookmarkCount"]}
D:{data["createDate"][:10]}
</p>"""

        self._html = f"""<!DOCTYPE html>
<html lang="ja">
<head>
<title>{o_rSign}{o_title}</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<!--<link rel="stylesheet" href="_style.css">-->
<style>
{o_css}
</style>
</head>
<body>
<h1>{o_title}</h1>
<div id="novel">
{o_content}
</div>
<div id="info">
<p>
{o_description}
</p>
{o_info}
</div>
<div id="data" data-novels='{o_json}'></div>
</body>
</html>"""

        return self._html

    def save(self): # Save to file (will return filename)
        def trunc(s, lenBytes, suffix=""):
            # Tuncate s so that (s+suffix).encode("utf-8") has at most lenBytes bytes, and return s+suffix
            # Will not chop in the middle of byte-sequence representing single unicode character
            lenS       = len(s)
            lenSU      = len(s.encode("utf-8"))
            lenSuffixU = len(suffix.encode("utf-8"))
            i          = lenS
            while len(s[:i].encode("utf-8")) + lenSuffixU > lenBytes:
                i -= 1
            return s[:i] + suffix
        title = self._data["title"].replace("/", " ").replace('"', " ")
        outPrefix = f"{getRSign(self._data['xRestrict'])}"
        outSuffix = f" - pixiv - {self._data['id']}.html"
        outfile = trunc(outPrefix + title, os.pathconf('/', 'PC_NAME_MAX'), outSuffix)
        open(outfile, "w").write(self.html())
        return outfile

    def _extractData(self, html): # parse show.php and get json inside meta[name=preload-data]
        # Extract json string
        s = None
        def onStart(match, attr):
            nonlocal s
            if attr("name") == "preload-data":
                s = attr("content")
        MyHTMLParser(onStart, lambda match, data: 0).feed(html)

        # Extract part of json
        json1 = json.loads(s)
        return json1["novel"][list(json1["novel"].keys())[0]]


### Download

class Download:

    # Resource downloading often breaks due to invalidated cookie, change in http request headers etc.
    # So we want unit-testable functions for each resource.

    class Pixiv:

        # 404 without headers
        _headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Pragma': 'no-cache',
            'Referer': 'https://www.pixiv.net/',
            'Cache-Control': 'no-cache'
        }

        # cookie = readCookiestxtAsHTTPCookieHeader("../cookies.txt", "pixiv.net")
        cookie = None

        @classmethod
        def hasCookie(cls):
            return bool(cls.cookie)

        @classmethod
        def cookieHeader(cls):
            return { "cookie": cls.cookie } if cls.hasCookie() else {}

        @classmethod
        def showPhp(cls, novelID):
            url = f"https://www.pixiv.net/novel/show.php?id={novelID}"
            return myRequest(url, headers=cls._headers, headers2=cls.cookieHeader())

        @classmethod
        def rankingPhp(cls, mode, date: str, page: int):
            # Check modes
            MODES = { "daily", "weekly", "monthly", "rookie", "weekly_original", "male", "female", "daily_r18", "weekly_r18", "male_r18", "female_r18", }
            if not mode in MODES:
                raise Exception(f"Download.Pixiv.rankingPhp: unknown mode {mode} (expected one of [ {', '.join(MODES)} ]")
            if (not cls.hasCookie()) and mode.endswith("r18"):
                raise Exception(f"Download.Pixiv.rankingPhp: cookie is needed to view ranking of mode {mode}")
            # Check date
            if not (isinstance(date, str) and re.match(r"\d\d\d\d-\d\d-\d\d", date)):
                raise Exception(f"Download.Pixiv.rankingPhp: invalid date {date} (should be YYYY-MM-DD as a str)")
            # Make URL
            url = f"https://www.pixiv.net/novel/ranking.php?mode={mode}&date={date.replace('-', '')}"
            if page > 1:
                url += f"&page={page}"
            # Download
            return myRequest(url, headers=cls._headers, headers2=(cls.cookieHeader() if mode.endswith("r18") else {}))

        @classmethod
        def jsonUserAll(cls, userID):
            url = f"https://www.pixiv.net/ajax/user/{userID}/profile/all?lang=ja"
            return myRequest(url, fmt="json", headers=cls._headers, headers2=cls.cookieHeader())

        @classmethod
        def jsonUserNovels(cls, userID, novelIDs):
            n = 100
            if len(novelIDs) <= 0:
                raise Exception(f"Download.Pixiv.apiUserIds: at least 1 novel IDs are required")
            if len(novelIDs) > n:
                raise Exception(f"Download.Pixiv.apiUserIds: at most {n} novel IDs are allowed to query at once; got {len(novelIDs)}")
            idsParams = "&".join([f"ids[]={x}" for x in novelIDs])
            url = f"https://www.pixiv.net/ajax/user/{userID}/profile/novels?{idsParams}"
            return myRequest(url, fmt="json", headers=cls._headers, headers2=cls.cookieHeader())

        @classmethod
        def jsonSearch(cls, word, page):
            wordEscaped = percentEncode(word).replace("+", "%20")
            params = f"?word={wordEscaped}&order=date_d&mode=all&p={page}&s_mode=s_tag&lang=ja"
            url = f"https://www.pixiv.net/ajax/search/novels/{wordEscaped}{params}"
            return myRequest(url, fmt="json", headers=cls._headers, headers2=cls.cookieHeader())

        @classmethod
        def artworkPagesJson(cls, id):
            url = f"https://www.pixiv.net/ajax/illust/{id}/pages?lang=ja"
            try:
                return myRequest(url, fmt="json", headers=cls._headers, headers2=cls.cookieHeader(), headers3={ "Accept": "application/json" })
            except urllib.error.HTTPError as e:
                if e.code == 404:
                    logging.warning("Download.Pixiv.artworkPagesJson: Artwork not found" + (", or cookie is required to view this artwork" if not cls.hasCookie() else "") + ":", e)
                else:
                    logging.error("Download.Pixiv.artworkPagesJson: Unknown error:", e)
                raise e

        @classmethod
        def artworkImage(cls, url):
            headers2 = { "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8" }
            return myRequest(url, fmt="raw", headers=cls._headers, headers2=headers2)

        @classmethod
        def uploadedImage(cls, url):
            headers2 = { "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8" }
            return myRequest(url, fmt="raw", headers=cls._headers)



### Character name colorizer

class CharaColor:
    _lightness = 0.52
    _saturation = 0.7

    _db0 = {
        # _db0[series][characterName] = color (format: "#xxxxxx" or "#xxx")
        # Idolmaster (https://imas-db.jp/misc/color.html) (imcomplete, and not very strict)
        "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Éü„É™„Ç™„É≥„É©„Ç§„Éñ": {
            "P": "#555555", "Ôº∞": "#555555",
            "Â§©Êµ∑ Êò•È¶ô": "#e22b30", "Â¶ÇÊúà ÂçÉÊó©": "#2743d2", "Ëê©Âéü Èõ™Ê≠©": "#d3dde9", "È´òÊßª „ÇÑ„Çà„ÅÑ": "#f39939", "ÁßãÊúà ÂæãÂ≠ê": "#01a860", "‰∏âÊµ¶ „ÅÇ„Åö„Åï": "#9238be", "Ê∞¥ÁÄ¨ ‰ºäÁπî": "#fd99e1", "ËèäÂú∞ Áúü": "#515558", "ÂèåÊµ∑ ‰∫úÁæé": "#ffe43f", "ÂèåÊµ∑ ÁúüÁæé": "#ffe43f", "Êòü‰∫ï ÁæéÂ∏å": "#b4e04b", "ÊàëÈÇ£Ë¶á Èüø": "#01adb9", "ÂõõÊù° Ë≤¥Èü≥": "#a6126a", "Èü≥ÁÑ° Â∞èÈ≥•": "#00ff00",
            "‰ºäÂêπ Áøº": "#fed552", " „Ç®„Éü„É™„Éº": "#554171", "Â§ßÁ•û Áí∞": "#ee762e", "Êò•Êó• Êú™Êù•": "#ea5b76", "Âåó‰∏ä È∫óËä±": "#6bb6b0", "ÂåóÊ≤¢ Âøó‰øù": "#afa690", "Êú®‰∏ã „Å≤„Å™„Åü": "#d1342c", "È´òÂùÇ Êµ∑Áæé": "#e9739b", "Ê°úÂÆà Ê≠åÁπî": "#274079", "‰ΩêÁ´π ÁæéÂ•àÂ≠ê": "#58a6dc", "ÁØ†ÂÆÆ ÂèØÊÜê": "#b63b40", "Â≥∂Âéü „Ç®„É¨„Éä": "#9bce92", " „Ç∏„É•„É™„Ç¢": "#d7385f", "ÁôΩÁü≥ Á¥¨": "#ebe1ff", "Âë®Èò≤ Ê°ÉÂ≠ê": "#efb864", "È´òÂ±± Á¥ó‰ª£Â≠ê": "#7f6575", "Áî∞‰∏≠ Áê¥Ëëâ": "#92cfbb", "Â§©Á©∫Ê©ã ÊúãËä±": "#bee3e3", "Âæ≥Â∑ù „Åæ„Å§„Çä": "#5abfb7", "ÊâÄ ÊÅµÁæé": "#454341", "Ë±äÂ∑ù È¢®Ëä±": "#7278a8", "‰∏≠Ë∞∑ ËÇ≤": "#f7e78e", "Ê∞∏Âêâ Êò¥": "#aeb49c", "‰∏ÉÂ∞æ ÁôæÂêàÂ≠ê": "#c7b83c", "‰∫åÈöéÂ†Ç ÂçÉÈ∂¥": "#f19557", "Èáé„ÄÖÂéü Ëåú": "#eb613f", "ÁÆ±Â¥é ÊòüÊ¢®Ëä±": "#ed90ba", "È¶¨Â†¥ „Åì„ÅÆ„Åø": "#f1becb", "Á¶èÁî∞ „ÅÆ„ÇäÂ≠ê": "#eceb70", "ËàûÊµú Ê≠©": "#e25a9b", "ÁúüÂ£Å ÁëûÂ∏å": "#99b7dc", "ÊùæÁî∞ ‰∫úÂà©Ê≤ô": "#b54461", "ÂÆÆÂ∞æ Áæé‰πü": "#d7a96b", "ÊúÄ‰∏ä ÈùôÈ¶ô": "#6495cf", "ÊúõÊúà ÊùèÂ•à": "#7e6ca8", "ÁôæÁÄ¨ ËéâÁ∑í": "#f19591", "Áü¢Âêπ ÂèØÂ•à": "#f5ad3b", "Ê®™Â±± Â•àÁ∑í": "#788bc5", " „É≠„Ç≥": "#fff03c",
            "ÈùíÁæΩ ÁæéÂí≤": "#57c7c4",
            " Áé≤Èü≥": "#512aa3", " Ë©©Ëä±": "#e6f9e5", "Â••Á©∫ ÂøÉÁôΩ": "#fefad4",
            "Êó•È´ò ÊÑõ": "#e85786", "Ê∞¥Ë∞∑ ÁµµÁêÜ": "#00adb9", "ÁßãÊúà Ê∂º": "#b2d468",
        },
        "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Ç∑„É≥„Éá„É¨„É©„Ç¨„Éº„É´„Ç∫": {
            "P": "#555555", "Ôº∞": "#555555",
            " „Ç¢„Éä„Çπ„Çø„Ç∑„Ç¢": "#b1c9e8", "ÂèäÂ∑ù Èõ´": "#f4f9ff", "Â§ßÊßª ÂîØ": "#f6be00", "‰πôÂÄâ ÊÇ†Ë≤¥": "#e3bec3", "ÂñúÂ§öË¶ã Êüö": "#f0ec74", "Ê°êÁîü „Å§„Åã„Åï": "#ab4ec6", "Â∞èÊó•Âêë ÁæéÁ©Ç": "#db3fb1", "È´òÊ£Æ ËóçÂ≠ê": "#ceea80", "ÈÅìÊòéÂØ∫ Ê≠åÈà¥": "#d22730", " „Éä„Çø„Éº„É™„Ç¢": "#f5633a", "Èõ£Ê≥¢ Á¨ëÁæé": "#e13c30", "ÊµúÂè£ „ÅÇ„ÇÑ„ÇÅ": "#450099", "Âß´Â∑ù ÂèãÁ¥Ä": "#ee8b00", "Ëó§Âéü ËÇá": "#9595d2", "Êòü ËºùÂ≠ê": "#a6093d", "Êú¨Áî∞ Êú™Â§Æ": "#feb81c", "‰∏âËàπ ÁæéÂÑ™": "#12bfb2", "‰∏âÊùë „Åã„Å™Â≠ê": "#feb1bb", "Â§¢Ë¶ã „Çä„ÅÇ„ÇÄ": "#e59bdc",
            "Áõ∏Ëëâ Â§ïÁæé": "#f1e991", "Ëµ§Âüé „Åø„Çä„ÅÇ": "#ffcd00", "ÊµÖÂà© ‰∏ÉÊµ∑": "#009cbc", "ÂÆâÈÉ® Ëèú„ÄÖ": "#ef4b81", "ËçíÊú® ÊØîÂ•à": "#a0d884", "‰∏Ä„ÉéÁÄ¨ ÂøóÂ∏å": "#a50050", "Á∑íÊñπ Êô∫ÁµµÈáå": "#6cc24a", "ÁâáÊ°ê Êó©Ëãó": "#dc4404", "‰∏äÊù° Êò•Ëèú": "#5ac2e7", "Á•ûË∞∑ Â•àÁ∑í": "#9678d3", "Â∑ùÂ≥∂ ÁëûÊ®π": "#485cc7", "Á•ûÂ¥é Ëò≠Â≠ê": "#84329b", "ÂñúÂ§ö Êó•ËèúÂ≠ê": "#fcd757", "Êú®Êùë Â§èÊ®π": "#2d2926", "ÈªíÂüº „Å°„Å®„Åõ": "#ef3340", "Â∞èÈñ¢ È∫óÂ•à": "#9b26b6", "Â∞èÊó©Â∑ù Á¥óÊûù": "#e56db1", "Ë•øÂúíÂØ∫ Áê¥Ê≠å": "#e8cdd0", "È∑∫Ê≤¢ ÊñáÈ¶ô": "#606eb2", "‰Ωê‰πÖÈñì „Åæ„ÇÜ": "#da1984", "‰Ωê„ÄÖÊú® ÂçÉÊûù": "#0072ce", "‰ΩêÂüé Èõ™Áæé": "#171c8f", "Ê§éÂêç Ê≥ïÂ≠ê": "#f8485e", "Â°©Ë¶ã Âë®Â≠ê": "#dce6ed", "Ê∏ãË∞∑ Âáõ": "#0f9dde", "Â≥∂Êùë ÂçØÊúà": "#f67499", "Âüé„É∂Â¥é ÁæéÂòâ": "#fe9d1a", "Âüé„É∂Â¥é ËéâÂòâ": "#fedd00", "ÁôΩËèä „Åª„Åü„Çã": "#c964cf", "ÁôΩÂùÇ Â∞èÊ¢Ö": "#abcae9", "ÁôΩÈõ™ ÂçÉÂ§ú": "#efd7e5", "Á†ÇÂ°ö „ÅÇ„Åç„Çâ": "#7e93a7", "Èñ¢ Ë£ïÁæé": "#ffb3ab", "È´òÂû£ Ê•ì": "#47d7ac", "È∑πÂØåÂ£´ ËåÑÂ≠ê": "#5c068c", "Â§öÁî∞ ÊùéË°£Ëèú": "#0177c8", "Ê©ò „ÅÇ„Çä„Åô": "#5c88da", "ËæªÈáé „ÅÇ„Åã„Çä": "#e10600", "ÂçóÊù° ÂÖâ": "#e4012b", "Êñ∞Áî∞ ÁæéÊ≥¢": "#71c5e8", "‰∫åÂÆÆ È£õÈ≥•": "#60249f", "Êó©ÂùÇ ÁæéÁé≤": "#c701a0", "ÈÄüÊ∞¥ Â•è": "#033087", "‰πÖÂ∑ù Âá™": "#f8a3bc", "‰πÖÂ∑ù È¢Ø": "#7eddd3", "Êó•Èáé Ëåú": "#fa423a", "Ëó§Êú¨ ÈáåÂ•à": "#623b2a", "ÂèåËëâ Êùè": "#f8a3bc", "ÂåóÊù° Âä†ËìÆ": "#2ad2c9", "Â†Ä Ë£ïÂ≠ê": "#eca154", "ÂâçÂ∑ù „Åø„Åè": "#ce0037", "ÊùæÊ∞∏ Ê∂º": "#221651", "ÁöÑÂ†¥ Ê¢®Ê≤ô": "#e01a95", "ÂÆÆÊú¨ „Éï„É¨„Éá„É™„Ç´": "#a20067", "Âêë‰∫ï ÊãìÊµ∑": "#b0008e", "Ê£üÊñπ ÊÑõÊµ∑": "#c7579a", "Êùë‰∏ä Â∑¥": "#a5192e", "Ê£Æ‰πÖ‰øù ‰πÉ„ÄÖ": "#9cdbd9", "Ë´∏Êòü „Åç„Çâ„Çä": "#ffd100", "ÂÖ´Á•û „Éû„Ç≠„Éé": "#a6a4e0", "Â§ßÂíå ‰∫úÂ≠£": "#28724f", "ÁµêÂüé Êô¥": "#71dad4", "ÈÅä‰Ωê „Åì„Åö„Åà": "#f4a6d7", "‰æùÁî∞ Ëä≥‰πÉ": "#c4bcb7", "ÈæçÂ¥é Ëñ´": "#fae053", "ËÑáÂ±± Áè†Áæé": "#407ec8",
        },
        "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Ç∑„É£„Ç§„Éã„Éº„Ç´„É©„Éº„Ç∫": {
            "P": "#555555", "Ôº∞": "#555555",
            "Ê´ªÊú® Áúü‰πÉ": "#ffbad6", "È¢®Èáé ÁÅØÁπî": "#144384", "ÂÖ´ÂÆÆ „ÇÅ„Åê„Çã": "#ffe012",
            "ÊúàÂ≤° ÊÅãÈêò": "#f84cad", "Áî∞‰∏≠ Êë©Áæé„ÄÖ": "#a846fb", "ÁôΩÁÄ¨ Âí≤ËÄ∂": "#006047", "‰∏âÂ≥∞ ÁµêËèØ": "#3b91c4", "ÂπΩË∞∑ ÈúßÂ≠ê": "#d9f2ff",
            "Â∞èÂÆÆ ÊûúÁ©Ç": "#e5461c", "ÂúíÁî∞ Êô∫‰ª£Â≠ê": "#f93b90", "Ë•øÂüé Ê®πÈáå": "#ffc602", "ÊùúÈáé Âáõ‰∏ñ": "#89c3eb", "ÊúâÊ†ñÂ∑ù Â§èËëâ": "#90e667",
            "Â§ßÂ¥é ÁîòÂ•à": "#f54275", "Â§ßÂ¥é ÁîúËä±": "#e75bec", "Ê°ëÂ±± ÂçÉÈõ™": "#fafafa",
            "ËäπÊ≤¢ „ÅÇ„Åï„Å≤": "#f30100", "Èªõ ÂÜ¨ÂÑ™Â≠ê": "#5aff19", "ÂíåÊ≥â ÊÑõ‰æù": "#ff00ff",
            "ÊµÖÂÄâ ÈÄè": "#50d0d0", "Ê®ãÂè£ ÂÜÜÈ¶ô": "#be1e3e", "Á¶è‰∏∏ Â∞èÁ≥∏": "#7967c3", "Â∏ÇÂ∑ù ÈõõËèú": "#ffc639",
            "‰∏ÉËçâ „Å´„Å°„Åã": "#a6cdb6", "Á∑ãÁî∞ ÁæéÁê¥": "#760f10",
            "ÊñëÈ≥© „É´„Ç´": "#23120c", "‰∏ÉËçâ „ÅØ„Å•„Åç": "#8adfff",
        },
        "„Éñ„É´„Éº„Ç¢„Éº„Ç´„Ç§„Éñ": {
            "ÂÖàÁîü": "#555555",
            "Ê†óÊùë „Ç¢„Ç§„É™": "#4f423e", "È∞êÊ∏ï „Ç¢„Ç´„É™": "#faf1b8", "Â§©Èõ® „Ç¢„Ç≥": "#a9bfdf", "Â§©Á´• „Ç¢„É™„Çπ": "#414a61", "Èô∏ÂÖ´È≠î „Ç¢„É´": "#f9b5ca",
            "ÁôΩÁü≥ „Ç¶„Çø„Éè": "#e9d0f3",
            "ÊùèÂ±± „Ç´„Ç∫„Çµ": "#2a2839",
            "‰∏≠Âãô „Ç≠„É™„Éé": "#fbf8f4",
            "Á©∫‰∫ï „Çµ„Ç≠": "#a491ad", "Ê≠å‰Ωè „Çµ„ÇØ„É©„Ç≥": "#f1ece8",
            "Á†ÇÁãº „Ç∑„É≠„Ç≥": "#c5c4c8",
            "Ê°êËó§ „Éä„ÇÆ„Çµ": "#e4dad1", "ÊüöÈ≥• „Éä„ÉÑ": "#fdebe4",
            "ÁîüÂ°© „Éé„Ç¢": "#e8eff3",
            "‰ºäËçâ „Éè„É´„Ç´": "#534665", "ÈªíËàò „Éè„É´„Éä": "#ccc9d5",
            "ÊßåÊ∞∏ „Éí„É®„É™": "#d5f5e9",
            "ÊÑõÊ∏Ö „Éï„Ç¶„Ç´": "#423856",
            "Â∞èÈ≥•ÈÅä „Éõ„Ç∑„Éé": "#fbe0e7",
            "Â∞èÂ°ó „Éû„Ç≠": "#d05a5b", "‰ºäËêΩ „Éû„É™„Éº": "#fcd9a3",
            "ËÅñÂúí „Éü„Ç´": "#ffebf3", "ÊâçÁæΩ „Éü„Éâ„É™": "#fdd8a3", "ËøëË°õ „Éü„Éä": "#bfc6c4", "ËíºÊ£Æ „Éü„Éç": "#c7ddf7", "ÊúàÈõ™ „Éü„É§„Ç≥": "#f1f5f9", "ÈúûÊ≤¢ „Éü„É¶": "#787285",
            "ÊµÖÈªÑ „É†„ÉÑ„Ç≠": "#f0eeed",
            "È¢®ÂÄâ „É¢„Ç®": "#beadab", "ÊâçÁæΩ „É¢„É¢„Ç§": "#fddca6",
            "Êó©ÁÄ¨ „É¶„Ç¶„Ç´": "#615b96", "Ëä±Â≤° „É¶„Ç∫": "#fc7c82",
            "ÂÆáÊ≤¢ „É¨„Ç§„Çµ": "#fef3fb",
        }
    }

    _db = {} # same as _db0, but with better saturation and lightness, and uses rgb() format. also last name and name with spaces stripped as keys
    for series in _db0:
        _db[series] = {}
        for name in _db0[series]:
            color1   = _db0[series][name]
            match    = re.match(r"#(..)(..)(..)", re.sub(r"#(.)(.)(.)$", r"\1\1\2\2\3\3", color1))
            h,l,s    = colorsys.rgb_to_hls(*[int(x, 16) / 256 for x in match.groups()])
            r,g,b    = colorsys.hls_to_rgb(*[h, _lightness, _saturation] if s > 0.01 else [h, l, s])
            color2   = "rgb(%s)" % ",".join([str(int(x * 256)) for x in [r, g, b]])
            lastname = re.search(r"\S*$", name).group(0)
            nospname = re.sub(r"\s", "", name)
            _db[series][lastname] = color2
            _db[series][nospname] = color2

    @classmethod
    def colorHTML(self, html):
        # color character names starting a serifu (e.g. Â§™ÈÉé in "Â§™ÈÉé„Äå„Åì„Çì„Å´„Å°„ÅØ„Äç")

        # regex: 1 = line beginning, 2 = name, 3 = open paren etc.
        #         (1   )(2  )(3                   )
        regex = r"(^\s*)(.*?)([^\S\r\n]*[(Ôºà„Äå„ÄéÔΩ¢])"

        # Find what series is this html (different serieses may have same name charas with different colors)
        # list of characters found in html (with multiplicity, excluding bad patterns)
        def isCharaName(s): return len(s) > 0 and (not s.startswith("‚Äï"))
        charaList = [x[1] for x in re.findall(regex, html, flags=re.MULTILINE) if isCharaName(x[1])]
        # get series with most matching names in charaList
        series = max(self._db.keys(), key = lambda series: len([s for s in charaList if s in self._db[series].keys()]))

        # If at most 1/2 of charaList will get colored, it's likely that series is incorrect
        # We don't have a db for the correct series
        if 0.5 * len(charaList) > len([s for s in charaList if s in self._db[series].keys()]):
            return html

        # Wrap with <span>
        def nosp(s): return s.replace(" ", "")
        def decorHTML(name):
            if not nosp(name) in self._db[series]: return name
            return f"<span class='name name_{nosp(name)}'>{name}</span>"

        # CSS
        style = "<style>\n.name { font-weight: bold }\n" + "\n".join([".name_%s { color: %s; }" % (nosp(name), color) for (name, color) in self._db[series].items() if name in charaList]) + "\n</style>\n"

        # HTML (modified)
        html = re.sub(regex, lambda m: m.group(1) + decorHTML(m.group(2)) + m.group(3), html, flags=re.MULTILINE)

        return style + html


### Misc mini functions

def myRequest(url, headers={}, headers2={}, headers3={}, fmt=None):
    # Please add functions in Download class and call myRequest() from there.

    # Prevent sending same request many times in a short period
    if not hasattr(myRequest, "_lastTime"): myRequest._lastTime = 0
    if not hasattr(myRequest, "_lastUrl"):  myRequest._lastUrl  = None
    now = time.time() * 1000
    tooFast = (url == myRequest._lastUrl and now - myRequest._lastTime < 500)
    myRequest._lastTime, myRequest._lastUrl = now, url
    if tooFast:
        raise Exception(f"myRequest: requests to the same url in a short period: {url}")

    # Check fmt argument
    if fmt and (not fmt in ["json", "raw", "string"]):
        raise Exception(f"myRequest: invalid fmt specified: {fmt} (expected one of 'json', 'raw', 'string' or None")

    # %-encode non-ascii chars
    regex = r'[^\x00-\x7F]'
    for m in re.findall(regex, url):
        url = url.replace(m, urllib.parse.quote_plus(m, encoding="utf-8"))

    # Combine headers and headers2
    headersAll = { **headers, **headers2, **headers3 } # same property from headers3, then headers2 will survive

    # Create request
    req = urllib.request.Request(url, headers=headersAll)

    # Actually send request and catch error
    try:
        res = urllib.request.urlopen(req)
    except urllib.error.HTTPError as e:
        logging.error("myRequest: HTTPError (update cookie or review request headers)", e.code, e.reason)
        raise e
        # Error when downloading from Pixiv, possibly cookies.txt is outdated, or review http request headers.
    except urllib.error.URLError as e:
        raise e
    except Exception as e:
        raise e

    # decompress, decode, and optionally parse json (and html?)
    compfmt = "Content-Encoding" in res.headers and res.headers["Content-Encoding"]
    data = res.read()
    data = gzip.decompress(data) if compfmt == "gzip" else data # TODO deflate and brotli?
    if fmt == "raw":
        return data

    data = data.decode("utf-8")
    if fmt == "json":
        return json.loads(data)
    else:
        return data

def replaceLinks(desc, addTag=False): # replace novel/xxxxx links and user/xxxxx links
    f1 = lambda m: mkurl("user", id=m[1])
    f2 = lambda m: mkurl("novel", id=m[1])
    g1 = lambda m: f'<a href="{mkurl("user", id=m[1])}">user/{m[1]}</a>'
    g2 = lambda m: f'<a href="{mkurl("novel", id=m[1])}">novel/{m[1]}</a>'
    for (regex, rep) in [
        (r"https://www.pixiv.net/users/([0-9]*)",              g1 if addTag else f1),
        (r"https://www.pixiv.net/novel/show.php\?id=([0-9]*)", g2 if addTag else f2),
    ]:
        desc = re.sub(regex, rep, desc)
    return desc

def getRSign(xRestrict):
    return ["", "R ", "G "][int(xRestrict)]

def percentEncode(word):
    return urllib.parse.quote_plus(word, encoding="utf-8")

def mkurl(*args, **kwargs):
    return "/" + "/".join(percentEncode(str(v)) for v in args if v) + ("?" if kwargs else "") + "&".join(f"{k}={percentEncode(str(v))}" for k, v in kwargs.items() if v)

def addMissingCloseTags(html, tags=["b", "s", "u", "strong"]):

    # "aaa<b"  ==>  "aaa"
    m = re.search("<[^>]*$", html)
    if m:
        html = html[:m.start(0)]

    # Count opened counts for each tag in `tags`
    counts = {}
    for m in re.finditer(r"<\s*(/?)\s*(" + "|".join(tags) + r")\s*>", html):
        t = m.group(2)
        if m.group(1) == "/":
            counts[t] = (counts[t] - 1) if t in counts else -1
        else:
            counts[t] = (counts[t] + 1) if t in counts else 1

    # For each tag, if opened count > closed count then add close tags
    for t in counts:
        if counts[t] > 0:
            html += ("</" + t + ">") * counts[t]

    return html

def readCookiestxtAsHTTPCookieHeader(cookiestxt, domain):
    # Read Netscape HTTP Cookie File and return string for urllib request header
    #   urllib.request.Request(url, headers={"Cookie": ...})
    # Only matching domains will be extracted (if domain=="a.com" then www.a.com, .a.com etc will match)

    try:
        with open(cookiestxt) as f:
            results = []

            for line in f:
                # Skip empty or comment lines
                if re.match(r"^\s*$", line) or re.match(r"^# ", line):
                    continue
                fields = line[:-1].split('\t')
                # Each line must have 7 fields and has the specified
                if len(fields) == 7 and (not domain or fields[0].find(domain) != -1):
                    results += [ f"{fields[5]}={fields[6]}" ]

            # Output will be like "name=val; name=val"
            return "; ".join(results)

    except OSError as e:
        logging.warning("Could not read cookies.txt; R-18 search results will be omitted!")
        return False

def openInBrowser(url):
    if shutil.which("termux-open-url"):
        subprocess.run(["termux-open-url", url])
    else:
        webbrowser.open(url)

def yesterday():
    return datetime.date.today() - datetime.timedelta(days = 1)


### test

def test():
    return


### Main

if __name__ == "__main__":

    ## Parse args
    parser = argparse.ArgumentParser(description="Start web server to view pixiv novels. Load cookies.txt if found in current dir.")
    parser.add_argument("-a", "--autosave", action="store_true", help="Enable autosave; save visited novels as files.")
    parser.add_argument("-b", "--bind", type=str, metavar="ADDRESS", default="0.0.0.0", help="Bind to this address. (default: 0.0.0.0)")
    parser.add_argument("-C", "--nocolor", action="store_true", help="Disable character name colors.")
    parser.add_argument("-c", "--cachedir", type=str, default="_cache", help="Directory to store cache (NONE to disable).")
    parser.add_argument("-d", "--download", type=str, metavar="URL", help="Download a novel and exit.")
    parser.add_argument("-p", "--port", type=int, default=8080, help="Port number. (default: 8080)")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose mode.")
    parser.add_argument("--browser", action="store_true", help="Open in browser.")
    parser.add_argument("--sslcert", type=str, help="HTTPS cert file.")
    parser.add_argument("--sslkey", type=str, help="HTTPS key file.")
    parser.add_argument("--test", action="store_true")
    # parser.add_argument("-R", "--nor18", action="store_true", help="Disable R18.")
    args = parser.parse_args()

    # Save some options on global
    save = args.autosave
    color = not args.nocolor
    verbose = args.verbose
    cachedir = args.cachedir

    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(name)s:%(thread)d:%(message)s', level=logging.DEBUG if verbose else logging.ERROR)

    # Readme
    # - Check out: cool reader (android app)
    # - cookies.txt
    # - how to visit server

    # Download novel and exit.
    if args.download:
        novelID = re.match(r"[0-9]*", args.download).group()
        if not novelID:
            logging.error("Invalid url")
        else:
            f = Fetch(novelID)
            outfile = f.save()
            print("Written to " + outfile, flush=True)
        exit()

    # Try read cookies.txt
    # To obtain cookies.txt, use https://addons.mozilla.org/ja/firefox/addon/cookies-txt/ or https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid or type document.cookie in devtool
    Download.Pixiv.cookie = readCookiestxtAsHTTPCookieHeader("cookies.txt", "pixiv.net")

    # Test code
    if args.test:
        test()
        quit()

    # Check HTTPS support
    if args.sslcert and args.sslkey:
        serverHttps = True
        serverCert  = args.sslcert
        serverKey   = args.sslkey
    elif args.sslcert or args.sslkey:
        raise Exception("Specify both --sslcert and --sslkey for HTTPS support.")
    else:
        serverHttps = serverCert = serverKey = None

    # Run server
    serverHost = args.bind
    serverPort = args.port

    serverThread = threading.Thread(
        None,
        target=run_threaded_https_server,
        args=(MyRequestHandler, serverHost, serverPort, serverHttps, serverCert, serverKey),
        daemon=True)
    serverThread.start()

    serverUrl = f"http{'s' if serverHttps else ''}://{serverHost}:{serverPort}"
    print(f"Serving at {serverUrl}", flush=True)

    # Open in browser
    if args.browser:
        openInBrowser(serverUrl)

    try:
        serverThread.join()
    except KeyboardInterrupt:
        pass

