#!/usr/bin/env python3

import argparse
import base64
import colorsys
import dataclasses
import datetime
import gzip
import html.parser
import http.server
import json
import logging
import os
import re
import shutil
import subprocess
import sys
import threading
import traceback
import time
import urllib.error, urllib.parse, urllib.request
import webbrowser
from html import escape, unescape
from typing import *

# base
# TODO custom style (e.g. load _style.css)
# TODO no-r mode
# TODO help page on how to get cookies.txt
# TODO html extractor: get innerHTML? (do ranking descriptions contain html tags??)
# TODO other websites? (and rename to ja-novels.py)
# TODO chara name db in separate files (one file per each series, and embed on build)
# TODO url like /search/abc/6?compact=... (for better history integration)

# as a cli util
# TODO json api (like &json=1, for cli, also for test automation)


### Configuration

CONFIG = {
    "cachedir" : "",     # Cache directory ('' to disable)
    "nocolor"  : False,  # Disable colorizing character names?
    "savedir"  : "",     # Save novels directory ('' to disable)
    "noimage"  : False,  # Disable embedding images?
}

emoji = { "love": "üíô", "search": "üîç" }

def fcache(expiry=600, namef=lambda:"None"):
    def decor(f):
        def f2(*args, **kwargs):
            return withFileCache(namef(*args, **kwargs), lambda: f(*args, **kwargs), expiry)
        return f2
    return decor


### HTTP Server

def run_threaded_https_server(RequestHandlerClass, host="0.0.0.0", port=8030, https=False, certfile="", keyfile=""):
    """Run http server with threading and ssl support.
if `https` is true, (`certfile`, `keyfile`) is passed to load_cert_chain.
Use `openssl req -new -x509 -keyout CERTFILE -out KEYFILE -days 365 -nodes`."""
    import http.server, ssl
    def ssl_wrap(httpd, certfile, keyfile):
        # https://gist.github.com/DannyHinshaw/a3ac5991d66a2fe6d97a569c6cdac534
        ctx = ssl.SSLContext(protocol=ssl.PROTOCOL_TLS_SERVER)
        ctx.load_cert_chain(certfile=certfile, keyfile=keyfile)
        httpd.socket = ctx.wrap_socket(httpd.socket, server_side=True)
    httpd = http.server.ThreadingHTTPServer((host, port), RequestHandlerClass)
    if https:
        ssl_wrap(httpd, certfile, keyfile)
    httpd.serve_forever()

class MyRequestHandler(http.server.BaseHTTPRequestHandler):
    def log_message(self, format, *args):
        cli = self.client_address
        logging.debug(("%s:%s " + format) % (cli[0], cli[1], *args))

    def send(self, status, mime, headers, body):

        gz = "gzip" in (self.headers["Accept-Encoding"] or "")

        if type(body) is str:
            body = body.encode()
        if gz:
            body = gzip.compress(body)

        self.send_response(status)

        for k, v in headers:
            self.send_header(k, v)
        self.send_header("Content-type", mime)
        if gz: self.send_header("Content-Encoding", "gzip")
        self.send_header("Content-Length", str(len(body)))
        self.end_headers()

        try:
            self.wfile.write(body)
        except BrokenPipeError:
            logging.warning("BrokenPipeError")
            return

    def do_GET(self):

        parsed = urllib.parse.urlparse(self.path)
        paths  = [x for x in parsed.path.split("/") if x]
        param  = { k: v[0] for k, v in urllib.parse.parse_qs(parsed.query).items() }

        try:
            status, mime, headers, body = self.action(paths, param)
        except Exception as e:
            print(traceback.format_exc(), file=sys.stderr)
            logging.error("Error occured\n" + str(e))
            status, mime, headers, body = 500, "text/plain", [], "500 Internal Server Error"

        self.send(status, mime, headers, body)

    def action(self, paths, param):
        if len(paths) == 0:
            return self.action(["pixiv", "ranking"], param)
        elif len(paths) == 1:
            return self.action(["pixiv"] + paths, param)
        elif len(paths) == 2:
            site, cmd = paths[0], paths[1]
            # Select backend for site
            try:
                backend = BACKEND_TABLE[site]
            except KeyError:
                return 400, "text/plain", [], f"No such site: {site}"
            # Select data generator function
            try:
                makeData = getattr(backend, cmd.title()) # e.g. BackendPixiv.Novel
            except AttributeError:
                return 400, "text/plain", [], f"No such cmd on site: {cmd} on {site}"
            # Get data
            data = makeData(**param).data() # e.g. BackendPixiv.Novel(**param).data() : viewNovelData
            # Select view function
            makeView = VIEW_TABLE[type(data)]
            # Render view
            html = makeView(data) # e.g. viewNovel(data:viewNovelData) : str (html as a string)
            # Save function
            if CONFIG["savedir"] and (type(data) is viewNovelData):
                saveFile(data.title,
                         html,
                         prefix=getRSign(data.rate),
                         suffix=f" - {data.site} - {data.id}.html")
            # Send response
            status, mime, headers, body = 200, "text/html", [], html
            return status, mime, headers, body
        else:
            return 400, "text/plain", [], f"Invalid request"


### Backend

class BackendPixiv:

    searchOptions = [
        ("q", "query", str),
    ]

    class Search:

        def __init__(self, q, bookmarkCount=0, page=1, npages=1, compact=0, **_):
            self._query         = q
            self._bookmarkCount = int(bookmarkCount)
            self._page          = int(page)
            self._npages        = int(npages)
            self._compact       = int(compact)

        def data(self):

            dataList = [
                x for x in self._getDataList()
                if int(x["bookmarkCount"]) >= self._bookmarkCount
            ]

            items = [viewSearchDataItem(
                title  = x["title"],
                id     = x["id"],
                tags   = x["tags"],
                rate   = getRSign(x["xRestrict"]),
                desc   = x["description"],
                score  = x["bookmarkCount"],
                length = x["textCount"],
                user   = (x["userId"], x["userName"])
            ) for x in dataList]

            attr = lambda a, d: getattr(self, a) if hasattr(self, a) else d

            d = viewSearchData(
                site   = "pixiv",
                title  = self._html_title(),
                query  = attr("_query", ""),
                score  = attr("_bookmarkCount", 0),
                page   = attr("_page", 0),
                npages = attr("_npages", 0),
                mode   = "compact" if self._compact else "detailed",
                items  = items,
                viewOption = viewSearchDataViewOption(
                    htmlHeader = lambda: self._html_header(self._compact),
                    htmlNav    = lambda: self._html_nav(self._compact),
                )
            )

            return d

        def _getDataList(self):
            dataList = []
            for i in range(self._npages):
                resJson = Resources.Pixiv.jsonSearch(self._query, i+self._page)
                dataList += resJson["body"]["novel"]["data"]
            return dataList

        def _html_title(self):
            return f""

        def _html_header(self, compact):
            return ""

        def _html_nav(self, compact):
            # navigation links (on both top and bottom of page)
            common = { "q": self._query, "npages": self._npages, "bookmarks": self._bookmarkCount }
            hrefPrev   = mkurl("search", **common, page=max(1,self._page-self._npages), compact=compact)
            hrefNext   = mkurl("search", **common, page=self._page+self._npages,        compact=compact)
            hrefToggle = mkurl("search", **common, page=self._page,                     compact=int(not compact))
            return f"""
                <div id="nav" style="display: flex">
                    <span style="flex: 1">
                        <a href="{hrefToggle}">{compact and "Ë©≥Á¥∞Ë°®Á§∫" or "„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫"}</a>
                    </span>
                    <span style="flex: 1"></span>
                    <span style="flex: 1; text-align: right">
                        <a href='{hrefPrev}'>Ââç„Å∏</a>
                        <a href='{hrefNext}'>Ê¨°„Å∏</a>
                    </span>
                </div>
            """

    class User(Search):

        def __init__(self, id, bookmarkCount=0, compact=0):
            self._userID        = int(id)
            self._bookmarkCount = int(bookmarkCount)
            self._compact       = int(compact)

        def _getDataList(self):

            # First, response includes all novelIDs of user
            json1 = Resources.Pixiv.jsonUserAll(self._userID)
            novels = json1["body"]["novels"] # a dict or a empty list
            if len(novels) == 0:
                return []
            novelIDs = list(novels.keys())

            # Next, get data for each novel (100 novels at once)
            # So, 0 novel = 0 request, 1-100 novels = 1 request, 101-200 = 2 etc.
            dataList = []
            n = 100
            numRequests = 1 + int((len(novelIDs)-1)/n)
            for ids in [novelIDs[n*i:n*(i+1)] for i in range(numRequests)]:
                json2 = Resources.Pixiv.jsonUserNovels(self._userID, ids)
                dataList += list(json2["body"]["works"].values())

            return dataList

        def _html_title(self):
            return f"User {self._userID}"

        def _html_header(self, compact):
            return ""

        def _html_nav(self, compact):
            hrefToggle = mkurl("user", id=self._userID, compact=int(not compact))
            return f"<a href='{hrefToggle}'>{compact and 'Ë©≥Á¥∞Ë°®Á§∫' or '„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫'}</a>"

    class Ranking(Search):

        _modeNames = {
            "daily":           "„Éá„Ç§„É™„Éº",
            "weekly":          "„Ç¶„Ç£„Éº„ÇØ„É™„Éº",
            "monthly":         "„Éû„É≥„Çπ„É™„Éº",
            "rookie":          "„É´„Éº„Ç≠„Éº",
            "weekly_original": "„Ç™„É™„Ç∏„Éä„É´",
            "male":            "Áî∑Â≠ê„Å´‰∫∫Ê∞ó",
            "female":          "Â•≥Â≠ê„Å´‰∫∫Ê∞ó",
            "daily_r18":       "„Éá„Ç§„É™„Éº R-18",
            "weekly_r18":      "„Ç¶„Ç£„Éº„ÇØ„É™„Éº R-18",
            "male_r18":        "Áî∑Â≠ê„Å´‰∫∫Ê∞ó R-18",
            "female_r18":      "Â•≥Â≠ê„Å´‰∫∫Ê∞ó R-18",
        }

        def __init__(self, mode="daily", date="", compact=0):
            self._mode = mode

            # set self._date to a date object (at most yesterday)
            if re.match(r"\d\d\d\d-\d\d-\d\d", date):
                d = datetime.date.fromisoformat(date)
                y = yesterday()
                self._date = d if d <= y else y
            else:
                self._date = yesterday()

            self._compact = compact
            self._bookmarkCount = 0

        def _cacheName(self):
            return f"pixiv-ranking-{self._mode}-{self._date.isoformat().replace('-', '')}"

        def _getDataList(self):
            dataList = []
            for page in [1, 2]:
                res = Resources.Pixiv.rankingPhp(self._mode, self._date, page)
                dataList += self._getDataListFromHTML(res)
            return dataList

        def _getDataListFromHTML(self, html):
            data = []

            p = StringParser(html) # faster than html parser
            toInt = lambda s: int(re.sub(r"\D+", "", s))
            for _ in range(50):
                d = {}
                # one novel for each ._novel-item
                if p.seek("_novel-item") == -1:
                    break
                d["xRestrict"]     = "r18" in self._mode
                # attrs from img.cover
                d["title"]         = re.sub(r"/.*", "", p.extract('alt="', '"'))
                d["tags"]          = p.extract('data-tags="', '"').split()
                d["id"]            = p.extract('data-id="', '"')
                # innerHTML from div.chars
                d["textCount"]     = toInt(p.extract('<div class="chars">', 'ÊñáÂ≠ó</div>'))
                # innerHTML from a.bookmark-count
                d["bookmarkCount"] = toInt(p.extract('bookmark-count', '</i>', '</a>'))
                # innerHTML from p.novel-caption
                d["description"]   = unescape(p.extract('<p class="novel-caption">', '</p>', default="").strip())
                # attrs from a.user
                d["userId"]        = p.extract('data-user_id="', '"')
                d["userName"]      = p.extract('data-user_name="', '"')
                # add entry to list
                data.append(d)

            return data

        def _html_title(self):
            return f"{self._modeNames[self._mode]} „É©„É≥„Ç≠„É≥„Ç∞ {self._date}"

        def _html_header(self, compact):
            # links for other rankings
            hrefBase = mkurl("ranking", compact=compact, date=self._date)
            modeLinks1 = "\n".join([f"""<a href="{hrefBase}&mode={mode}"{ ' class="ranking-selected"' if mode == self._mode else ""}>{self._modeNames[mode]}</a>""" for mode in self._modeNames.keys() if not "r18" in mode])
            if Resources.Pixiv.hasCookie():
                modeLinks2 = "<span>R-18:</span>"
                modeLinks2 += "\n".join([f"""<a href="{hrefBase}&mode={mode}"{ ' class="ranking-selected"' if mode == self._mode else ""}>{self._modeNames[mode].replace(" R-18", "")}</a>""" for mode in self._modeNames.keys() if "r18" in mode])
            else:
                modeLinks2 = "R-18 „É©„É≥„Ç≠„É≥„Ç∞„ÇíË¶ã„Çã„Å´„ÅØ cookies.txt „ÅåÂøÖË¶Å„Åß„Åô„ÄÇ"
            modeLinksCSS = """
                #ranking-modes { margin: .8em 0; font-size: small; line-height: 1.8 }
                #ranking-modes a { margin-right: 1.4em }
                #ranking-modes span { margin-right: 1.0em }
                .ranking-selected { font-weight: bold }
            """
            modeLinks = f"""
                <style>
                    {modeLinksCSS}
                </style>
                <p id="ranking-modes">
                    {modeLinks1}
                    <br>
                    {modeLinks2}
                </p>"""

            # construct html
            return f"""
                {modeLinks}
                <form style="text-align: center; margin: .5em 0" action=ranking>
                    <label for="date">Êó•‰ªò:</label>
                    <input type="date" id="date" name="date" value="{self._date}" max="{yesterday()}">
                    <input type="submit" value="{emoji['search']}">
                    <input type="hidden" name="mode" value="{self._mode}">
                    <input type="hidden" name="compact" value="{1 if compact else ''}">
                </form>
            """

        def _html_nav(self, compact):
            hrefToggle = mkurl("ranking", q=self._mode, compact=int(not compact), date=self._date)
            return f"<a href='{hrefToggle}'>{compact and 'Ë©≥Á¥∞Ë°®Á§∫' or '„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫'}</a>"

    class Novel:

        def __init__(self, id):
            self._novelID = id

        def data(self):
            html = Resources.Pixiv.showPhp(self._novelID)
            jso  = self._extractData(html)

            o_content = jso["content"]
            for (regex, replace) in [
                    (r"$", "<br>"),
                    (r"\[newpage\]", "<hr>\n"),
                    (r"\[chapter:(.*?)\]", "<h2>\\1</h2>\n"),
                    (r"\[\[rb:(.*?)(>|&gt;)(.*?)\]\]", "<ruby>\\1<rt>\\3</rt></ruby>"),
                    ]:
                o_content = re.sub(regex, replace, o_content, flags=re.MULTILINE)

            # threshold on total embedded image size
            # if total image size exceeds MAX_TOTAL_IMAGE_SIZE, return only link next time
            MAX_TOTAL_IMAGE_SIZE  = 0 if CONFIG["noimage"] else (5 * 10**6) # 5 MBytes
            currentTotalImageSize = 0 # 0 Byte

            # pixivimage
            def getPixivImg(imgId):
                assert imgId.isdigit()
                jso = Resources.Pixiv.artworkPagesJson(imgId)
                url = jso["body"][0]["urls"]["original"]
                return url, (currentTotalImageSize < MAX_TOTAL_IMAGE_SIZE) and Resources.Pixiv.artworkImage(url)

            # uploadedimage
            def getUploadedImg(imgId):
                assert imgId.isdigit()
                url = jso["textEmbeddedImages"][imgId]["urls"]["original"]
                return url, (currentTotalImageSize < MAX_TOTAL_IMAGE_SIZE) and Resources.Pixiv.uploadedImage(url)

            # create image tag
            def getImgTag(imgType:"Literal['uploadedimage', 'pixivimage']", imgId):
                nonlocal currentTotalImageSize
                url, img = (getPixivImg if imgType == "pixivimage" else getUploadedImg)(imgId)
                if not img:
                    return f"""<figure><a href="{url}">[{imgType}:{imgId}]</a></figure>"""
                currentTotalImageSize += len(img)
                imgB64 = base64.b64encode(img).decode("utf-8")
                return f"""<figure><a href="{url}"><img src=\"data:image/png;base64,{imgB64}\" alt=\"[{imgType}:{imgId}]\" style=\"width: 100%\"></a></figure>"""

            # replace image links
            o_content = re.sub(r"\[(pixivimage|uploadedimage):(.*?)\]", lambda m: getImgTag(m.group(1), m.group(2)), o_content)

            # colorize character names
            if not CONFIG["nocolor"]:
                o_content = CharaColor.colorHTML(o_content)

            tags = [x["tag"] for x in jso["tags"]["tags"]]

            # embed json
            # e.g. <div data='{"x":10,"y":"„ÅÇ"}'></div>
            # jso1 = json.dumps(jso, ensure_ascii=False, separators=(',', ':'))
            # for (fromStr, toStr) in [ # unnecessary if b64 is used
            #         ("'", "&#39;"), # ("\"", "&quot;"), ("<", "&lt;"), (">", "&gt;")
            #         ]:
            #     jso1 = jso1.replace(fromStr, toStr)
            # jso2 = gzip.compress(jso1.encode("utf-8"))
            # jso3 = base64.b64encode(jso2).decode("utf-8")

            data = viewNovelData(
                site  = "pixiv",
                title = jso["title"],
                id    = jso["id"],
                rate  = getRSign(jso["xRestrict"]),
                body  = o_content,
                desc  = replaceLinks(jso["description"]),
                tags  = tags,
                orig  = f"https://www.pixiv.net/novel/show.php?id={jso['id']}",
                user  = (jso["userId"], jso["userId"]),
                score = jso["bookmarkCount"],
                date  = datetime.datetime.strptime(jso["createDate"][:10], "%Y-%m-%d"),
            )

            return data

        def _extractData(self, html): # parse show.php and get json inside meta[name=preload-data]
            # Extract json string
            # querySelector("meta[name=meta-preload-data]").content
            s = sfind(html, ["meta-preload-data\" content='", "'"])
            s = unescape(s)

            # Extract part of json
            json1 = json.loads(s)
            return json1["novel"][list(json1["novel"].keys())[0]]

BACKEND_TABLE = {
    "pixiv": BackendPixiv,
}


### Views

@dataclasses.dataclass
class viewNovelDataPage:
    page:  int
    title: str
    id:    str
    desc:  str

@dataclasses.dataclass
class viewNovelData:
    site:  str
    title: str
    id:    str # content id (used for id= param)
    rate:  Literal["", "R", "G"]
    body:  str
    desc:  str
    tags:  list[str]
    orig:  str
    user:  tuple[str, str] # (id, name) such that /<site>/user?id=<id> will work
    score: int
    date:  datetime.datetime
    pages: Optional[list[viewNovelDataPage]] = None # For toc page (mokuji)

def viewNovel(d:viewNovelData):

    # print(json.dumps(dataclasses.asdict(dataclasses.replace(d, body='')), default=str, ensure_ascii=False, indent=2))

    o_css = """
        body { max-width: 700px; margin: 1em auto; padding: 0 .5em; }
        @media screen and (max-aspect-ratio: .75) and (max-width: 13cm) { /* Mobile */
            body { max-width: 100%; margin: 0.5em 0.5em }
        }
        #novel { line-height: 1.9; border-bottom: solid #888 1px; margin-bottom: 2em; padding-bottom: 3em; }
        #data { display: none }
    """

    o_tags = ",\n".join(f"<a href='{mkurl(d.site, 'search', q=x)}'>{x}</a>" for x in d.tags)

    o_rSign = re.sub("^ *", " ", d.rate) if d.rate else ""

    o_info = f"""
        <p> „Çø„Ç∞: {o_tags} </p>
        <p>
            <a href="{d.orig}">{d.site.capitalize()}„ÅßÈñã„Åè</a>
            ID:{d.id}
            U:<a href="{mkurl(d.site, 'user', id=d.user[0])}">{d.user[1]}</a>
            B:{d.score}
            D:{d.date.strftime("%Y-%m-%d")}
        </p>
    """

    nl = "\n"
    o_toc = "" if not d.pages else f"""
        <ul>
            {nl.join(f'''
                <li>
                    <a href="{mkurl(d.site, 'novel', id=p.id, page=p.page)}">
                        {p.page}
                        {escape(p.title)}
                    </a>
                    <br>
                    <div>{p.desc}</div>
                </li>
            ''' for p in d.pages)}
        </ul>
    """

    o_html = f"""
        <!DOCTYPE html>
        <html lang="ja">
        <head>
            <title>{o_rSign}{d.title} - {d.site}</title>
            <meta http-equiv="content-type" content="text/html; charset=utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <!--<link rel="stylesheet" href="_style.css">-->
            <style>{o_css}</style>
        </head>
        <body>
            <h1>{d.title}</h1>
            <div id="novel">
                {d.body}
                {o_toc}
            </div>
            <div id="info"> <p> {d.desc} </p> {o_info} </div>
        </body>
        </html>
    """
    o_html = "\n".join(x.strip() for x in o_html.splitlines())
    # <div id="data" data-novels='{o_json}'></div>

    return o_html

@dataclasses.dataclass
class viewSearchDataItem:
    title:  str
    id:     str
    tags:   list[str]
    rate:   Literal["", "R", "G"]
    desc:   str
    score:  int
    length: int
    user:   tuple[str, str] # (id, name)

@dataclasses.dataclass
class viewSearchDataViewOption:
    htmlHeader: str|Callable = ""
    htmlNav:    str|Callable = ""

@dataclasses.dataclass
class viewSearchData:
    site:   str
    title:  str # if nonempty used as title
    query:  str
    score:  int # TODO: replace with generic filtering condition
    # filter: dict
    page:   int
    npages: int
    mode:   Literal["detailed", "compact", "tiled"] # view mode
    items:  list[viewSearchDataItem]
    viewOption: Optional[viewSearchDataViewOption] = None

def viewSearch(d:viewSearchData):

    def searchBar(query="", compact=False, bookmarks=0, npages=1):
        html = f"""
            <form style="text-align: right; line-height: 1.5" action=search>
                <input type="text" name="q" placeholder="Ê§úÁ¥¢" value="{query}">
                <input type="submit" value="{emoji['search']}">
                <input type="hidden" name="compact" value="{1 if compact else ''}">
        """
        html += f"""
                <br>
                <select name="npages">
                <option value="1"{" selected" if npages == 1 else ""}>1„Éö„Éº„Ç∏„Åö„Å§
                <option value="2"{" selected" if npages == 2 else ""}>2„Éö„Éº„Ç∏„Åö„Å§
                <option value="3"{" selected" if npages == 3 else ""}>3„Éö„Éº„Ç∏„Åö„Å§
                </select>
                {emoji['love']}:<input type="text" name="bookmarks" value="{bookmarks}" size="3">
        """ if query else ""
        html += f"""
            </form>
        """
        return html

    # common css
    css = """
        body { max-width: 750px; margin: 1em auto; padding: 0 .5em; }
        #main { margin: 1em 0 }
        li p { margin: 1.5em 0 }
        li > a.title { color: white }
        td:not(:nth-child(4)) { text-align: center; padding: 0 .35em }
        td:nth-child(4) { padding-left: 2em }
    """

    # novels
    novels = "<table>" if d.mode == "compact" else "<ul>"
    for x in d.items: # self._novels
        href = mkurl(d.site, "novel", id=x.id)
        if d.mode == "compact":
            novels += f"<tr><td><a href=\"{href}\">{x.id}</a></td><td>{re.sub('^ *', ' ', x.rate) if x.rate else ''}</td><td>{x.score}</td><td>{x.title}</td></tr>\n"
        else:
            desc = "<br>".join(replaceLinks(x.desc).split("<br />")[0:5])
            desc = addMissingCloseTags(desc, tags=["b", "s", "u", "strong"])
            tags = ", ".join([f"<a href=\"/{d.site}/search?q={escape(y)}\">{y}</a>" for y in x.tags])
            novels += f"<li>{x.title} ({x.length}Â≠ó) <a href=\"{href}\">[{x.id}]</a><p>{desc}</p>{emoji['love']} {x.score}<br>{tags}</li><hr>\n"
    novels += "</table>" if d.mode == "compact" else "</ul>"

    # search bar
    o_searchBar = searchBar(query=d.query, compact=(d.mode == "compact"), bookmarks=d.score, npages=d.npages)

    if vo := d.viewOption:
        o_header = vo.htmlHeader if (type(vo.htmlHeader) is str) else vo.htmlHeader()
        o_nav    = vo.htmlNav    if (type(vo.htmlNav)    is str) else vo.htmlNav()
    else:
        o_header = o_nav = ""

    o_title = d.title or f"{d.query}{(f' ({d.page})' if d.page > 1 else '')}"

    # final html
    o_html = f"""
        <!DOCTYPE html>
        <html lang="ja">
        <head>
            <title>{o_title} - {d.site}</title>
            <meta http-equiv="content-type" content="text/html; charset=utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <style> {css} </style>
        </head>
        <body>
            <h1>{o_title}</h1>
            {o_searchBar}
            {o_header}
            <hr>
            {o_nav}
            <div id="main"> {novels} </div>
            {o_nav}
        </body>
        </html>
    """
    o_html = "\n".join(x.strip() for x in o_html.splitlines())
    return o_html

VIEW_TABLE = {
    viewNovelData:  viewNovel,
    viewSearchData: viewSearch,
}


### Resources

class Resources:

    # Resource downloading often breaks due to expired cookies, change in request headers etc.
    # So we want unit-testable functions for each resource.

    class Pixiv:

        # 404 without headers
        _headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Pragma': 'no-cache',
            'Referer': 'https://www.pixiv.net/',
            'Cache-Control': 'no-cache'
        }

        # cookie = readCookiestxtAsHTTPCookieHeader("../cookies.txt", "pixiv.net")
        cookie = None

        @classmethod
        def hasCookie(cls):
            return bool(cls.cookie)

        @classmethod
        def cookieHeader(cls):
            return { "cookie": cls.cookie } if cls.hasCookie() else {}

        @classmethod
        @fcache(3*86400, lambda cls, novelID: f"pixiv-showPhp-{novelID}")
        def showPhp(cls, novelID):
            url = f"https://www.pixiv.net/novel/show.php?id={novelID}"
            return httpGet(url, headers=[cls._headers, cls.cookieHeader()])

        @classmethod
        @fcache(3600, lambda cls, mode, date, page: f"pixiv-ranking-{mode}-{date.isoformat().replace('-', '')}-{page}")
        def rankingPhp(cls, mode, date: datetime.date, page: int):
            # Check modes
            MODES = { "daily", "weekly", "monthly", "rookie", "weekly_original", "male", "female", "daily_r18", "weekly_r18", "male_r18", "female_r18", }
            if not mode in MODES:
                raise Exception(f"Resources.Pixiv.rankingPhp: unknown mode {mode} (expected one of {MODES})")
            if (not cls.hasCookie()) and mode.endswith("r18"):
                raise Exception(f"Resources.Pixiv.rankingPhp: cookie is needed to view ranking of mode {mode}")
            # Make URL
            dateiso = date.isoformat()
            url = f"https://www.pixiv.net/novel/ranking.php?mode={mode}&date={dateiso.replace('-', '')}"
            if page > 1:
                url += f"&page={page}"
            # Download
            return httpGet(url, headers=[cls._headers, (cls.cookieHeader() if mode.endswith("r18") else {})])

        @classmethod
        @fcache(3600, lambda cls, userID: f"pixiv-user-{userID}")
        def jsonUserAll(cls, userID):
            url = f"https://www.pixiv.net/ajax/user/{userID}/profile/all?lang=ja"
            return httpGet(url, fmt="json", headers=[cls._headers, cls.cookieHeader()])

        @classmethod
        @fcache(3600, lambda cls, userID, novelIDs: f"pixiv-user-{userID}-{sum(map(int, novelIDs))}")
        def jsonUserNovels(cls, userID, novelIDs):
            n = 100
            if len(novelIDs) <= 0:
                raise Exception(f"Resources.Pixiv.apiUserIds: at least 1 novel IDs are required")
            if len(novelIDs) > n:
                raise Exception(f"Resources.Pixiv.apiUserIds: at most {n} novel IDs are allowed to query at once; got {len(novelIDs)}")
            idsParams = "&".join([f"ids[]={x}" for x in novelIDs])
            url = f"https://www.pixiv.net/ajax/user/{userID}/profile/novels?{idsParams}"
            return httpGet(url, fmt="json", headers=[cls._headers, cls.cookieHeader()])

        @classmethod
        @fcache(600, lambda cls, word, page: f"pixiv-search-{''.join(hex(x)[2:] for x in word.encode())}-{page}")
        def jsonSearch(cls, word, page):
            params = f"?word={word}&order=date_d&mode=all&p={page}&s_mode=s_tag&lang=ja"
            url = f"https://www.pixiv.net/ajax/search/novels/{word}{params}"
            return httpGet(url, fmt="json", headers=[cls._headers, cls.cookieHeader()])

        @classmethod
        def artworkPagesJson(cls, id):
            url = f"https://www.pixiv.net/ajax/illust/{id}/pages?lang=ja"
            try:
                return httpGet(url, fmt="json", headers=[cls._headers, cls.cookieHeader(), { "Accept": "application/json" }])
            except urllib.error.HTTPError as e:
                if e.code == 404:
                    logging.warning("Resources.Pixiv.artworkPagesJson: Artwork not found" + (", or cookie is required to view this artwork" if not cls.hasCookie() else "") + ":", e)
                else:
                    logging.error("Resources.Pixiv.artworkPagesJson: Unknown error:", e)
                raise e

        @classmethod
        def artworkImage(cls, url):
            headers2 = { "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8" }
            return httpGet(url, fmt="bytes", headers=[cls._headers, headers2])

        @classmethod
        def uploadedImage(cls, url):
            # headers2 = { "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8" }
            return httpGet(url, fmt="bytes", headers=cls._headers)


### Character name colorizer

class CharaColor:
    _lightness = 0.52
    _saturation = 0.7

    _db0 = {
        # _db0[series][characterName] = color (format: "#xxxxxx" or "#xxx")
        # Idolmaster (https://imas-db.jp/misc/color.html) (imcomplete, and not very strict)
        "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Éü„É™„Ç™„É≥„É©„Ç§„Éñ": {
            "P": "#555555", "Ôº∞": "#555555",
            "Â§©Êµ∑ Êò•È¶ô": "#e22b30", "Â¶ÇÊúà ÂçÉÊó©": "#2743d2", "Ëê©Âéü Èõ™Ê≠©": "#d3dde9", "È´òÊßª „ÇÑ„Çà„ÅÑ": "#f39939", "ÁßãÊúà ÂæãÂ≠ê": "#01a860", "‰∏âÊµ¶ „ÅÇ„Åö„Åï": "#9238be", "Ê∞¥ÁÄ¨ ‰ºäÁπî": "#fd99e1", "ËèäÂú∞ Áúü": "#515558", "ÂèåÊµ∑ ‰∫úÁæé": "#ffe43f", "ÂèåÊµ∑ ÁúüÁæé": "#ffe43f", "Êòü‰∫ï ÁæéÂ∏å": "#b4e04b", "ÊàëÈÇ£Ë¶á Èüø": "#01adb9", "ÂõõÊù° Ë≤¥Èü≥": "#a6126a", "Èü≥ÁÑ° Â∞èÈ≥•": "#00ff00",
            "‰ºäÂêπ Áøº": "#fed552", " „Ç®„Éü„É™„Éº": "#554171", "Â§ßÁ•û Áí∞": "#ee762e", "Êò•Êó• Êú™Êù•": "#ea5b76", "Âåó‰∏ä È∫óËä±": "#6bb6b0", "ÂåóÊ≤¢ Âøó‰øù": "#afa690", "Êú®‰∏ã „Å≤„Å™„Åü": "#d1342c", "È´òÂùÇ Êµ∑Áæé": "#e9739b", "Ê°úÂÆà Ê≠åÁπî": "#274079", "‰ΩêÁ´π ÁæéÂ•àÂ≠ê": "#58a6dc", "ÁØ†ÂÆÆ ÂèØÊÜê": "#b63b40", "Â≥∂Âéü „Ç®„É¨„Éä": "#9bce92", " „Ç∏„É•„É™„Ç¢": "#d7385f", "ÁôΩÁü≥ Á¥¨": "#ebe1ff", "Âë®Èò≤ Ê°ÉÂ≠ê": "#efb864", "È´òÂ±± Á¥ó‰ª£Â≠ê": "#7f6575", "Áî∞‰∏≠ Áê¥Ëëâ": "#92cfbb", "Â§©Á©∫Ê©ã ÊúãËä±": "#bee3e3", "Âæ≥Â∑ù „Åæ„Å§„Çä": "#5abfb7", "ÊâÄ ÊÅµÁæé": "#454341", "Ë±äÂ∑ù È¢®Ëä±": "#7278a8", "‰∏≠Ë∞∑ ËÇ≤": "#f7e78e", "Ê∞∏Âêâ Êò¥": "#aeb49c", "‰∏ÉÂ∞æ ÁôæÂêàÂ≠ê": "#c7b83c", "‰∫åÈöéÂ†Ç ÂçÉÈ∂¥": "#f19557", "Èáé„ÄÖÂéü Ëåú": "#eb613f", "ÁÆ±Â¥é ÊòüÊ¢®Ëä±": "#ed90ba", "È¶¨Â†¥ „Åì„ÅÆ„Åø": "#f1becb", "Á¶èÁî∞ „ÅÆ„ÇäÂ≠ê": "#eceb70", "ËàûÊµú Ê≠©": "#e25a9b", "ÁúüÂ£Å ÁëûÂ∏å": "#99b7dc", "ÊùæÁî∞ ‰∫úÂà©Ê≤ô": "#b54461", "ÂÆÆÂ∞æ Áæé‰πü": "#d7a96b", "ÊúÄ‰∏ä ÈùôÈ¶ô": "#6495cf", "ÊúõÊúà ÊùèÂ•à": "#7e6ca8", "ÁôæÁÄ¨ ËéâÁ∑í": "#f19591", "Áü¢Âêπ ÂèØÂ•à": "#f5ad3b", "Ê®™Â±± Â•àÁ∑í": "#788bc5", " „É≠„Ç≥": "#fff03c",
            "ÈùíÁæΩ ÁæéÂí≤": "#57c7c4",
            " Áé≤Èü≥": "#512aa3", " Ë©©Ëä±": "#e6f9e5", "Â••Á©∫ ÂøÉÁôΩ": "#fefad4",
            "Êó•È´ò ÊÑõ": "#e85786", "Ê∞¥Ë∞∑ ÁµµÁêÜ": "#00adb9", "ÁßãÊúà Ê∂º": "#b2d468",
        },
        "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Ç∑„É≥„Éá„É¨„É©„Ç¨„Éº„É´„Ç∫": {
            "P": "#555555", "Ôº∞": "#555555",
            " „Ç¢„Éä„Çπ„Çø„Ç∑„Ç¢": "#b1c9e8", "ÂèäÂ∑ù Èõ´": "#f4f9ff", "Â§ßÊßª ÂîØ": "#f6be00", "‰πôÂÄâ ÊÇ†Ë≤¥": "#e3bec3", "ÂñúÂ§öË¶ã Êüö": "#f0ec74", "Ê°êÁîü „Å§„Åã„Åï": "#ab4ec6", "Â∞èÊó•Âêë ÁæéÁ©Ç": "#db3fb1", "È´òÊ£Æ ËóçÂ≠ê": "#ceea80", "ÈÅìÊòéÂØ∫ Ê≠åÈà¥": "#d22730", " „Éä„Çø„Éº„É™„Ç¢": "#f5633a", "Èõ£Ê≥¢ Á¨ëÁæé": "#e13c30", "ÊµúÂè£ „ÅÇ„ÇÑ„ÇÅ": "#450099", "Âß´Â∑ù ÂèãÁ¥Ä": "#ee8b00", "Ëó§Âéü ËÇá": "#9595d2", "Êòü ËºùÂ≠ê": "#a6093d", "Êú¨Áî∞ Êú™Â§Æ": "#feb81c", "‰∏âËàπ ÁæéÂÑ™": "#12bfb2", "‰∏âÊùë „Åã„Å™Â≠ê": "#feb1bb", "Â§¢Ë¶ã „Çä„ÅÇ„ÇÄ": "#e59bdc",
            "Áõ∏Ëëâ Â§ïÁæé": "#f1e991", "Ëµ§Âüé „Åø„Çä„ÅÇ": "#ffcd00", "ÊµÖÂà© ‰∏ÉÊµ∑": "#009cbc", "ÂÆâÈÉ® Ëèú„ÄÖ": "#ef4b81", "ËçíÊú® ÊØîÂ•à": "#a0d884", "‰∏Ä„ÉéÁÄ¨ ÂøóÂ∏å": "#a50050", "Á∑íÊñπ Êô∫ÁµµÈáå": "#6cc24a", "ÁâáÊ°ê Êó©Ëãó": "#dc4404", "‰∏äÊù° Êò•Ëèú": "#5ac2e7", "Á•ûË∞∑ Â•àÁ∑í": "#9678d3", "Â∑ùÂ≥∂ ÁëûÊ®π": "#485cc7", "Á•ûÂ¥é Ëò≠Â≠ê": "#84329b", "ÂñúÂ§ö Êó•ËèúÂ≠ê": "#fcd757", "Êú®Êùë Â§èÊ®π": "#2d2926", "ÈªíÂüº „Å°„Å®„Åõ": "#ef3340", "Â∞èÈñ¢ È∫óÂ•à": "#9b26b6", "Â∞èÊó©Â∑ù Á¥óÊûù": "#e56db1", "Ë•øÂúíÂØ∫ Áê¥Ê≠å": "#e8cdd0", "È∑∫Ê≤¢ ÊñáÈ¶ô": "#606eb2", "‰Ωê‰πÖÈñì „Åæ„ÇÜ": "#da1984", "‰Ωê„ÄÖÊú® ÂçÉÊûù": "#0072ce", "‰ΩêÂüé Èõ™Áæé": "#171c8f", "Ê§éÂêç Ê≥ïÂ≠ê": "#f8485e", "Â°©Ë¶ã Âë®Â≠ê": "#dce6ed", "Ê∏ãË∞∑ Âáõ": "#0f9dde", "Â≥∂Êùë ÂçØÊúà": "#f67499", "Âüé„É∂Â¥é ÁæéÂòâ": "#fe9d1a", "Âüé„É∂Â¥é ËéâÂòâ": "#fedd00", "ÁôΩËèä „Åª„Åü„Çã": "#c964cf", "ÁôΩÂùÇ Â∞èÊ¢Ö": "#abcae9", "ÁôΩÈõ™ ÂçÉÂ§ú": "#efd7e5", "Á†ÇÂ°ö „ÅÇ„Åç„Çâ": "#7e93a7", "Èñ¢ Ë£ïÁæé": "#ffb3ab", "È´òÂû£ Ê•ì": "#47d7ac", "È∑πÂØåÂ£´ ËåÑÂ≠ê": "#5c068c", "Â§öÁî∞ ÊùéË°£Ëèú": "#0177c8", "Ê©ò „ÅÇ„Çä„Åô": "#5c88da", "ËæªÈáé „ÅÇ„Åã„Çä": "#e10600", "ÂçóÊù° ÂÖâ": "#e4012b", "Êñ∞Áî∞ ÁæéÊ≥¢": "#71c5e8", "‰∫åÂÆÆ È£õÈ≥•": "#60249f", "Êó©ÂùÇ ÁæéÁé≤": "#c701a0", "ÈÄüÊ∞¥ Â•è": "#033087", "‰πÖÂ∑ù Âá™": "#f8a3bc", "‰πÖÂ∑ù È¢Ø": "#7eddd3", "Êó•Èáé Ëåú": "#fa423a", "Ëó§Êú¨ ÈáåÂ•à": "#623b2a", "ÂèåËëâ Êùè": "#f8a3bc", "ÂåóÊù° Âä†ËìÆ": "#2ad2c9", "Â†Ä Ë£ïÂ≠ê": "#eca154", "ÂâçÂ∑ù „Åø„Åè": "#ce0037", "ÊùæÊ∞∏ Ê∂º": "#221651", "ÁöÑÂ†¥ Ê¢®Ê≤ô": "#e01a95", "ÂÆÆÊú¨ „Éï„É¨„Éá„É™„Ç´": "#a20067", "Âêë‰∫ï ÊãìÊµ∑": "#b0008e", "Ê£üÊñπ ÊÑõÊµ∑": "#c7579a", "Êùë‰∏ä Â∑¥": "#a5192e", "Ê£Æ‰πÖ‰øù ‰πÉ„ÄÖ": "#9cdbd9", "Ë´∏Êòü „Åç„Çâ„Çä": "#ffd100", "ÂÖ´Á•û „Éû„Ç≠„Éé": "#a6a4e0", "Â§ßÂíå ‰∫úÂ≠£": "#28724f", "ÁµêÂüé Êô¥": "#71dad4", "ÈÅä‰Ωê „Åì„Åö„Åà": "#f4a6d7", "‰æùÁî∞ Ëä≥‰πÉ": "#c4bcb7", "ÈæçÂ¥é Ëñ´": "#fae053", "ËÑáÂ±± Áè†Áæé": "#407ec8",
        },
        "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Ç∑„É£„Ç§„Éã„Éº„Ç´„É©„Éº„Ç∫": {
            "P": "#555555", "Ôº∞": "#555555",
            "Ê´ªÊú® Áúü‰πÉ": "#ffbad6", "È¢®Èáé ÁÅØÁπî": "#144384", "ÂÖ´ÂÆÆ „ÇÅ„Åê„Çã": "#ffe012",
            "ÊúàÂ≤° ÊÅãÈêò": "#f84cad", "Áî∞‰∏≠ Êë©Áæé„ÄÖ": "#a846fb", "ÁôΩÁÄ¨ Âí≤ËÄ∂": "#006047", "‰∏âÂ≥∞ ÁµêËèØ": "#3b91c4", "ÂπΩË∞∑ ÈúßÂ≠ê": "#d9f2ff",
            "Â∞èÂÆÆ ÊûúÁ©Ç": "#e5461c", "ÂúíÁî∞ Êô∫‰ª£Â≠ê": "#f93b90", "Ë•øÂüé Ê®πÈáå": "#ffc602", "ÊùúÈáé Âáõ‰∏ñ": "#89c3eb", "ÊúâÊ†ñÂ∑ù Â§èËëâ": "#90e667",
            "Â§ßÂ¥é ÁîòÂ•à": "#f54275", "Â§ßÂ¥é ÁîúËä±": "#e75bec", "Ê°ëÂ±± ÂçÉÈõ™": "#fafafa",
            "ËäπÊ≤¢ „ÅÇ„Åï„Å≤": "#f30100", "Èªõ ÂÜ¨ÂÑ™Â≠ê": "#5aff19", "ÂíåÊ≥â ÊÑõ‰æù": "#ff00ff",
            "ÊµÖÂÄâ ÈÄè": "#50d0d0", "Ê®ãÂè£ ÂÜÜÈ¶ô": "#be1e3e", "Á¶è‰∏∏ Â∞èÁ≥∏": "#7967c3", "Â∏ÇÂ∑ù ÈõõËèú": "#ffc639",
            "‰∏ÉËçâ „Å´„Å°„Åã": "#a6cdb6", "Á∑ãÁî∞ ÁæéÁê¥": "#760f10",
            "ÊñëÈ≥© „É´„Ç´": "#23120c", "‰∏ÉËçâ „ÅØ„Å•„Åç": "#8adfff",
        },
        "„Éñ„É´„Éº„Ç¢„Éº„Ç´„Ç§„Éñ": {
            "ÂÖàÁîü": "#555555",
            "Ê†óÊùë „Ç¢„Ç§„É™": "#4f423e", "È∞êÊ∏ï „Ç¢„Ç´„É™": "#faf1b8", "Â§©Èõ® „Ç¢„Ç≥": "#a9bfdf", "Â§©Á´• „Ç¢„É™„Çπ": "#414a61", "Èô∏ÂÖ´È≠î „Ç¢„É´": "#f9b5ca",
            "ÁôΩÁü≥ „Ç¶„Çø„Éè": "#e9d0f3",
            "ÊùèÂ±± „Ç´„Ç∫„Çµ": "#2a2839",
            "‰∏≠Âãô „Ç≠„É™„Éé": "#fbf8f4",
            "Á©∫‰∫ï „Çµ„Ç≠": "#a491ad", "Ê≠å‰Ωè „Çµ„ÇØ„É©„Ç≥": "#f1ece8",
            "Á†ÇÁãº „Ç∑„É≠„Ç≥": "#c5c4c8",
            "Ê°êËó§ „Éä„ÇÆ„Çµ": "#e4dad1", "ÊüöÈ≥• „Éä„ÉÑ": "#fdebe4",
            "ÁîüÂ°© „Éé„Ç¢": "#e8eff3",
            "‰ºäËçâ „Éè„É´„Ç´": "#534665", "ÈªíËàò „Éè„É´„Éä": "#ccc9d5",
            "ÊßåÊ∞∏ „Éí„É®„É™": "#d5f5e9",
            "ÊÑõÊ∏Ö „Éï„Ç¶„Ç´": "#423856",
            "Â∞èÈ≥•ÈÅä „Éõ„Ç∑„Éé": "#fbe0e7",
            "Â∞èÂ°ó „Éû„Ç≠": "#d05a5b", "‰ºäËêΩ „Éû„É™„Éº": "#fcd9a3",
            "ËÅñÂúí „Éü„Ç´": "#ffebf3", "ÊâçÁæΩ „Éü„Éâ„É™": "#fdd8a3", "ËøëË°õ „Éü„Éä": "#bfc6c4", "ËíºÊ£Æ „Éü„Éç": "#c7ddf7", "ÊúàÈõ™ „Éü„É§„Ç≥": "#f1f5f9", "ÈúûÊ≤¢ „Éü„É¶": "#787285",
            "ÊµÖÈªÑ „É†„ÉÑ„Ç≠": "#f0eeed",
            "È¢®ÂÄâ „É¢„Ç®": "#beadab", "ÊâçÁæΩ „É¢„É¢„Ç§": "#fddca6",
            "Êó©ÁÄ¨ „É¶„Ç¶„Ç´": "#615b96", "Ëä±Â≤° „É¶„Ç∫": "#fc7c82",
            "ÂÆáÊ≤¢ „É¨„Ç§„Çµ": "#fef3fb",
        },
        "Â≠¶Âúí„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº": { # from https://github.com/vertesan/gakumasu-diff/blob/main/CharacterColor.yaml
            "P": "#555555", "Ôº∞": "#555555",
            "Ëä±Êµ∑ Âí≤Â≠£": "#FF4F64",
            "ÊúàÊùë ÊâãÊØ¨": "#27B4EB",
            "Ëó§Áî∞ „Åì„Å®„Å≠": "#FFD203",
            "ÊúâÊùë È∫ªÂ§Æ": "#C45DC8",
            "ËëõÂüé „É™„Éº„É™„É§": "#D2E3E4",
            "ÂÄâÊú¨ ÂçÉÂ•à": "#FE8A22",
            "Á¥´Èõ≤ Ê∏ÖÂ§è": "#92DE5A",
            "ÁØ†Êæ§ Â∫É": "#00BED8",
            "Âß´Â¥é ËéâÊ≥¢": "#FD7EC2",
            "ÂçÅÁéã ÊòüÂçó": "#FFAC28",
            "Áß¶Ë∞∑ ÁæéÈà¥": "#6EA3FC",
            "Ëä±Êµ∑ ‰ΩëËäΩ": "#F74C2C",
            "Ê†πÁ∑í ‰∫úÁ¥óÈáå": "#988D83",
            "ÂçÅÁéã ÈÇ¶Â§´": "#988D83",
        },
    }

    _db = {} # same as _db0, but with better saturation and lightness, and uses rgb() format. also last name and name with spaces stripped as keys
    for series in _db0:
        _db[series] = {}
        for name in _db0[series]:
            color1   = _db0[series][name]
            match    = re.match(r"#(..)(..)(..)", re.sub(r"#(.)(.)(.)$", r"\1\1\2\2\3\3", color1))
            assert match, f"Invalid color {color1} (expected: #rrggbb or #rgb)"
            h,l,s    = colorsys.rgb_to_hls(*[int(x, 16) / 256 for x in match.groups()])
            r,g,b    = colorsys.hls_to_rgb(*[h, _lightness, _saturation] if s > 0.01 else [h, l, s])
            color2   = "rgb(%s)" % ",".join([str(int(x * 256)) for x in [r, g, b]])
            lastname = re.search(r"\S*$", name).group(0)
            nospname = re.sub(r"\s", "", name)
            _db[series][lastname] = color2
            _db[series][nospname] = color2

    @classmethod
    def colorHTML(self, html):
        # color character names starting a serifu (e.g. Â§™ÈÉé in "Â§™ÈÉé„Äå„Åì„Çì„Å´„Å°„ÅØ„Äç")

        # regex: 1 = line beginning, 2 = name, 3 = open paren etc.
        #         (1   )(2  )(3                   )
        regex = r"(^\s*)(.*?)([^\S\r\n]*[(Ôºà„Äå„ÄéÔΩ¢])" # ) dummy parent to fix indent

        # Find what series is this html (different serieses may have same name charas with different colors)
        # list of characters found in html (with multiplicity, excluding bad patterns)
        def isCharaName(s): return len(s) > 0 and (not s.startswith("‚Äï"))
        charaList = [x[1] for x in re.findall(regex, html, flags=re.MULTILINE) if isCharaName(x[1])]
        # get series with most matching names in charaList
        series = max(self._db.keys(), key = lambda series: len([s for s in charaList if s in self._db[series].keys()]))

        # If at most 1/2 of charaList will get colored, it's likely that series is incorrect
        # We don't have a db for the correct series
        if 0.5 * len(charaList) > len([s for s in charaList if s in self._db[series].keys()]):
            return html

        # Wrap with <span>
        def nosp(s): return s.replace(" ", "")
        def decorHTML(name):
            if not nosp(name) in self._db[series]: return name
            return f"<span class='name name_{nosp(name)}'>{name}</span>"

        # CSS
        style = "<style>\n.name { font-weight: bold }\n" + "\n".join([".name_%s { color: %s; }" % (nosp(name), color) for (name, color) in self._db[series].items() if name in charaList]) + "\n</style>\n"

        # HTML (modified)
        html = re.sub(regex, lambda m: m.group(1) + decorHTML(m.group(2)) + m.group(3), html, flags=re.MULTILINE)

        return style + html


### Misc mini functions

class HttpGet:

    def __init__(self):
        self.times = {}

    def tryDecode(self, data):
        for enc in ["utf-8", "shift-jis", "euc-jp", "cp932"]:
            try:
                return data.decode(enc)
            except UnicodeDecodeError:
                pass
        assert False, "Could not decode data"

    def __call__(self, url, fmt:"Literal['str','json','bytes']"="str", headers={}):
        # fmt: "str" (default), "json", "bytes"
        # headers: dict or list of dicts (later element har priority)

        assert fmt in ["str", "json", "bytes"]

        # %-encode chars not allowed in URI, see https://en.wikipedia.org/wiki/Percent-encoding
        regex = r"""[^][!"#$&'()*+,/:;=?@A-Za-z0-9_.~-]+"""
        for m in re.findall(regex, url):
            url = url.replace(m, urllib.parse.quote(m))

        # Rate limiting per domain:port
        loc = urllib.parse.urlparse(url).netloc
        y = max(self.times.setdefault(loc, 0), (x := time.time())) # send request at y
        self.times[loc] = y + 0.5 + min(3, y - x) # exponential wait time, at most 3.5 between requests
        # Sleep if needed
        if y - x > 0.1:
            logging.info(f"HttpGet: requests to the same domain {loc} in a short period, sleeping for {y-x}")
            time.sleep(y - x)

        # Merge headers
        if isinstance(headers, list):
            headers = dict(sum((list(x.items()) for x in headers), []))

        # Actually send request and catch error
        req = urllib.request.Request(url, headers=headers)
        try:
            res = urllib.request.urlopen(req)
        except (urllib.error.HTTPError, urllib.error.URLError, Exception) as e:
            raise e

        if res.status != 200:
            raise Exception(f"http non-200: {res.status}")

        # Decompress, decode, and optionally parse json (and html?)
        data = res.read()
        if "gzip" == ("Content-Encoding" in res.headers and res.headers["Content-Encoding"]):
            data = gzip.decompress(data) # may miss deflate and brotli
        if fmt == "bytes":
            return data
        data = self.tryDecode(data)
        if fmt == "json":
            return json.loads(data)
        elif fmt == "str":
            return data
        else:
            assert False, f"Invalid fmt: {fmt}"

httpGet = HttpGet()

def withFileCache(name, getDefault, expiry=600):
    # note: when cache is expired and getDefault fails, returns old cache
    # expiry is in seconds
    # getDefault should return json-serializable data
    cachedir = CONFIG["cachedir"]
    if not cachedir:
        return getDefault()
    if not re.match(r"^[a-zA-Z0-9-._]*$", name):
        raise Exception("Invalid cache name", name)
    if not os.path.isdir(cachedir):
        os.makedirs(cachedir, exist_ok=True)
    file = cachedir + os.sep + name
    # TODO avoid json encoding/decoding for strings and bytes
    def updateCache():
        value = getDefault()
        with open(file, "w") as f:
            json.dump(value, f, ensure_ascii=False, separators=(",", ":"))
        return value
    def readCache():
        with open(file, "r") as f:
            return json.load(f)
    try:
        if datetime.datetime.now().timestamp() - os.stat(file).st_mtime > expiry:
            try:
                value = updateCache()
                logging.debug("cache updating " + name)
            except:
                value = readCache()
                logging.debug("cache failed updating, using old " + name)
        else:
            value = readCache()
            logging.debug("cache is used " + name)
    except FileNotFoundError:
        value = updateCache()
        logging.debug("cache new item " + name)
    return value

class StringParser:

    def __init__(self, string:str):
        self.string = string
        self.cursor = 0

    def seek(self, sub:str):
        "seek to the next substring sub."
        ret = self.string.find(sub, self.cursor)
        if ret == -1:
            return -1
        self.cursor = ret + len(sub)

    def extract(self, *subs:str, default=None):
        "find substrings from subs and return string between the last two. no seek."
        p1, nt1, p2, nt2 = 0, 0, self.cursor, 0
        for t in subs:
            p1, nt1, p2, nt2 = p2, nt2, self.string.find(t, p2 + nt2), len(t)
            if p2 == -1:
                assert not (default is None), "matching failed and no default given"
                return default
        return self.string[p1+nt1:p2]

def replaceLinks(desc, addTag=False): # replace novel/xxxxx links and user/xxxxx links
    f1 = lambda m: mkurl("user", id=m[1])
    f2 = lambda m: mkurl("novel", id=m[1])
    g1 = lambda m: f'<a href="{mkurl("user", id=m[1])}">user/{m[1]}</a>'
    g2 = lambda m: f'<a href="{mkurl("novel", id=m[1])}">novel/{m[1]}</a>'
    for (regex, rep) in [
        (r"https://www.pixiv.net/users/([0-9]*)",              g1 if addTag else f1),
        (r"https://www.pixiv.net/novel/show.php\?id=([0-9]*)", g2 if addTag else f2),
    ]:
        desc = re.sub(regex, rep, desc)
    return desc

def getRSign(spec):
    "Get canonical rating sign"
    mapping = {
        0: "", "0": "", "": "",
        1: "R ", "1": "R ", "r": "R ", "R": "R ", "R ": "R ",
        2: "G ", "2": "G ", "g": "R ", "G": "R ", "G ": "R ",
    }
    return mapping[spec]

def percentEncode(word):
    return urllib.parse.quote_plus(word, encoding="utf-8")

def mkurl(*args, **kwargs):
    return "/" + "/".join(percentEncode(str(v)) for v in args if v) + ("?" if kwargs else "") + "&".join(f"{k}={percentEncode(str(v))}" for k, v in kwargs.items() if v)

def addMissingCloseTags(html, tags=["b", "s", "u", "strong"]):

    # "aaa<b"  ==>  "aaa"
    m = re.search("<[^>]*$", html)
    if m:
        html = html[:m.start(0)]

    # Count opened counts for each tag in `tags`
    counts = {}
    for m in re.finditer(r"<\s*(/?)\s*(" + "|".join(tags) + r")\s*>", html):
        t = m.group(2)
        if m.group(1) == "/":
            counts[t] = (counts[t] - 1) if t in counts else -1
        else:
            counts[t] = (counts[t] + 1) if t in counts else 1

    # For each tag, if opened count > closed count then add close tags
    for t in counts:
        if counts[t] > 0:
            html += ("</" + t + ">") * counts[t]

    return html

def readCookiestxtAsHTTPCookieHeader(cookiestxt, domain):
    # Read Netscape HTTP Cookie File and return string for urllib request header
    #   urllib.request.Request(url, headers={"Cookie": ...})
    # Only matching domains will be extracted (if domain=="a.com" then www.a.com, .a.com etc will match)

    try:
        with open(cookiestxt) as f:
            results = []

            for line in f:
                # Skip empty or comment lines
                if re.match(r"^\s*$", line) or re.match(r"^# ", line):
                    continue
                fields = line[:-1].split('\t')
                # Each line must have 7 fields and has the specified
                if len(fields) == 7 and (not domain or fields[0].find(domain) != -1):
                    results += [ f"{fields[5]}={fields[6]}" ]

            logging.info(f"Loaded {len(results)} cookies for {domain} from {cookiestxt}")
            return "; ".join(results) # something like "name=val; name=val"

    except OSError as e:
        logging.warning("Could not read cookies.txt; R-18 search results will be omitted!")
        return False

def openInBrowser(url):
    if shutil.which("termux-open-url"):
        subprocess.run(["termux-open-url", url])
    else:
        webbrowser.open(url)

def yesterday():
    return datetime.date.today() - datetime.timedelta(days = 1)

def saveFile(name, text, maxLenBytes=os.pathconf('/', 'PC_NAME_MAX'), prefix="", suffix=""):
    def fsSafeChars(s):
        return re.sub(r'[\\/*<>|]', " ", s).replace('"', "‚Äù").replace(":", "Ôºö").replace("?", "Ôºü")
    def trunc(s, lenBytes, suffix=""):
        # Tuncate s so that (s+suffix).encode("utf-8") has at most lenBytes bytes, and return s+suffix
        # Will not chop in the middle of byte-sequence representing single unicode character
        lenS       = len(s)
        # lenSU      = len(s.encode("utf-8"))
        lenSuffixU = len(suffix.encode("utf-8"))
        i          = lenS
        while len(s[:i].encode("utf-8")) + lenSuffixU > lenBytes:
            i -= 1
        return s[:i] + suffix
    outfile = trunc(fsSafeChars(prefix + name), maxLenBytes, fsSafeChars(suffix))
    savedir = CONFIG["savedir"]
    if not os.path.isdir(savedir): os.makedirs(savedir, exist_ok=True)
    with open(savedir + os.sep + outfile, "w") as f: f.write(text)
    return outfile

def sfind(string:str, tokens:list[str]):
    # find str
    p1, nt1, p2, nt2 = 0, 0, 0, 0
    for t in tokens:
        p1, nt1, p2, nt2 = p2, nt2, string.find(t, p2 + nt2), len(t)
    return string[p1+nt1:p2]

def loadPlugins(plugDir):
    if not os.path.isdir(plugDir):
        raise Exception(f"plugin directory {plugDir} does not exist")
    import glob, importlib.util
    logging.debug(f"Searching plugins in {plugDir}")
    for plugFile in glob.glob("plugin-*.py", root_dir=plugDir):
        logging.debug(f"Loaded plugin from {plugFile}")
        module_name = plugFile[2:-3]
        file_path = os.path.join(plugDir, plugFile)
        # https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly
        spec = importlib.util.spec_from_file_location(module_name, file_path)
        module = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = module
        spec.loader.exec_module(module)


### test

def test():
    import subprocess as sp, time, urllib.error as ue, urllib.parse as up, urllib.request as ur
    port = 8001
    proc = sp.Popen(["python", "pixiv-novel.py", "-p", str(port), "-c", ""])
    fail = 0

    def test(path):
        # test http status and response length
        # if success, show time taken for request+response
        # if success return 0, if fail return 1
        time.sleep(1)
        t1 = datetime.datetime.now().timestamp()
        try:
            res = ur.urlopen(f"http://localhost:{port}{path}")
        except ue.HTTPError as e:
            print(f"FAIL {path}   ", e)
            return 1
        data = res.read()
        t2 = datetime.datetime.now().timestamp()
        if len(data) < 1000:
            print(f"FAIL {path}   ", "response too short", len(data))
            return 1
        print(f"OK   {path}   {round(t2-t1,3)}s")
        return 0

    try:
        fail += test("/")
        fail += test("/novel?id=15898879")
        fail += test("/search?q=" + up.quote("Ëëó‰ΩúÊ®©„Éï„É™„Éº"))
        fail += test("/user?id=15370995")
    finally:
        proc.kill()
        exit(fail)


### Main

def main():

    global CONFIG

    ## Parse args
    parser = argparse.ArgumentParser(description="Start web server to view pixiv novels.")
    A = parser.add_argument
    D1 = " (default: %(default)s)"
    D2 = " (default: %(default)s; set '' to disable)"
    A("-b", "--bind",     metavar="ADDR", default="0.0.0.0", help="Bind to this address" + D1)
    A("-c", "--cachedir", metavar="DIR", default="_cache",   help="Cache directory" + D2)
    A("-d", "--download", metavar="URL",                     help="Download a novel and exit")
    A("-k", "--cookie",   default="cookies.txt",             help="Path of cookies.txt" + D2)
    A("-l", "--plugdir",  metavar="DIR", default="",         help="Plugin directory" + D2)
    A("-p", "--port",     type=int, default=8080,            help="Port number" + D1)
    A("-s", "--savedir",  metavar="DIR", default="_save",    help="Auto save novels in this directory" + D2)
    A("-v", "--verbose",  action="store_true",               help="Verbose mode")
    A("--browser", action="store_true", help="Open in browser")
    A("--nocolor", action="store_true", help="Disable character name colors")
    A("--noimage", action="store_true", help="Disable image embedding")
    A("--sslcert", help="HTTPS cert file")
    A("--sslkey",  help="HTTPS key file")
    A("--test",    action="store_true")
    # parser.add_argument("--nor18", action="store_true", help="Disable R18.")
    args = parser.parse_args()

    # Save some options on global
    CONFIG["cachedir"] = args.cachedir
    CONFIG["nocolor"]  = args.nocolor
    CONFIG["savedir"]  = args.savedir
    CONFIG["noimage"]  = args.noimage

    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(name)s:%(thread)d:%(message)s', level=logging.DEBUG if args.verbose else logging.ERROR)

    if args.plugdir:
        loadPlugins(args.plugdir)

    # Readme
    # - Check out: cool reader (android app)
    # - cookies.txt
    # - how to visit server

    # Download novel and exit.
    if args.download:
        novelID = re.match(r"[0-9]*", args.download).group()
        if not novelID:
            logging.error("Invalid url")
        else:
            f = Fetch(novelID)
            outfile = f.save()
            print("Written to " + outfile, flush=True)
        exit()

    # Try read cookies.txt
    # To obtain cookies.txt, use https://addons.mozilla.org/ja/firefox/addon/cookies-txt/ or https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid or type document.cookie in devtool
    if args.cookie:
        Resources.Pixiv.cookie = readCookiestxtAsHTTPCookieHeader(args.cookie, "pixiv.net")

    # Test code
    if args.test:
        test()
        quit()

    # Check HTTPS support
    if args.sslcert and args.sslkey:
        serverHttps = True
        serverCert  = args.sslcert
        serverKey   = args.sslkey
    elif args.sslcert or args.sslkey:
        raise Exception("Specify both --sslcert and --sslkey for HTTPS support.")
    else:
        serverHttps = serverCert = serverKey = None

    # Run server
    serverHost = args.bind
    serverPort = args.port

    serverThread = threading.Thread(
        None,
        target=run_threaded_https_server,
        args=(MyRequestHandler, serverHost, serverPort, serverHttps, serverCert, serverKey),
        daemon=True)
    serverThread.start()

    serverUrl = f"http{'s' if serverHttps else ''}://{serverHost}:{serverPort}"
    print(f"Serving at {serverUrl}", flush=True)

    # Open in browser
    if args.browser:
        openInBrowser(serverUrl)

    try:
        serverThread.join()
    except KeyboardInterrupt:
        pass

if __name__ == "__main__":
    main()
