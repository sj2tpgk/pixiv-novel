#!/usr/bin/env python

from http.server import BaseHTTPRequestHandler, HTTPServer
from html.parser import HTMLParser
import argparse
import colorsys
import datetime
import json
import os
import random
import re
import shutil
import subprocess
import urllib.parse
import urllib.request
import webbrowser

# base
# TODO no-r mode
# TODO help page on how to get cookies.txt
# TODO html extractor: get innerHTML? (do ranking descriptions contain html tags??)
# TODO other websites? (and rename to ja-novels.py)
# TODO chara name db in separate files (one file per each series, and embed on build)
# TODO url like /search/abc/6?compact=... (for better history integration)

# as a cli util
# TODO json api (like &json=1, for cli, also for test automation)


### Configuration

cookie = None  # Cookie string
save   = False # Save visited novels?
color  = False # colorize character names?

def hasCookie(): return bool(cookie)

emoji = { "love": "üíô", "search": "üîç" }


### Server

class MyServer(BaseHTTPRequestHandler):
    def log_message(self, format, *args):
        return # supress logs

    def do_GET(self):

        def sendHTML(html):
            self.send_response(200)
            self.send_header("Content-type", "text/html")
            self.end_headers()
            try:
                self.wfile.write(bytes(html, "utf-8"))
            except BrokenPipeError:
                # print("BrokenPipeError")
                return

        params = urllib.parse.parse_qs(self.path[2:])
        # print(self.requestline, params)

        def param(name, default=None):
            if not name in params:
                return default
            else:
                return params[name][0]

        def getHTML():
            cmd = param("cmd", "ranking")
            if cmd == "fetch":
                if not (novelID := param("id")):
                    return "missing id"
                else:
                    f = Fetch(novelID)
                    if save:
                        f.save()
                    return f.html()
            elif cmd in ["search", "searchuser", "ranking"]:
                if cmd == "search":
                    s = Search(param("q", ""), param("bookmarks", 0), param("page", "1"), param("npages", 1))
                elif cmd == "searchuser":
                    s = SearchUser(param("q", ""), param("bookmarks", 0))
                elif cmd == "ranking":
                    s = SearchRanking(param("mode", "daily"), param("date", ""))
                return s.html(param("compact", 1))
            else:
                return "unknown cmd"

        if "favicon" in self.requestline:
            sendHTML("")
        else:
            # sendHTML(getHTML())
            try:
                sendHTML(getHTML())
            except Exception as e:
                print ("Error occured\n" + str(e))
                return "Error occured<br>" + str(e)


### Search

def searchEntry(title, tags, id, description, xRestrict, bookmarkCount, textCount):
    # Abstracts (1) response json format and (2) difference between websites
    return {
            "title"         : title,
            "tags"          : tags,
            "id"            : id,
            "description"   : description,
            "xRestrict"     : xRestrict,
            "bookmarkCount" : bookmarkCount,
            "textCount"     : textCount,
            }

def html_searchBar(simple=True, query="", compact=False, bookmarks=0, npages=1):
    html = f"""<form style="text-align: right; line-height: 1.5">
<input type="text" name="q" placeholder="Ê§úÁ¥¢" value="{query}">
<input type="submit" value="{emoji['search']}">
<input type="hidden" name="cmd" value="search">
<input type="hidden" name="compact" value="{1 if compact else ''}">
"""
    if not simple:
        html += f"""<br>
<select name="npages">
<option value="1"{" selected" if npages == 1 else ""}>1„Éö„Éº„Ç∏„Åö„Å§
<option value="2"{" selected" if npages == 2 else ""}>2„Éö„Éº„Ç∏„Åö„Å§
<option value="3"{" selected" if npages == 3 else ""}>3„Éö„Éº„Ç∏„Åö„Å§
</select>
{emoji['love']}:<input type="text" name="bookmarks" value="{bookmarks}" size="3">
"""
    html += "</form>"
    return html

class Search():

    # Usage: print(Search("abc",10,1,1).html())

    def __init__(self, query, bookmarkCount, page, npages):
        self._novels = []

        self._query = query
        self._bookmarkCount = int(bookmarkCount)
        self._page = int(page)
        self._npages = int(npages)

        self._doSearch()

    def _doSearch(self):
        dataList = self._getDataList()
        self._novels = [
                searchEntry(x["title"], x["tags"], x["id"], x["description"], x["xRestrict"], x["bookmarkCount"], x["textCount"])
                for x in dataList
                if int(x["bookmarkCount"]) >= self._bookmarkCount
                ]

    def _getDataList(self):
        self._wordEscaped = percentEncode(self._query).replace("+", "%20")
        dataList = []
        for i in range(self._npages):
            url1 = f"{self._wordEscaped}?word={self._wordEscaped}&order=date_d&mode=all&p={i+self._page}&s_mode=s_tag&lang=ja"
            url = f"https://www.pixiv.net/ajax/search/novels/{url1}"
            resJson = myRequest(url, toJson=True)
            dataList += resJson["body"]["novel"]["data"]
        return dataList

    def html(self, compact):
        compact = compact == "1"

        # common css
        css = """body { max-width: 750px; margin: 1em auto; padding: 0 .5em; }
#main { margin: 1em 0 }
li p { margin: 1.5em 0 }
td:not(:nth-child(4)) { text-align: center; padding: 0 .35em }
td:nth-child(4)       { padding-left: 2em }"""

        # novels
        novels = "<table>" if compact else "<ul>"
        for x in self._novels:
            href = f"/?cmd=fetch&id={x['id']}"
            if compact:
                novels += f"<tr><td><a href=\"{href}\">{x['id']}</a></td><td>{getRSign(x['xRestrict'])}</td><td>{x['bookmarkCount']}</td><td>{x['title']}</td></tr>\n"
            else:
                desc = "<br>".join(replaceLinks(x["description"]).split("<br />")[0:5])
                desc = addMissingCloseTags(desc, tags=["b", "s", "u", "strong"])
                tags = ", ".join([f"<a href=\"/?cmd=search&q={percentEncode(t)}\">{t}</a>" for t in x["tags"]])
                novels += f"<li>{x['title']} ({x['textCount']}Â≠ó) <a href=\"{href}\">[{x['id']}]</a><p>{desc}</p>{emoji['love']} {x['bookmarkCount']}<br>{tags}</li><hr>\n"
        novels += "</table>" if compact else "</ul>"

        # final html
        return f"""<!DOCTYPE html>
<html lang="ja">
<head>
{self._html_head()}
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
{css}
</style>
</head>
<body>
{self._html_header(compact)}
{self._html_nav(compact)}
<div id="main">
{novels}
</div>
{self._html_nav(compact)}
</body>
</html>"""

    def _html_head(self):
        return f"<title>Search {self._query}{(f' ({self._page})' if self._page > 1 else '')}</title>"

    def _html_header(self, compact):
        return f"""<h1>Search {self._query}{(f' ({self._page})' if self._page > 1 else '')}</h1>
{html_searchBar(simple=False, query=self._query, compact=compact, bookmarks=self._bookmarkCount, npages=self._npages)}
<hr>"""

    def _html_nav(self, compact):
        # navigation links (on both top and bottom of page)
        paramCompact    = "&compact=1" if     compact else ""
        paramCompactNeg = "&compact=1" if not compact else ""
        commonParams = f"?cmd=search&q={self._wordEscaped}&npages={self._npages}&bookmarks={self._bookmarkCount}"
        hrefPrev = f"/{commonParams}&page={max(1,self._page-self._npages)}{paramCompact}"
        hrefNext = f"/{commonParams}&page={self._page+self._npages}{paramCompact}"
        hrefToggle = f"/{commonParams}&page={self._page}{paramCompactNeg}"
        return f"""<div id="nav" style="display: flex">
<span style="flex: 1">
<a href="{hrefToggle}">{compact and "Ë©≥Á¥∞Ë°®Á§∫" or "„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫"}</a>
</span>
<span style="flex: 1"></span>
<span style="flex: 1; text-align: right">
<a href='{hrefPrev}'>Ââç„Å∏</a>
<a href='{hrefNext}'>Ê¨°„Å∏</a>
</span>
</div>"""

class SearchUser(Search):

    def __init__(self, userID, bookmarkCount):
        self._novels = []

        self._userID = userID
        self._bookmarkCount = int(bookmarkCount)

        self._doSearch()

    def _getDataList(self):
        dataList = []

        # First, response includes all novelIDs of user
        url1 = f"https://www.pixiv.net/ajax/user/{self._userID}/profile/all?lang=ja"
        json1 = myRequest(url1, toJson=True)
        novelIDs = list(json1["body"]["novels"].keys())

        # Next, get data for each novel (100 novels at once)
        dataList = []
        n = 100
        for ids in [novelIDs[n*i:n*(i+1)] for i in range(1+int(len(novelIDs)/n))]:
            idsParams = "&".join([f"ids[]={x}" for x in ids])
            url2 = f"https://www.pixiv.net/ajax/user/{self._userID}/profile/novels?{idsParams}"
            json2 = myRequest(url2, toJson=True)
            dataList += list(json2["body"]["works"].values())

        return dataList

    def _html_head(self):
        return f"<title>Search {self._userID}</title>"

    def _html_header(self, compact):
        return f"""<h1>Search {self._userID}</h1>
{html_searchBar(simple=True, query="", compact=compact)}
<hr>"""

    def _html_nav(self, compact):
        hrefToggle = f"/?cmd=searchuser&q={self._userID}{'&compact=1' if not compact else ''}"
        return f"<a href='{hrefToggle}'>{compact and 'Ë©≥Á¥∞Ë°®Á§∫' or '„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫'}</a>"

class SearchRanking(Search):

    _modeNames = {
            "daily":           "„Éá„Ç§„É™„Éº",
            "weekly":          "„Ç¶„Ç£„Éº„ÇØ„É™„Éº",
            "monthly":         "„Éû„É≥„Çπ„É™„Éº",
            "rookie":          "„É´„Éº„Ç≠„Éº",
            "weekly_original": "„Ç™„É™„Ç∏„Éä„É´",
            "male":            "Áî∑Â≠ê„Å´‰∫∫Ê∞ó",
            "female":          "Â•≥Â≠ê„Å´‰∫∫Ê∞ó",
            "daily_r18":       "„Éá„Ç§„É™„Éº R-18",
            "weekly_r18":      "„Ç¶„Ç£„Éº„ÇØ„É™„Éº R-18",
            "male_r18":        "Áî∑Â≠ê„Å´‰∫∫Ê∞ó R-18",
            "female_r18":      "Â•≥Â≠ê„Å´‰∫∫Ê∞ó R-18",
            }

    def __init__(self, mode, date):
        self._novels = []

        self._mode = mode

        # set self._date to a date object (at most yesterday)
        if re.match(r"\d\d\d\d-\d\d-\d\d", date):
            d = datetime.date.fromisoformat(date)
            y = yesterday()
            self._date = d if d <= y else y
        else:
            self._date = yesterday()

        self._doSearch()

    def _doSearch(self):
        dataList = self._getDataList()
        self._novels = [
                searchEntry(x["title"], x["tags"], x["id"], x["description"], x["xRestrict"], x["bookmarkCount"], x["textCount"])
                for x in dataList
                ]

    def _getDataList(self):
        url = f"https://www.pixiv.net/novel/ranking.php?mode={self._mode}&date={self._date.isoformat().replace('-', '')}"
        # return self._getDataListFromHTML("".join(open("../r_daily","r").readlines()))
        dataList = []
        for page in [1, 2]:
            res = myRequest(url + ("&page=2" if page == 2 else ""))
            dataList += self._getDataListFromHTML(res)
        return dataList

    def _getDataListFromHTML(self, html):
        # searchEntry(title, tags, id, description, xRestrict, bookmarkCount, textCount):
        data = []
        current = None
        def done():
            nonlocal current, data
            if current:
                data += [searchEntry(**current)]
            current = {}
        def setp(prop, val):
            current[prop] = val
        def onStart(match, attr):
            if match("._ranking-item"):
                done()
                setp("xRestrict", "r18" in self._mode)
                setp("description", "") # some novels don't have description
            elif match(".cover"):
                setp("title", re.sub(r"/.*", "", attr("alt")))
                setp("tags",  attr("data-tags").split())
                setp("id",    attr("data-id"))
        def onData(match, data):
            if match(".bookmark-count"):
                setp("bookmarkCount", re.sub(r"[^\d]", "", data))
            elif match(".chars"):
                setp("textCount", re.sub(r"[^\d]", "", data))
            elif match(".novel-caption"):
                setp("description", data)

        MyHTMLParser(onStart, onData).feed(html)
        done()
        # print(json.dumps([[x["title"]] for x in data], indent=2, sort_keys=True, ensure_ascii=False))
        return data

    def _html_head(self):
        return f"<title>{self._modeNames[self._mode]} „É©„É≥„Ç≠„É≥„Ç∞ {self._date}</title>"

    def _html_header(self, compact):
        # links for other rankings
        hrefBase = f"/?cmd=ranking{'&compact=1' if compact else ''}&date={self._date}"
        modeLinks1 = "\n".join([f"""<a href="{hrefBase}&mode={mode}"{ ' class="ranking-selected"' if mode == self._mode else ""}>{self._modeNames[mode]}</a>""" for mode in self._modeNames.keys() if not "r18" in mode])
        if hasCookie():
            modeLinks2 = "<span>R-18:</span>"
            modeLinks2 += "\n".join([f"""<a href="{hrefBase}&mode={mode}"{ ' class="ranking-selected"' if mode == self._mode else ""}>{self._modeNames[mode].replace(" R-18", "")}</a>""" for mode in self._modeNames.keys() if "r18" in mode])
        else:
            modeLinks2 = "R-18 „É©„É≥„Ç≠„É≥„Ç∞„ÇíË¶ã„Çã„Å´„ÅØ cookies.txt „ÅåÂøÖË¶Å„Åß„Åô„ÄÇ"
        modeLinks = """<style>
#ranking-modes { margin: .8em 0; font-size: small; line-height: 1.8 }
#ranking-modes a { margin-right: 1.4em }
#ranking-modes span { margin-right: 1.0em }
.ranking-selected { font-weight: bold }
</style>""" + f"""
<p id="ranking-modes">
{modeLinks1}
<br>
{modeLinks2}
</p>"""

        # construct html
        return f"""<h1>{self._modeNames[self._mode]} „É©„É≥„Ç≠„É≥„Ç∞ {self._date}</h1>
{html_searchBar(simple=True, query="", compact=compact)}
{modeLinks}
<hr>
<form style="text-align: center; margin: .5em 0">
<label for="date">Êó•‰ªò:</label>
<input type="date" id="date" name="date" value="{self._date}" max="{yesterday()}">
<input type="submit" value="{emoji['search']}">
<input type="hidden" name="cmd" value="ranking">
<input type="hidden" name="mode" value="{self._mode}">
<input type="hidden" name="compact" value="{1 if compact else ''}">
</form>"""

    def _html_nav(self, compact):
        hrefToggle = f"/?cmd=ranking&q={self._mode}{'&compact=1' if not compact else ''}&date={self._date}"
        return f"<a href='{hrefToggle}'>{compact and 'Ë©≥Á¥∞Ë°®Á§∫' or '„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫'}</a>"

class MyHTMLParser(HTMLParser):
    # Usage: MyHTMLParser(onStart, onData).feed(html)
    # On each handle_starttag, onStart(match, attr) is called, where match(query) checks if the tag matches the CSS query, and attr(attrName) is like getAttribute(attrName)
    # On each handle_data, onData(match, data) is called, where match(query) is the same above, and data is the same as in handle_data
    def __init__(self, onStart, onData):
        super().__init__()
        self._stack = []
        self._onStart = onStart
        self._onData = onData
    def _attr(self, attr):
        if len(self._stack) == 0: return
        for (k, v) in self._stack[-1]["attrs"]:
            if k == attr:
                return v
    def _match(self, query):
        if not re.match(r"^\.[^\s.]*$", query):
            print("Query must be like .CLASS")
            return
        return re.sub(r"\.", "", query) in (self._attr("class") or "").split()
    def handle_starttag(self, tag, attrs):
        self._stack.append({ "tag": tag, "attrs": attrs })
        self._onStart(self._match, self._attr)
        if tag in ["img"]:
            self._stack.pop()
    def handle_endtag(self, tag):
        self._stack.pop()
    def handle_data(self, data):
        self._onData(self._match, data)


### Fetch

class Fetch():

    def __init__(self, novelID):
        self._novelID = novelID
        self._data = self._getData()
        self._html = None

    def _getData(self):
        url = "https://www.pixiv.net/novel/show.php?id=%s" % self._novelID
        html = myRequest(url)
        return self._extractData(html)

    def html(self):
        if self._html: return self._html

        ## Construct html

        data = self._getData()

        o_css = """body { max-width: 700px; margin: 1em auto; padding: 0 .5em; }
@media screen and (max-aspect-ratio: .75) and (max-width: 13cm) { /* Mobile */
    body { max-width: 100%; margin: 0.5em 0.5em }
}
#novel { line-height: 1.9; border-bottom: solid #888 1px; margin-bottom: 2em; padding-bottom: 3em; }
#data { display: none }"""

        o_rSign = getRSign(data["xRestrict"])
        o_title = data["title"]
        o_description = replaceLinks(data["description"])
        o_content = data["content"]
        for (regex, replace) in [
                (r"$", "<br>"),
                (r"\[newpage\]", "<hr>\n"),
                (r"\[chapter:(.*?)\]", "<h2>\\1</h2>\n"),
                (r"\[\[rb:(.*?)(>|&gt;)(.*?)\]\]", "<ruby>\\1<rt>\\3</rt></ruby>"),
                ]:
            o_content = re.sub(regex, replace, o_content, flags=re.MULTILINE)

        # uploadedimage
        def getImageTag(novelImageId):
            url = data["textEmbeddedImages"][novelImageId]["urls"]["original"]
            return f"""<figure>
<a href="{url}">
<img src=\"{url}\" alt=\"[uploadedimage:{novelImageId}\" style=\"width: 100%\">
</a>
</figure>"""
        o_content = re.sub(r"\[uploadedimage:([0-9]*)(.*?)\]", lambda m: getImageTag(m.group(1)), o_content)

        # pixivimage (not simple to retrieve images)
        o_content = re.sub(r"\[pixivimage:([0-9]*)(.*?)\]", "<a href=\"https://www.pixiv.net/artworks/\\1\">ÁîªÂÉè \\1\\2</a>", o_content)

        # colorize character names
        if color:
            o_content = CharaColor.colorHTML(o_content)

        # embed json
        # „ÉÄ„Éñ„É´„ÇØ„Ç™„Éº„Éà„ÇíÊÆã„Åó„ÄÅÂÖ®‰Ωì„Çí„Ç∑„É≥„Ç∞„É´„ÇØ„Ç™„Éº„Éà„ÅßÂõ≤„Åø„ÄÅensure_ascii=False „Çí‰Ωø„ÅÜ„Å®„ÄÅÂúßÁ∏Æ„Åß„Åç„Çã
        # „Å®„ÅØ„ÅÑ„Åàjson„Å™„Åó„Çà„Çä„ÅØ„Åã„Å™„ÇäÂ§ß„Åç„ÅÑ (47k vs 114k vs 171k)
        o_json = json.dumps(data, ensure_ascii=False, separators=(',', ':'))
        for (fromStr, toStr) in [
                ("'", "&#39;"), # ("\"", "&quot;"), ("<", "&lt;"), (">", "&gt;")
                ]:
            o_json = o_json.replace(fromStr, toStr)

        o_tags = ",\n".join(map(lambda y: f"<a href='/?cmd=search&q={percentEncode(y)}'>{y}</a>", [x["tag"] for x in data["tags"]["tags"]]))
        o_info = f"""<p>
„Çø„Ç∞:
{o_tags}
</p>
<p>
<a href="https://www.pixiv.net/novel/show.php?id={data["id"]}">Pixiv„ÅßÈñã„Åè</a>
ID:{data["id"]}
U:<a href="/?cmd=searchuser&q={data["userId"]}">{data["userId"]}</a>
B:{data["bookmarkCount"]}
D:{data["createDate"][:10]}
</p>"""

        self._html = f"""<!DOCTYPE html>
<html lang="ja">
<head>
<title>{o_rSign}{o_title}</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<!--<link rel="stylesheet" href="_style.css">-->
<style>
{o_css}
</style>
</head>
<body>
<h1>{o_title}</h1>
<div id="novel">
{o_content}
</div>
<div id="info">
<p>
{o_description}
</p>
{o_info}
</div>
<div id="data" data-novels='{o_json}'></div>
</body>
</html>"""

        return self._html

    def save(self): # Save to file (will return filename)
        def trunc(s, lenBytes, suffix=""):
            # Tuncate s so that (s+suffix).encode("utf-8") has at most lenBytes bytes, and return s+suffix
            # Will not chop in the middle of byte-sequence representing single unicode character
            lenS       = len(s)
            lenSU      = len(s.encode("utf-8"))
            lenSuffixU = len(suffix.encode("utf-8"))
            i          = lenS
            while len(s[:i].encode("utf-8")) + lenSuffixU > lenBytes:
                i -= 1
            return s[:i] + suffix
        title = self._data["title"].replace("/", " ")
        outPrefix = f"{getRSign(self._data['xRestrict'])}"
        outSuffix = f" - pixiv - {self._data['id']}.html"
        outfile = trunc(outPrefix + title, os.pathconf('/', 'PC_NAME_MAX'), outSuffix)
        open(outfile, "w").write(self.html())
        return outfile

    def _extractData(self, html): # parse show.php and get json inside meta[name=preload-data]
        # Extract json string
        s = None
        def onStart(match, attr):
            nonlocal s
            if attr("name") == "preload-data":
                s = attr("content")
        MyHTMLParser(onStart, lambda match, data: 0).feed(html)

        # Extract part of json
        json1 = json.loads(s)
        return json1["novel"][list(json1["novel"].keys())[0]]


### Character name colorizer

class CharaColor:
    _lightness = 0.52
    _saturation = 0.7

    _db0 = { # _db0[series][characterName] = color (format: "#xxxxxx" or "#xxx")
            # Idolmaster (https://imas-db.jp/misc/color.html) (imcomplete, and not very strict)
            "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Éü„É™„Ç™„É≥„É©„Ç§„Éñ": {
                "P": "#555555", "Ôº∞": "#555555",
                "Â§©Êµ∑ Êò•È¶ô": "#e22b30", "Â¶ÇÊúà ÂçÉÊó©": "#2743d2", "Ëê©Âéü Èõ™Ê≠©": "#d3dde9", "È´òÊßª „ÇÑ„Çà„ÅÑ": "#f39939", "ÁßãÊúà ÂæãÂ≠ê": "#01a860", "‰∏âÊµ¶ „ÅÇ„Åö„Åï": "#9238be", "Ê∞¥ÁÄ¨ ‰ºäÁπî": "#fd99e1", "ËèäÂú∞ Áúü": "#515558", "ÂèåÊµ∑ ‰∫úÁæé": "#ffe43f", "ÂèåÊµ∑ ÁúüÁæé": "#ffe43f", "Êòü‰∫ï ÁæéÂ∏å": "#b4e04b", "ÊàëÈÇ£Ë¶á Èüø": "#01adb9", "ÂõõÊù° Ë≤¥Èü≥": "#a6126a", "Èü≥ÁÑ° Â∞èÈ≥•": "#00ff00",
                "‰ºäÂêπ Áøº": "#fed552", " „Ç®„Éü„É™„Éº": "#554171", "Â§ßÁ•û Áí∞": "#ee762e", "Êò•Êó• Êú™Êù•": "#ea5b76", "Âåó‰∏ä È∫óËä±": "#6bb6b0", "ÂåóÊ≤¢ Âøó‰øù": "#afa690", "Êú®‰∏ã „Å≤„Å™„Åü": "#d1342c", "È´òÂùÇ Êµ∑Áæé": "#e9739b", "Ê°úÂÆà Ê≠åÁπî": "#274079", "‰ΩêÁ´π ÁæéÂ•àÂ≠ê": "#58a6dc", "ÁØ†ÂÆÆ ÂèØÊÜê": "#b63b40", "Â≥∂Âéü „Ç®„É¨„Éä": "#9bce92", " „Ç∏„É•„É™„Ç¢": "#d7385f", "ÁôΩÁü≥ Á¥¨": "#ebe1ff", "Âë®Èò≤ Ê°ÉÂ≠ê": "#efb864", "È´òÂ±± Á¥ó‰ª£Â≠ê": "#7f6575", "Áî∞‰∏≠ Áê¥Ëëâ": "#92cfbb", "Â§©Á©∫Ê©ã ÊúãËä±": "#bee3e3", "Âæ≥Â∑ù „Åæ„Å§„Çä": "#5abfb7", "ÊâÄ ÊÅµÁæé": "#454341", "Ë±äÂ∑ù È¢®Ëä±": "#7278a8", "‰∏≠Ë∞∑ ËÇ≤": "#f7e78e", "Ê∞∏Âêâ Êò¥": "#aeb49c", "‰∏ÉÂ∞æ ÁôæÂêàÂ≠ê": "#c7b83c", "‰∫åÈöéÂ†Ç ÂçÉÈ∂¥": "#f19557", "Èáé„ÄÖÂéü Ëåú": "#eb613f", "ÁÆ±Â¥é ÊòüÊ¢®Ëä±": "#ed90ba", "È¶¨Â†¥ „Åì„ÅÆ„Åø": "#f1becb", "Á¶èÁî∞ „ÅÆ„ÇäÂ≠ê": "#eceb70", "ËàûÊµú Ê≠©": "#e25a9b", "ÁúüÂ£Å ÁëûÂ∏å": "#99b7dc", "ÊùæÁî∞ ‰∫úÂà©Ê≤ô": "#b54461", "ÂÆÆÂ∞æ Áæé‰πü": "#d7a96b", "ÊúÄ‰∏ä ÈùôÈ¶ô": "#6495cf", "ÊúõÊúà ÊùèÂ•à": "#7e6ca8", "ÁôæÁÄ¨ ËéâÁ∑í": "#f19591", "Áü¢Âêπ ÂèØÂ•à": "#f5ad3b", "Ê®™Â±± Â•àÁ∑í": "#788bc5", " „É≠„Ç≥": "#fff03c",
                "ÈùíÁæΩ ÁæéÂí≤": "#57c7c4",
                " Áé≤Èü≥": "#512aa3", " Ë©©Ëä±": "#e6f9e5", "Â••Á©∫ ÂøÉÁôΩ": "#fefad4",
                "Êó•È´ò ÊÑõ": "#e85786", "Ê∞¥Ë∞∑ ÁµµÁêÜ": "#00adb9", "ÁßãÊúà Ê∂º": "#b2d468",
                },
            "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Ç∑„É≥„Éá„É¨„É©„Ç¨„Éº„É´„Ç∫": {
                "P": "#555555", "Ôº∞": "#555555",
                " „Ç¢„Éä„Çπ„Çø„Ç∑„Ç¢": "#b1c9e8", "ÂèäÂ∑ù Èõ´": "#f4f9ff", "Â§ßÊßª ÂîØ": "#f6be00", "‰πôÂÄâ ÊÇ†Ë≤¥": "#e3bec3", "ÂñúÂ§öË¶ã Êüö": "#f0ec74", "Ê°êÁîü „Å§„Åã„Åï": "#ab4ec6", "Â∞èÊó•Âêë ÁæéÁ©Ç": "#db3fb1", "È´òÊ£Æ ËóçÂ≠ê": "#ceea80", "ÈÅìÊòéÂØ∫ Ê≠åÈà¥": "#d22730", " „Éä„Çø„Éº„É™„Ç¢": "#f5633a", "Èõ£Ê≥¢ Á¨ëÁæé": "#e13c30", "ÊµúÂè£ „ÅÇ„ÇÑ„ÇÅ": "#450099", "Âß´Â∑ù ÂèãÁ¥Ä": "#ee8b00", "Ëó§Âéü ËÇá": "#9595d2", "ÊòüËºù Â≠ê": "#a6093d", "Êú¨Áî∞ Êú™Â§Æ": "#feb81c", "‰∏âËàπ ÁæéÂÑ™": "#12bfb2", "‰∏âÊùë „Åã„Å™Â≠ê": "#feb1bb", "Â§¢Ë¶ã „Çä„ÅÇ„ÇÄ": "#e59bdc",
                "Áõ∏Ëëâ Â§ïÁæé": "#f1e991", "Ëµ§Âüé „Åø„Çä„ÅÇ": "#ffcd00", "ÊµÖÂà© ‰∏ÉÊµ∑": "#009cbc", "ÂÆâÈÉ® Ëèú„ÄÖ": "#ef4b81", "ËçíÊú® ÊØîÂ•à": "#a0d884", "‰∏Ä„ÉéÁÄ¨ ÂøóÂ∏å": "#a50050", "Á∑íÊñπ Êô∫ÁµµÈáå": "#6cc24a", "ÁâáÊ°ê Êó©Ëãó": "#dc4404", "‰∏äÊù° Êò•Ëèú": "#5ac2e7", "Á•ûË∞∑ Â•àÁ∑í": "#9678d3", "Â∑ùÂ≥∂ ÁëûÊ®π": "#485cc7", "Á•ûÂ¥é Ëò≠Â≠ê": "#84329b", "ÂñúÂ§ö Êó•ËèúÂ≠ê": "#fcd757", "Êú®Êùë Â§èÊ®π": "#2d2926", "ÈªíÂüº „Å°„Å®„Åõ": "#ef3340", "Â∞èÈñ¢ È∫óÂ•à": "#9b26b6", "Â∞èÊó©Â∑ù Á¥óÊûù": "#e56db1", "Ë•øÂúíÂØ∫ Áê¥Ê≠å": "#e8cdd0", "È∑∫Ê≤¢ ÊñáÈ¶ô": "#606eb2", "‰Ωê‰πÖÈñì „Åæ„ÇÜ": "#da1984", "‰Ωê„ÄÖÊú® ÂçÉÊûù": "#0072ce", "‰ΩêÂüé Èõ™Áæé": "#171c8f", "Ê§éÂêç Ê≥ïÂ≠ê": "#f8485e", "Â°©Ë¶ã Âë®Â≠ê": "#dce6ed", "Ê∏ãË∞∑ Âáõ": "#0f9dde", "Â≥∂Êùë ÂçØÊúà": "#f67499", "Âüé„É∂Â¥é ÁæéÂòâ": "#fe9d1a", "Âüé„É∂Â¥é ËéâÂòâ": "#fedd00", "ÁôΩËèä „Åª„Åü„Çã": "#c964cf", "ÁôΩÂùÇ Â∞èÊ¢Ö": "#abcae9", "ÁôΩÈõ™ ÂçÉÂ§ú": "#efd7e5", "Á†ÇÂ°ö „ÅÇ„Åç„Çâ": "#7e93a7", "Èñ¢ Ë£ïÁæé": "#ffb3ab", "È´òÂû£ Ê•ì": "#47d7ac", "È∑πÂØåÂ£´ ËåÑÂ≠ê": "#5c068c", "Â§öÁî∞ ÊùéË°£Ëèú": "#0177c8", "Ê©ò„ÅÇ „Çä„Åô": "#5c88da", "ËæªÈáé „ÅÇ„Åã„Çä": "#e10600", "ÂçóÊù° ÂÖâ": "#e4012b", "Êñ∞Áî∞ ÁæéÊ≥¢": "#71c5e8", "‰∫åÂÆÆ È£õÈ≥•": "#60249f", "Êó©ÂùÇ ÁæéÁé≤": "#c701a0", "ÈÄüÊ∞¥ Â•è": "#033087", "‰πÖÂ∑ù Âá™": "#f8a3bc", "‰πÖÂ∑ù È¢Ø": "#7eddd3", "Êó•Èáé Ëåú": "#fa423a", "Ëó§Êú¨ ÈáåÂ•à": "#623b2a", "ÂèåËëâ Êùè": "#f8a3bc", "ÂåóÊù° Âä†ËìÆ": "#2ad2c9", "Â†Ä Ë£ïÂ≠ê": "#eca154", "ÂâçÂ∑ù „Åø„Åè": "#ce0037", "ÊùæÊ∞∏ Ê∂º": "#221651", "ÁöÑÂ†¥ Ê¢®Ê≤ô": "#e01a95", "ÂÆÆÊú¨ „Éï„É¨„Éá„É™„Ç´": "#a20067", "Âêë‰∫ï ÊãìÊµ∑": "#b0008e", "Ê£üÊñπ ÊÑõÊµ∑": "#c7579a", "Êùë‰∏ä Â∑¥": "#a5192e", "Ê£Æ‰πÖ‰øù ‰πÉ„ÄÖ": "#9cdbd9", "Ë´∏Êòü „Åç„Çâ„Çä": "#ffd100", "ÂÖ´Á•û „Éû„Ç≠„Éé": "#a6a4e0", "Â§ßÂíå ‰∫úÂ≠£": "#28724f", "ÁµêÂüé Êô¥": "#71dad4", "ÈÅä‰Ωê „Åì„Åö„Åà": "#f4a6d7", "‰æùÁî∞ Ëä≥‰πÉ": "#c4bcb7", "ÈæçÂ¥é Ëñ´": "#fae053", "ËÑáÂ±± Áè†Áæé": "#407ec8",
                },
            "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Ç∑„É£„Ç§„Éã„Éº„Ç´„É©„Éº„Ç∫": {
                "P": "#555555", "Ôº∞": "#555555",
                "Ê´ªÊú® Áúü‰πÉ": "#ffbad6", "È¢®Èáé ÁÅØÁπî": "#144384", "ÂÖ´ÂÆÆ „ÇÅ„Åê„Çã": "#ffe012",
                "ÊúàÂ≤° ÊÅãÈêò": "#f84cad", "Áî∞‰∏≠ Êë©Áæé„ÄÖ": "#a846fb", "ÁôΩÁÄ¨ Âí≤ËÄ∂": "#006047", "‰∏âÂ≥∞ ÁµêËèØ": "#3b91c4", "ÂπΩË∞∑ ÈúßÂ≠ê": "#d9f2ff",
                "Â∞èÂÆÆ ÊûúÁ©Ç": "#e5461c", "ÂúíÁî∞ Êô∫‰ª£Â≠ê": "#f93b90", "Ë•øÂüé Ê®πÈáå": "#ffc602", "ÊùúÈáé Âáõ‰∏ñ": "#89c3eb", "ÊúâÊ†ñÂ∑ù Â§èËëâ": "#90e667",
                "Â§ßÂ¥é ÁîòÂ•à": "#f54275", "Â§ßÂ¥é ÁîúËä±": "#e75bec", "Ê°ëÂ±± ÂçÉÈõ™": "#fafafa",
                "ËäπÊ≤¢ „ÅÇ„Åï„Å≤": "#f30100", "Èªõ ÂÜ¨ÂÑ™Â≠ê": "#5aff19", "ÂíåÊ≥â ÊÑõ‰æù": "#ff00ff",
                "ÊµÖÂÄâ ÈÄè": "#50d0d0", "Ê®ãÂè£ ÂÜÜÈ¶ô": "#be1e3e", "Á¶è‰∏∏ Â∞èÁ≥∏": "#7967c3", "Â∏ÇÂ∑ù ÈõõËèú": "#ffc639",
                "‰∏ÉËçâ „Å´„Å°„Åã": "#a6cdb6", "Á∑ãÁî∞ ÁæéÁê¥": "#760f10",
                "ÊñëÈ≥© „É´„Ç´": "#23120c", "‰∏ÉËçâ „ÅØ„Å•„Åç": "#8adfff",
                }
            }

    _db = {} # same as _db0, but with better saturation and lightness, and uses rgb() format. also last name and name with spaces stripped as keys
    for series in _db0:
        _db[series] = {}
        for name in _db0[series]:
            color1   = _db0[series][name]
            match    = re.match(r"#(..)(..)(..)", re.sub(r"#(.)(.)(.)$", r"\1\1\2\2\3\3", color1))
            h,l,s    = colorsys.rgb_to_hls(*[int(x, 16) / 256 for x in match.groups()])
            r,g,b    = colorsys.hls_to_rgb(*[h, _lightness, _saturation] if s > 0.01 else [h, l, s])
            color2   = "rgb(%s)" % ",".join([str(int(x * 256)) for x in [r, g, b]])
            lastname = re.search(r"\S*$", name).group(0)
            nospname = re.sub(r"\s", "", name)
            _db[series][lastname] = color2
            _db[series][nospname] = color2

    @classmethod
    def colorHTML(self, html):
        # color character names starting a serifu (e.g. Â§™ÈÉé in "Â§™ÈÉé„Äå„Åì„Çì„Å´„Å°„ÅØ„Äç")

        # regex: 1 = line beginning, 2 = name, 3 = open paren etc.
        #         (1   )(2  )(3                   )
        regex = r"(^\s*)(.*?)([^\S\r\n]*[(Ôºà„Äå„ÄéÔΩ¢])"

        # Find what series is this html (different serieses may have same name charas with different colors)
        # list of characters found in html (with multiplicity, excluding bad patterns)
        def isCharaName(s): return len(s) > 0 and (not s.startswith("‚Äï"))
        charaList = [x[1] for x in re.findall(regex, html, flags=re.MULTILINE) if isCharaName(x[1])]
        # get series with most matching names in charaList
        series = max(self._db.keys(), key = lambda series: len([s for s in charaList if s in self._db[series].keys()]))

        # If at most 1/2 of charaList will get colored, it's likely that series is incorrect
        # We don't have a db for the correct series
        if 0.5 * len(charaList) > len([s for s in charaList if s in self._db[series].keys()]):
            return html

        # Wrap with <span>
        def nosp(s): return s.replace(" ", "")
        def decorHTML(name):
            if not nosp(name) in self._db[series]: return name
            return f"<span class='name name_{nosp(name)}'>{name}</span>"

        # CSS
        style = "<style>\n.name { font-weight: bold }\n" + "\n".join([".name_%s { color: %s; }" % (nosp(name), color) for (name, color) in self._db[series].items() if name in charaList]) + "\n</style>\n"

        # HTML (modified)
        html = re.sub(regex, lambda m: m.group(1) + decorHTML(m.group(2)) + m.group(3), html, flags=re.MULTILINE)

        return style + html


### Misc mini functions

def myRequest(url, toJson=False):
    # %-encode non-ascii chars
    regex = r'[^\x00-\x7F]'
    for m in re.findall(regex, url):
        url = url.replace(m, urllib.parse.quote_plus(m, encoding="utf-8"))

    headers = { # 404 without these
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100",
            "Host": "www.pixiv.net",
            "Accept": "application/json",
            "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
            "Referer": "https://www.pixiv.net/",
            "x-user-id": str(random.randrange(0, 100000000)),
            "Connection": "keep-alive",
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-origin",
            "TE": "trailers",
            }

    global cookie
    if cookie:
        headers["Cookie"] = cookie

    import urllib.error
    try:
        req = urllib.request.Request(url, headers=headers)
        out = urllib.request.urlopen(req).read().decode("utf-8")
        return json.loads(out) if toJson else out
    except urllib.error.HTTPError as e:
        print("Error when downloading from Pixiv, possibly cookies.txt is outdated.")
        print("HTTPError: ", e)
    except urllib.error.URLError as e:
        print("URLError: ", e)
    except Exception as e:
        print(e)

def replaceLinks(desc): # replace novel/xxxxx links and user/xxxxx links
    for (regex, rep) in [
            (r"https://www.pixiv.net/users/([0-9]*)", "/?cmd=searchuser&q=\\1"),
            (r"https://www.pixiv.net/novel/show.php\?id=([0-9]*)", "/?cmd=fetch&id=\\1"),
            ]:
        desc = re.sub(regex, rep, desc)
    return desc

def getRSign(xRestrict):
    return ["", "R ", "G "][int(xRestrict)]

def percentEncode(word):
    return urllib.parse.quote_plus(word, encoding="utf-8")

def addMissingCloseTags(html, tags=["b", "s", "u", "strong"]):

    # "aaa<b"  ==>  "aaa"
    m = re.search("<[^>]*$", html)
    if m:
        html = html[:m.start(0)]

    # Count opened counts for each tag in `tags`
    counts = {}
    for m in re.finditer(r"<\s*(/?)\s*(" + "|".join(tags) + r")\s*>", html):
        t = m.group(2)
        if m.group(1) == "/":
            counts[t] = (counts[t] - 1) if t in counts else -1
        else:
            counts[t] = (counts[t] + 1) if t in counts else 1

    # For each tag, if opened count > closed count then add close tags
    for t in counts:
        if counts[t] > 0:
            html += ("</" + t + ">") * counts[t]

    return html

def readCookiestxtAsCookieHeader():
    # Read Netscape HTTP Cookie File and return string for urllib request header
    #   urllib.request.Request(url, headers={"Cookie": ...})

    cookiestxt = "cookies.txt"

    try:
        with open(cookiestxt) as f:
            results = []

            for line in f:
                # Skip empty or comment lines
                if re.match(r"^\s*$", line) or re.match(r"^# ", line):
                    continue
                fields = line[:-1].split('\t')
                # Each line must have 7 fields and domain name must be pixiv related
                if len(fields) == 7 and fields[0].find("pixiv.net") != -1:
                    results += [ f"{fields[5]}={fields[6]}" ]

            # Output will be like "name=val; name=val"
            return "; ".join(results)

    except OSError as e:
        print("Could not read cookies.txt; R-18 search results will be omitted!")
        return False

def openInBrowser(url):
    if shutil.which("termux-open-url"):
        subprocess.run(["termux-open-url", url])
    else:
        webbrowser.open(url)

def yesterday():
    return datetime.date.today() - datetime.timedelta(days = 1)


### Main

if __name__ == "__main__":

    ## Parse args
    parser = argparse.ArgumentParser(description="Start web server to view pixiv novels. Load cookies.txt if found in current dir.")
    parser.add_argument("-a", "--autosave", action="store_true", help="Enable autosave; save visited novels as files.")
    parser.add_argument("-B", "--nobrowser", action="store_true", help="Don't open browser.")
    parser.add_argument("-C", "--nocolor", action="store_true", help="Disable character name colors.")
    parser.add_argument("-d", "--download", type=str, metavar="URL", help="Download a novel and exit.")
    parser.add_argument("-p", "--port", type=int, default=8080, help="Port number. (default: 8080)")
    # parser.add_argument("-R", "--nor18", action="store_true", help="Disable R18.")

    # Save some options on global
    args = parser.parse_args()
    save = args.autosave
    color = not args.nocolor

    # Readme
    # - Check out: cool reader (android app)
    # - cookies.txt
    # - how to visit server

    # Download novel and exit.
    if args.download:
        novelID = re.match(r"[0-9]*", args.download).group()
        if not novelID:
            print("Invalid url")
        else:
            f = Fetch(novelID)
            outfile = f.save()
            print(outfile)
        exit()

    # Try read cookies.txt
    # To obtain cookies.txt, use https://addons.mozilla.org/ja/firefox/addon/cookies-txt/ or https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid or type document.cookie in devtool
    cookie = readCookiestxtAsCookieHeader()

    # Run server
    hostName = "0.0.0.0" # Also accessible from other computers
    serverPort = args.port
    webServer = HTTPServer((hostName, serverPort), MyServer)
    serverUrl = "http://%s:%s" % (hostName, serverPort)
    print("Server at %s" % serverUrl)

    # Open in browser
    if not args.nobrowser:
        openInBrowser(serverUrl)

    try:
        webServer.serve_forever()
    except KeyboardInterrupt:
        pass

    webServer.server_close()

