#!/usr/bin/env python3

import argparse
import base64
import colorsys
import dataclasses
import datetime
import gzip
import html.parser
import http.server
import json
import logging
import os
import re
import shutil
import subprocess
import sys
import threading
import traceback
import time
import urllib.error, urllib.parse, urllib.request
import webbrowser
from typing import *

# base
# TODO custom style (e.g. load _style.css)
# TODO no-r mode
# TODO help page on how to get cookies.txt
# TODO html extractor: get innerHTML? (do ranking descriptions contain html tags??)
# TODO other websites? (and rename to ja-novels.py)
# TODO chara name db in separate files (one file per each series, and embed on build)
# TODO url like /search/abc/6?compact=... (for better history integration)

# as a cli util
# TODO json api (like &json=1, for cli, also for test automation)


### Configuration

cachedir = ""     # Cache directory ('' to disable)
color    = False  # Colorize character names?
savedir  = ""     # Save novels directory ('' to disable)

emoji = { "love": "üíô", "search": "üîç" }


### HTTP Server

def run_threaded_https_server(RequestHandlerClass, host="0.0.0.0", port=8030, https=False, certfile="", keyfile=""):
    """Run http server with threading and ssl support.
if `https` is true, (`certfile`, `keyfile`) is passed to load_cert_chain.
Use `openssl req -new -x509 -keyout CERTFILE -out KEYFILE -days 365 -nodes`."""
    import http.server, ssl
    def ssl_wrap(httpd, certfile, keyfile):
        # https://gist.github.com/DannyHinshaw/a3ac5991d66a2fe6d97a569c6cdac534
        ctx = ssl.SSLContext(protocol=ssl.PROTOCOL_TLS_SERVER)
        ctx.load_cert_chain(certfile=certfile, keyfile=keyfile)
        httpd.socket = ctx.wrap_socket(httpd.socket, server_side=True)
    httpd = http.server.ThreadingHTTPServer((host, port), RequestHandlerClass)
    if https:
        ssl_wrap(httpd, certfile, keyfile)
    httpd.serve_forever()

class MyRequestHandler(http.server.BaseHTTPRequestHandler):
    def log_message(self, format, *args):
        cli = self.client_address
        logging.debug(("%s:%s " + format) % (cli[0], cli[1], *args))

    def sendHTML(self, html):
        gz = "gzip" in (self.headers["Accept-Encoding"] or "")
        data = bytes(html, "utf-8")
        if gz: data = gzip.compress(data)
        self.send_response(200)
        self.send_header("Content-type", "text/html")
        if gz: self.send_header("Content-Encoding", "gzip")
        self.send_header("Content-Length", str(len(data)))
        self.end_headers()
        try:
            self.wfile.write(data)
        except BrokenPipeError:
            logging.warning("BrokenPipeError")
            return

    def do_GET(self):

        parsed = urllib.parse.urlparse(self.path)
        paths  = [x for x in parsed.path.split("/") if x]
        param  = { k: v[0] for k, v in urllib.parse.parse_qs(parsed.query).items() }.get

        try:
            html = ""
            err = b"400 Bad Request"
            if len(paths) == 0:
                s = SearchRanking(param("mode", "daily"), param("date", ""))
                html = s.html(param("compact", 1))
            elif len(paths) == 1:
                if paths[0] == "ranking":
                    s = SearchRanking(param("mode", "daily"), param("date", ""))
                    html = s.html(param("compact", 1))
                elif paths[0] == "search":
                    s = Search(param("q", ""), param("bookmarks", 0), param("page", "1"), param("npages", 1))
                    html = s.html(param("compact", 1))
                elif paths[0] == "user":
                    s = SearchUser(param("id", ""), param("bookmarks", 0))
                    html = s.html(param("compact", 1))
                elif paths[0] == "novel":
                    if not (novelID := param("id")):
                        err = b"400 Bad Request: missing id"
                    else:
                        f = Fetch(novelID)
                        if savedir:
                            f.save()
                        html = f.html()
            if html:
                self.sendHTML(html)
            else:
                self.send_response(400)
                self.send_header("Content-Length", str(len(err)))
                self.end_headers()
                self.wfile.write(err)
        except Exception as e:
            print(traceback.format_exc(), file=sys.stderr)
            logging.error("Error occured\n" + str(e))


### Search

def searchEntry(title, tags, id, description, xRestrict, bookmarkCount, textCount):
    # Abstracts (1) response json format and (2) difference between websites
    return {
            "title"         : title,
            "tags"          : tags,
            "id"            : id,
            "description"   : description,
            "xRestrict"     : xRestrict,
            "bookmarkCount" : bookmarkCount,
            "textCount"     : textCount,
            }

def html_searchBar(simple=True, query="", compact=False, bookmarks=0, npages=1):
    html = f"""<form style="text-align: right; line-height: 1.5" action=search>
<input type="text" name="q" placeholder="Ê§úÁ¥¢" value="{query}">
<input type="submit" value="{emoji['search']}">
<input type="hidden" name="compact" value="{1 if compact else ''}">
"""
    if not simple:
        html += f"""<br>
<select name="npages">
<option value="1"{" selected" if npages == 1 else ""}>1„Éö„Éº„Ç∏„Åö„Å§
<option value="2"{" selected" if npages == 2 else ""}>2„Éö„Éº„Ç∏„Åö„Å§
<option value="3"{" selected" if npages == 3 else ""}>3„Éö„Éº„Ç∏„Åö„Å§
</select>
{emoji['love']}:<input type="text" name="bookmarks" value="{bookmarks}" size="3">
"""
    html += "</form>"
    return html

class Search():

    # Usage: print(Search("abc",10,1,1).html())

    def __init__(self, query, bookmarkCount, page, npages):
        self._novels = []

        self._query = query
        self._bookmarkCount = int(bookmarkCount)
        self._page = int(page)
        self._npages = int(npages)

        self._doSearch()

    def _doSearch(self):
        dataList = self._getDataList()
        self._novels = [
                searchEntry(x["title"], x["tags"], x["id"], x["description"], x["xRestrict"], x["bookmarkCount"], x["textCount"])
                for x in dataList
                if int(x["bookmarkCount"]) >= self._bookmarkCount
                ]

    def _getDataList(self):
        dataList = []
        for i in range(self._npages):
            resJson = Resources.Pixiv.jsonSearch(self._query, i+self._page)
            dataList += resJson["body"]["novel"]["data"]
        return dataList

    def html(self, compact):
        compact = compact == "1"

        # common css
        css = """body { max-width: 750px; margin: 1em auto; padding: 0 .5em; }
#main { margin: 1em 0 }
li p { margin: 1.5em 0 }
td:not(:nth-child(4)) { text-align: center; padding: 0 .35em }
td:nth-child(4)       { padding-left: 2em }"""

        # novels
        novels = "<table>" if compact else "<ul>"
        for x in self._novels:
            href = mkurl("novel", id=x['id'])
            if compact:
                novels += f"<tr><td><a href=\"{href}\">{x['id']}</a></td><td>{getRSign(x['xRestrict'])}</td><td>{x['bookmarkCount']}</td><td>{x['title']}</td></tr>\n"
            else:
                desc = "<br>".join(replaceLinks(x["description"]).split("<br />")[0:5])
                desc = addMissingCloseTags(desc, tags=["b", "s", "u", "strong"])
                tags = ", ".join([f"<a href=\"{mkurl('search', q=t)}\">{t}</a>" for t in x["tags"]])
                novels += f"<li>{x['title']} ({x['textCount']}Â≠ó) <a href=\"{href}\">[{x['id']}]</a><p>{desc}</p>{emoji['love']} {x['bookmarkCount']}<br>{tags}</li><hr>\n"
        novels += "</table>" if compact else "</ul>"

        # final html
        return f"""<!DOCTYPE html>
<html lang="ja">
<head>
{self._html_head()}
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
{css}
</style>
</head>
<body>
{self._html_header(compact)}
{self._html_nav(compact)}
<div id="main">
{novels}
</div>
{self._html_nav(compact)}
</body>
</html>"""

    def _html_head(self):
        return f"<title>Search {self._query}{(f' ({self._page})' if self._page > 1 else '')}</title>"

    def _html_header(self, compact):
        return f"""<h1>Search {self._query}{(f' ({self._page})' if self._page > 1 else '')}</h1>
{html_searchBar(simple=False, query=self._query, compact=compact, bookmarks=self._bookmarkCount, npages=self._npages)}
<hr>"""

    def _html_nav(self, compact):
        # navigation links (on both top and bottom of page)
        common = { "q": self._query, "npages": self._npages, "bookmarks": self._bookmarkCount }
        hrefPrev   = mkurl("search", **common, page=max(1,self._page-self._npages), compact=compact)
        hrefNext   = mkurl("search", **common, page=self._page+self._npages,        compact=compact)
        hrefToggle = mkurl("search", **common, page=self._page,                     compact=int(not compact))
        return f"""<div id="nav" style="display: flex">
<span style="flex: 1">
<a href="{hrefToggle}">{compact and "Ë©≥Á¥∞Ë°®Á§∫" or "„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫"}</a>
</span>
<span style="flex: 1"></span>
<span style="flex: 1; text-align: right">
<a href='{hrefPrev}'>Ââç„Å∏</a>
<a href='{hrefNext}'>Ê¨°„Å∏</a>
</span>
</div>"""

class SearchUser(Search):

    def __init__(self, userID, bookmarkCount):
        self._novels = []

        self._userID = userID
        self._bookmarkCount = int(bookmarkCount)

        self._doSearch()

    def _getDataList(self):

        # First, response includes all novelIDs of user
        json1 = Resources.Pixiv.jsonUserAll(self._userID)
        novels = json1["body"]["novels"] # a dict or a empty list
        if len(novels) == 0:
            return []
        novelIDs = list(novels.keys())

        # Next, get data for each novel (100 novels at once)
        # So, 0 novel = 0 request, 1-100 novels = 1 request, 101-200 = 2 etc.
        dataList = []
        n = 100
        numRequests = 1 + int((len(novelIDs)-1)/n)
        for ids in [novelIDs[n*i:n*(i+1)] for i in range(numRequests)]:
            json2 = Resources.Pixiv.jsonUserNovels(self._userID, ids)
            dataList += list(json2["body"]["works"].values())

        return dataList

    def _html_head(self):
        return f"<title>Search {self._userID}</title>"

    def _html_header(self, compact):
        return f"""<h1>Search {self._userID}</h1>
{html_searchBar(simple=True, query="", compact=compact)}
<hr>"""

    def _html_nav(self, compact):
        hrefToggle = mkurl("user", id=self._userID, compact=int(not compact))
        return f"<a href='{hrefToggle}'>{compact and 'Ë©≥Á¥∞Ë°®Á§∫' or '„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫'}</a>"

class SearchRanking(Search):

    _modeNames = {
            "daily":           "„Éá„Ç§„É™„Éº",
            "weekly":          "„Ç¶„Ç£„Éº„ÇØ„É™„Éº",
            "monthly":         "„Éû„É≥„Çπ„É™„Éº",
            "rookie":          "„É´„Éº„Ç≠„Éº",
            "weekly_original": "„Ç™„É™„Ç∏„Éä„É´",
            "male":            "Áî∑Â≠ê„Å´‰∫∫Ê∞ó",
            "female":          "Â•≥Â≠ê„Å´‰∫∫Ê∞ó",
            "daily_r18":       "„Éá„Ç§„É™„Éº R-18",
            "weekly_r18":      "„Ç¶„Ç£„Éº„ÇØ„É™„Éº R-18",
            "male_r18":        "Áî∑Â≠ê„Å´‰∫∫Ê∞ó R-18",
            "female_r18":      "Â•≥Â≠ê„Å´‰∫∫Ê∞ó R-18",
            }

    def __init__(self, mode, date):
        self._novels = []

        self._mode = mode

        # set self._date to a date object (at most yesterday)
        if re.match(r"\d\d\d\d-\d\d-\d\d", date):
            d = datetime.date.fromisoformat(date)
            y = yesterday()
            self._date = d if d <= y else y
        else:
            self._date = yesterday()

        self._doSearch()

    def _doSearch(self):
        dataList = withFileCache(
            f"pixiv-ranking-{self._mode}-{self._date.isoformat().replace('-', '')}",
            self._getDataList, 3600)
        self._novels = [
                searchEntry(x["title"], x["tags"], x["id"], x["description"], x["xRestrict"], x["bookmarkCount"], x["textCount"])
                for x in dataList
                ]

    def _getDataList(self):
        # return self._getDataListFromHTML("".join(open("../r_daily","r").readlines()))
        dataList = []
        for page in [1, 2]:
            res = Resources.Pixiv.rankingPhp(self._mode, self._date.isoformat(), page)
            dataList += self._getDataListFromHTML(res)
        return dataList

    def _getDataListFromHTML(self, html):
        # searchEntry(title, tags, id, description, xRestrict, bookmarkCount, textCount):
        data = []
        current = None
        def done():
            nonlocal current, data
            if current:
                data += [searchEntry(**current)]
            current = {}
        def setp(prop, val):
            current[prop] = val
        def onStart(match, attr):
            if match("._ranking-item"):
                done()
                setp("xRestrict", "r18" in self._mode)
                setp("description", "") # some novels don't have description
            elif match(".cover"):
                setp("title", re.sub(r"/.*", "", attr("alt")))
                setp("tags",  attr("data-tags").split())
                setp("id",    attr("data-id"))
        def onData(match, data):
            if match(".bookmark-count"):
                setp("bookmarkCount", re.sub(r"[^\d]", "", data))
            elif match(".chars"):
                setp("textCount", re.sub(r"[^\d]", "", data))
            elif match(".novel-caption"):
                setp("description", data)

        MyHTMLParser(onStart, onData).feed(html)
        done()
        # print(json.dumps([[x["title"]] for x in data], indent=2, sort_keys=True, ensure_ascii=False))
        return data

    def _html_head(self):
        return f"<title>{self._modeNames[self._mode]} „É©„É≥„Ç≠„É≥„Ç∞ {self._date}</title>"

    def _html_header(self, compact):
        # links for other rankings
        hrefBase = mkurl("ranking", compact=compact, date=self._date)
        modeLinks1 = "\n".join([f"""<a href="{hrefBase}&mode={mode}"{ ' class="ranking-selected"' if mode == self._mode else ""}>{self._modeNames[mode]}</a>""" for mode in self._modeNames.keys() if not "r18" in mode])
        if Resources.Pixiv.hasCookie():
            modeLinks2 = "<span>R-18:</span>"
            modeLinks2 += "\n".join([f"""<a href="{hrefBase}&mode={mode}"{ ' class="ranking-selected"' if mode == self._mode else ""}>{self._modeNames[mode].replace(" R-18", "")}</a>""" for mode in self._modeNames.keys() if "r18" in mode])
        else:
            modeLinks2 = "R-18 „É©„É≥„Ç≠„É≥„Ç∞„ÇíË¶ã„Çã„Å´„ÅØ cookies.txt „ÅåÂøÖË¶Å„Åß„Åô„ÄÇ"
        modeLinks = """<style>
#ranking-modes { margin: .8em 0; font-size: small; line-height: 1.8 }
#ranking-modes a { margin-right: 1.4em }
#ranking-modes span { margin-right: 1.0em }
.ranking-selected { font-weight: bold }
</style>""" + f"""
<p id="ranking-modes">
{modeLinks1}
<br>
{modeLinks2}
</p>"""

        # construct html
        return f"""<h1>{self._modeNames[self._mode]} „É©„É≥„Ç≠„É≥„Ç∞ {self._date}</h1>
{html_searchBar(simple=True, query="", compact=compact)}
{modeLinks}
<hr>
<form style="text-align: center; margin: .5em 0" action=ranking>
<label for="date">Êó•‰ªò:</label>
<input type="date" id="date" name="date" value="{self._date}" max="{yesterday()}">
<input type="submit" value="{emoji['search']}">
<input type="hidden" name="mode" value="{self._mode}">
<input type="hidden" name="compact" value="{1 if compact else ''}">
</form>"""

    def _html_nav(self, compact):
        hrefToggle = mkurl("ranking", q=self._mode, compact=int(not compact), date=self._date)
        return f"<a href='{hrefToggle}'>{compact and 'Ë©≥Á¥∞Ë°®Á§∫' or '„Ç≥„É≥„Éë„ÇØ„ÉàË°®Á§∫'}</a>"

class MyHTMLParser(html.parser.HTMLParser):
    # Usage: MyHTMLParser(onStart, onData).feed(html)
    # On each handle_starttag, onStart(match, attr) is called, where match(query) checks if the tag matches the CSS query, and attr(attrName) is like getAttribute(attrName)
    # On each handle_data, onData(match, data) is called, where match(query) is the same above, and data is the same as in handle_data
    def __init__(self, onStart, onData):
        super().__init__()
        self._stack = []
        self._onStart = onStart
        self._onData = onData
    def _attr(self, attr):
        if len(self._stack) == 0: return
        for (k, v) in self._stack[-1]["attrs"]:
            if k == attr:
                return v
    def _match(self, query):
        if not re.match(r"^\.[^\s.]*$", query):
            logging.error("Query must be like .CLASS")
            return
        return re.sub(r"\.", "", query) in (self._attr("class") or "").split()
    def handle_starttag(self, tag, attrs):
        self._stack.append({ "tag": tag, "attrs": attrs })
        self._onStart(self._match, self._attr)
        if tag in ["img"]:
            self._stack.pop()
    def handle_endtag(self, tag):
        self._stack.pop()
    def handle_data(self, data):
        self._onData(self._match, data)


### Fetch

class Fetch():

    def __init__(self, novelID):
        self._novelID = novelID
        self._data = self._getData()
        self._html = None

    def _getData(self):
        data = withFileCache(
            f"pixiv-showPhp-{self._novelID}",
            lambda: self._extractData(Resources.Pixiv.showPhp(self._novelID)),
            expiry=3*86400
        )
        return data

    def html(self):
        if self._html: return self._html

        data = self._data

        o_content = data["content"]
        for (regex, replace) in [
                (r"$", "<br>"),
                (r"\[newpage\]", "<hr>\n"),
                (r"\[chapter:(.*?)\]", "<h2>\\1</h2>\n"),
                (r"\[\[rb:(.*?)(>|&gt;)(.*?)\]\]", "<ruby>\\1<rt>\\3</rt></ruby>"),
                ]:
            o_content = re.sub(regex, replace, o_content, flags=re.MULTILINE)

        # uploadedimage
        def getUploadedImageTag(novelImageId):
            url = data["textEmbeddedImages"][novelImageId]["urls"]["original"]
            imgB64 = base64.b64encode(Resources.Pixiv.uploadedImage(url)).decode("utf-8")
            return f"""<figure>
    <a href="{url}">
    <img src=\"data:image/png;base64,{imgB64}\" alt=\"[uploadedimage:{novelImageId}]\" style=\"width: 100%\">
    </a>
    </figure>"""
        o_content = re.sub(r"\[uploadedimage:([0-9]*)(.*?)\]", lambda m: getUploadedImageTag(m.group(1)), o_content)

        # colorize character names
        if color:
            o_content = CharaColor.colorHTML(o_content)

        tags = [(y, mkurl('search', q=y)) for y in [x["tag"] for x in data["tags"]["tags"]]]

        # embed json
        # e.g. <div data='{"x":10,"y":"„ÅÇ"}'></div>
        # jso1 = json.dumps(data, ensure_ascii=False, separators=(',', ':'))
        # for (fromStr, toStr) in [ # unnecessary if b64 is used
        #         ("'", "&#39;"), # ("\"", "&quot;"), ("<", "&lt;"), (">", "&gt;")
        #         ]:
        #     jso1 = jso1.replace(fromStr, toStr)
        # jso2 = gzip.compress(jso1.encode("utf-8"))
        # jso3 = base64.b64encode(jso2).decode("utf-8")

        d = viewNovelData(
            site  = "pixiv",
            title = data["title"],
            id    = data["id"],
            rate  = getRSign(data["xRestrict"]),
            body  = o_content,
            desc  = replaceLinks(data["description"]),
            tags  = tags,
            orig  = f"https://www.pixiv.net/novel/show.php?id={data["id"]}",
            user  = (data["userId"], mkurl('user', id=data["userId"])),
            score = data["bookmarkCount"],
            date  = datetime.datetime.strptime(data["createDate"][:10], "%Y-%m-%d"),
        )

        self._html = viewNovel(d)

        return self._html

    def save(self): # Save to file (will return filename)
        return saveFile(
            self._data["title"],
            self.html(),
            prefix=getRSign(self._data["xRestrict"]),
            suffix=f" - pixiv - {self._data['id']}.html")

    def _extractData(self, html): # parse show.php and get json inside meta[name=preload-data]
        # Extract json string
        s = None
        def onStart(match, attr):
            nonlocal s
            if attr("name") == "preload-data":
                s = attr("content")
        MyHTMLParser(onStart, lambda match, data: 0).feed(html)

        # Extract part of json
        json1 = json.loads(s)
        return json1["novel"][list(json1["novel"].keys())[0]]


### Views

@dataclasses.dataclass
class viewNovelData:
    site:  str
    title: str
    id:    str # content id (used for id= param)
    rate:  Literal["", "R", "G"]
    body:  str
    desc:  str
    tags:  list[tuple[str, str]] # list of (name, link)
    orig:  str
    user:  tuple[str, str] # (name, link)
    score: int
    date:  datetime.datetime

def viewNovel(d:viewNovelData):

    # print(json.dumps(dataclasses.asdict(dataclasses.replace(d, body='')), default=str, ensure_ascii=False, indent=2))

    o_css = """
        body { max-width: 700px; margin: 1em auto; padding: 0 .5em; }
        @media screen and (max-aspect-ratio: .75) and (max-width: 13cm) { /* Mobile */
            body { max-width: 100%; margin: 0.5em 0.5em }
        }
        #novel { line-height: 1.9; border-bottom: solid #888 1px; margin-bottom: 2em; padding-bottom: 3em; }
        #data { display: none }
    """

    o_tags = ",\n".join(f"<a href='{x[1]}'>{x[0]}</a>" for x in d.tags)

    o_rSign = re.sub("^ *", " ", d.rate) if d.rate else ""

    o_info = f"""
        <p> „Çø„Ç∞: {o_tags} </p>
        <p>
            <a href="{d.orig}">{d.site.capitalize()}„ÅßÈñã„Åè</a>
            ID:{d.id}
            U:<a href="{d.user[1]}">{d.user[0]}</a>
            B:{d.score}
            D:{d.date.strftime("%Y-%m-%d")}
        </p>
    """

    o_html = f"""
        <!DOCTYPE html>
        <html lang="ja">
        <head>
            <title>{o_rSign}{d.title}</title>
            <meta http-equiv="content-type" content="text/html; charset=utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <!--<link rel="stylesheet" href="_style.css">-->
            <style>{o_css}</style>
        </head>
        <body>
            <h1>{d.title}</h1>
            <div id="novel"> {d.body} </div>
            <div id="info"> <p> {d.desc} </p> {o_info} </div>
        </body>
        </html>
    """
    o_html = "\n".join(x.strip() for x in o_html.splitlines())
    # <div id="data" data-novels='{o_json}'></div>

    return o_html


### Resources

class Resources:

    # Resource downloading often breaks due to expired cookies, change in request headers etc.
    # So we want unit-testable functions for each resource.

    class Pixiv:

        # 404 without headers
        _headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Pragma': 'no-cache',
            'Referer': 'https://www.pixiv.net/',
            'Cache-Control': 'no-cache'
        }

        # cookie = readCookiestxtAsHTTPCookieHeader("../cookies.txt", "pixiv.net")
        cookie = None

        @classmethod
        def hasCookie(cls):
            return bool(cls.cookie)

        @classmethod
        def cookieHeader(cls):
            return { "cookie": cls.cookie } if cls.hasCookie() else {}

        @classmethod
        def showPhp(cls, novelID):
            url = f"https://www.pixiv.net/novel/show.php?id={novelID}"
            return httpGet(url, headers=[cls._headers, cls.cookieHeader()])

        @classmethod
        def rankingPhp(cls, mode, date: str, page: int):
            # Check modes
            MODES = { "daily", "weekly", "monthly", "rookie", "weekly_original", "male", "female", "daily_r18", "weekly_r18", "male_r18", "female_r18", }
            if not mode in MODES:
                raise Exception(f"Resources.Pixiv.rankingPhp: unknown mode {mode} (expected one of {MODES})")
            if (not cls.hasCookie()) and mode.endswith("r18"):
                raise Exception(f"Resources.Pixiv.rankingPhp: cookie is needed to view ranking of mode {mode}")
            # Check date
            if not (isinstance(date, str) and re.match(r"\d\d\d\d-\d\d-\d\d", date)):
                raise Exception(f"Resources.Pixiv.rankingPhp: invalid date {date} (should be YYYY-MM-DD as a str)")
            # Make URL
            url = f"https://www.pixiv.net/novel/ranking.php?mode={mode}&date={date.replace('-', '')}"
            if page > 1:
                url += f"&page={page}"
            # Download
            return httpGet(url, headers=[cls._headers, (cls.cookieHeader() if mode.endswith("r18") else {})])

        @classmethod
        def jsonUserAll(cls, userID):
            url = f"https://www.pixiv.net/ajax/user/{userID}/profile/all?lang=ja"
            return httpGet(url, fmt="json", headers=[cls._headers, cls.cookieHeader()])

        @classmethod
        def jsonUserNovels(cls, userID, novelIDs):
            n = 100
            if len(novelIDs) <= 0:
                raise Exception(f"Resources.Pixiv.apiUserIds: at least 1 novel IDs are required")
            if len(novelIDs) > n:
                raise Exception(f"Resources.Pixiv.apiUserIds: at most {n} novel IDs are allowed to query at once; got {len(novelIDs)}")
            idsParams = "&".join([f"ids[]={x}" for x in novelIDs])
            url = f"https://www.pixiv.net/ajax/user/{userID}/profile/novels?{idsParams}"
            return httpGet(url, fmt="json", headers=[cls._headers, cls.cookieHeader()])

        @classmethod
        def jsonSearch(cls, word, page):
            params = f"?word={word}&order=date_d&mode=all&p={page}&s_mode=s_tag&lang=ja"
            url = f"https://www.pixiv.net/ajax/search/novels/{word}{params}"
            return httpGet(url, fmt="json", headers=[cls._headers, cls.cookieHeader()])

        @classmethod
        def artworkPagesJson(cls, id):
            url = f"https://www.pixiv.net/ajax/illust/{id}/pages?lang=ja"
            try:
                return httpGet(url, fmt="json", headers=[cls._headers, cls.cookieHeader(), { "Accept": "application/json" }])
            except urllib.error.HTTPError as e:
                if e.code == 404:
                    logging.warning("Resources.Pixiv.artworkPagesJson: Artwork not found" + (", or cookie is required to view this artwork" if not cls.hasCookie() else "") + ":", e)
                else:
                    logging.error("Resources.Pixiv.artworkPagesJson: Unknown error:", e)
                raise e

        @classmethod
        def artworkImage(cls, url):
            headers2 = { "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8" }
            return httpGet(url, fmt="raw", headers=[cls._headers, headers2])

        @classmethod
        def uploadedImage(cls, url):
            # headers2 = { "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8" }
            return httpGet(url, fmt="raw", headers=cls._headers)


### Character name colorizer

class CharaColor:
    _lightness = 0.52
    _saturation = 0.7

    _db0 = {
        # _db0[series][characterName] = color (format: "#xxxxxx" or "#xxx")
        # Idolmaster (https://imas-db.jp/misc/color.html) (imcomplete, and not very strict)
        "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Éü„É™„Ç™„É≥„É©„Ç§„Éñ": {
            "P": "#555555", "Ôº∞": "#555555",
            "Â§©Êµ∑ Êò•È¶ô": "#e22b30", "Â¶ÇÊúà ÂçÉÊó©": "#2743d2", "Ëê©Âéü Èõ™Ê≠©": "#d3dde9", "È´òÊßª „ÇÑ„Çà„ÅÑ": "#f39939", "ÁßãÊúà ÂæãÂ≠ê": "#01a860", "‰∏âÊµ¶ „ÅÇ„Åö„Åï": "#9238be", "Ê∞¥ÁÄ¨ ‰ºäÁπî": "#fd99e1", "ËèäÂú∞ Áúü": "#515558", "ÂèåÊµ∑ ‰∫úÁæé": "#ffe43f", "ÂèåÊµ∑ ÁúüÁæé": "#ffe43f", "Êòü‰∫ï ÁæéÂ∏å": "#b4e04b", "ÊàëÈÇ£Ë¶á Èüø": "#01adb9", "ÂõõÊù° Ë≤¥Èü≥": "#a6126a", "Èü≥ÁÑ° Â∞èÈ≥•": "#00ff00",
            "‰ºäÂêπ Áøº": "#fed552", " „Ç®„Éü„É™„Éº": "#554171", "Â§ßÁ•û Áí∞": "#ee762e", "Êò•Êó• Êú™Êù•": "#ea5b76", "Âåó‰∏ä È∫óËä±": "#6bb6b0", "ÂåóÊ≤¢ Âøó‰øù": "#afa690", "Êú®‰∏ã „Å≤„Å™„Åü": "#d1342c", "È´òÂùÇ Êµ∑Áæé": "#e9739b", "Ê°úÂÆà Ê≠åÁπî": "#274079", "‰ΩêÁ´π ÁæéÂ•àÂ≠ê": "#58a6dc", "ÁØ†ÂÆÆ ÂèØÊÜê": "#b63b40", "Â≥∂Âéü „Ç®„É¨„Éä": "#9bce92", " „Ç∏„É•„É™„Ç¢": "#d7385f", "ÁôΩÁü≥ Á¥¨": "#ebe1ff", "Âë®Èò≤ Ê°ÉÂ≠ê": "#efb864", "È´òÂ±± Á¥ó‰ª£Â≠ê": "#7f6575", "Áî∞‰∏≠ Áê¥Ëëâ": "#92cfbb", "Â§©Á©∫Ê©ã ÊúãËä±": "#bee3e3", "Âæ≥Â∑ù „Åæ„Å§„Çä": "#5abfb7", "ÊâÄ ÊÅµÁæé": "#454341", "Ë±äÂ∑ù È¢®Ëä±": "#7278a8", "‰∏≠Ë∞∑ ËÇ≤": "#f7e78e", "Ê∞∏Âêâ Êò¥": "#aeb49c", "‰∏ÉÂ∞æ ÁôæÂêàÂ≠ê": "#c7b83c", "‰∫åÈöéÂ†Ç ÂçÉÈ∂¥": "#f19557", "Èáé„ÄÖÂéü Ëåú": "#eb613f", "ÁÆ±Â¥é ÊòüÊ¢®Ëä±": "#ed90ba", "È¶¨Â†¥ „Åì„ÅÆ„Åø": "#f1becb", "Á¶èÁî∞ „ÅÆ„ÇäÂ≠ê": "#eceb70", "ËàûÊµú Ê≠©": "#e25a9b", "ÁúüÂ£Å ÁëûÂ∏å": "#99b7dc", "ÊùæÁî∞ ‰∫úÂà©Ê≤ô": "#b54461", "ÂÆÆÂ∞æ Áæé‰πü": "#d7a96b", "ÊúÄ‰∏ä ÈùôÈ¶ô": "#6495cf", "ÊúõÊúà ÊùèÂ•à": "#7e6ca8", "ÁôæÁÄ¨ ËéâÁ∑í": "#f19591", "Áü¢Âêπ ÂèØÂ•à": "#f5ad3b", "Ê®™Â±± Â•àÁ∑í": "#788bc5", " „É≠„Ç≥": "#fff03c",
            "ÈùíÁæΩ ÁæéÂí≤": "#57c7c4",
            " Áé≤Èü≥": "#512aa3", " Ë©©Ëä±": "#e6f9e5", "Â••Á©∫ ÂøÉÁôΩ": "#fefad4",
            "Êó•È´ò ÊÑõ": "#e85786", "Ê∞¥Ë∞∑ ÁµµÁêÜ": "#00adb9", "ÁßãÊúà Ê∂º": "#b2d468",
        },
        "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Ç∑„É≥„Éá„É¨„É©„Ç¨„Éº„É´„Ç∫": {
            "P": "#555555", "Ôº∞": "#555555",
            " „Ç¢„Éä„Çπ„Çø„Ç∑„Ç¢": "#b1c9e8", "ÂèäÂ∑ù Èõ´": "#f4f9ff", "Â§ßÊßª ÂîØ": "#f6be00", "‰πôÂÄâ ÊÇ†Ë≤¥": "#e3bec3", "ÂñúÂ§öË¶ã Êüö": "#f0ec74", "Ê°êÁîü „Å§„Åã„Åï": "#ab4ec6", "Â∞èÊó•Âêë ÁæéÁ©Ç": "#db3fb1", "È´òÊ£Æ ËóçÂ≠ê": "#ceea80", "ÈÅìÊòéÂØ∫ Ê≠åÈà¥": "#d22730", " „Éä„Çø„Éº„É™„Ç¢": "#f5633a", "Èõ£Ê≥¢ Á¨ëÁæé": "#e13c30", "ÊµúÂè£ „ÅÇ„ÇÑ„ÇÅ": "#450099", "Âß´Â∑ù ÂèãÁ¥Ä": "#ee8b00", "Ëó§Âéü ËÇá": "#9595d2", "Êòü ËºùÂ≠ê": "#a6093d", "Êú¨Áî∞ Êú™Â§Æ": "#feb81c", "‰∏âËàπ ÁæéÂÑ™": "#12bfb2", "‰∏âÊùë „Åã„Å™Â≠ê": "#feb1bb", "Â§¢Ë¶ã „Çä„ÅÇ„ÇÄ": "#e59bdc",
            "Áõ∏Ëëâ Â§ïÁæé": "#f1e991", "Ëµ§Âüé „Åø„Çä„ÅÇ": "#ffcd00", "ÊµÖÂà© ‰∏ÉÊµ∑": "#009cbc", "ÂÆâÈÉ® Ëèú„ÄÖ": "#ef4b81", "ËçíÊú® ÊØîÂ•à": "#a0d884", "‰∏Ä„ÉéÁÄ¨ ÂøóÂ∏å": "#a50050", "Á∑íÊñπ Êô∫ÁµµÈáå": "#6cc24a", "ÁâáÊ°ê Êó©Ëãó": "#dc4404", "‰∏äÊù° Êò•Ëèú": "#5ac2e7", "Á•ûË∞∑ Â•àÁ∑í": "#9678d3", "Â∑ùÂ≥∂ ÁëûÊ®π": "#485cc7", "Á•ûÂ¥é Ëò≠Â≠ê": "#84329b", "ÂñúÂ§ö Êó•ËèúÂ≠ê": "#fcd757", "Êú®Êùë Â§èÊ®π": "#2d2926", "ÈªíÂüº „Å°„Å®„Åõ": "#ef3340", "Â∞èÈñ¢ È∫óÂ•à": "#9b26b6", "Â∞èÊó©Â∑ù Á¥óÊûù": "#e56db1", "Ë•øÂúíÂØ∫ Áê¥Ê≠å": "#e8cdd0", "È∑∫Ê≤¢ ÊñáÈ¶ô": "#606eb2", "‰Ωê‰πÖÈñì „Åæ„ÇÜ": "#da1984", "‰Ωê„ÄÖÊú® ÂçÉÊûù": "#0072ce", "‰ΩêÂüé Èõ™Áæé": "#171c8f", "Ê§éÂêç Ê≥ïÂ≠ê": "#f8485e", "Â°©Ë¶ã Âë®Â≠ê": "#dce6ed", "Ê∏ãË∞∑ Âáõ": "#0f9dde", "Â≥∂Êùë ÂçØÊúà": "#f67499", "Âüé„É∂Â¥é ÁæéÂòâ": "#fe9d1a", "Âüé„É∂Â¥é ËéâÂòâ": "#fedd00", "ÁôΩËèä „Åª„Åü„Çã": "#c964cf", "ÁôΩÂùÇ Â∞èÊ¢Ö": "#abcae9", "ÁôΩÈõ™ ÂçÉÂ§ú": "#efd7e5", "Á†ÇÂ°ö „ÅÇ„Åç„Çâ": "#7e93a7", "Èñ¢ Ë£ïÁæé": "#ffb3ab", "È´òÂû£ Ê•ì": "#47d7ac", "È∑πÂØåÂ£´ ËåÑÂ≠ê": "#5c068c", "Â§öÁî∞ ÊùéË°£Ëèú": "#0177c8", "Ê©ò „ÅÇ„Çä„Åô": "#5c88da", "ËæªÈáé „ÅÇ„Åã„Çä": "#e10600", "ÂçóÊù° ÂÖâ": "#e4012b", "Êñ∞Áî∞ ÁæéÊ≥¢": "#71c5e8", "‰∫åÂÆÆ È£õÈ≥•": "#60249f", "Êó©ÂùÇ ÁæéÁé≤": "#c701a0", "ÈÄüÊ∞¥ Â•è": "#033087", "‰πÖÂ∑ù Âá™": "#f8a3bc", "‰πÖÂ∑ù È¢Ø": "#7eddd3", "Êó•Èáé Ëåú": "#fa423a", "Ëó§Êú¨ ÈáåÂ•à": "#623b2a", "ÂèåËëâ Êùè": "#f8a3bc", "ÂåóÊù° Âä†ËìÆ": "#2ad2c9", "Â†Ä Ë£ïÂ≠ê": "#eca154", "ÂâçÂ∑ù „Åø„Åè": "#ce0037", "ÊùæÊ∞∏ Ê∂º": "#221651", "ÁöÑÂ†¥ Ê¢®Ê≤ô": "#e01a95", "ÂÆÆÊú¨ „Éï„É¨„Éá„É™„Ç´": "#a20067", "Âêë‰∫ï ÊãìÊµ∑": "#b0008e", "Ê£üÊñπ ÊÑõÊµ∑": "#c7579a", "Êùë‰∏ä Â∑¥": "#a5192e", "Ê£Æ‰πÖ‰øù ‰πÉ„ÄÖ": "#9cdbd9", "Ë´∏Êòü „Åç„Çâ„Çä": "#ffd100", "ÂÖ´Á•û „Éû„Ç≠„Éé": "#a6a4e0", "Â§ßÂíå ‰∫úÂ≠£": "#28724f", "ÁµêÂüé Êô¥": "#71dad4", "ÈÅä‰Ωê „Åì„Åö„Åà": "#f4a6d7", "‰æùÁî∞ Ëä≥‰πÉ": "#c4bcb7", "ÈæçÂ¥é Ëñ´": "#fae053", "ËÑáÂ±± Áè†Áæé": "#407ec8",
        },
        "„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº„Ç∑„É£„Ç§„Éã„Éº„Ç´„É©„Éº„Ç∫": {
            "P": "#555555", "Ôº∞": "#555555",
            "Ê´ªÊú® Áúü‰πÉ": "#ffbad6", "È¢®Èáé ÁÅØÁπî": "#144384", "ÂÖ´ÂÆÆ „ÇÅ„Åê„Çã": "#ffe012",
            "ÊúàÂ≤° ÊÅãÈêò": "#f84cad", "Áî∞‰∏≠ Êë©Áæé„ÄÖ": "#a846fb", "ÁôΩÁÄ¨ Âí≤ËÄ∂": "#006047", "‰∏âÂ≥∞ ÁµêËèØ": "#3b91c4", "ÂπΩË∞∑ ÈúßÂ≠ê": "#d9f2ff",
            "Â∞èÂÆÆ ÊûúÁ©Ç": "#e5461c", "ÂúíÁî∞ Êô∫‰ª£Â≠ê": "#f93b90", "Ë•øÂüé Ê®πÈáå": "#ffc602", "ÊùúÈáé Âáõ‰∏ñ": "#89c3eb", "ÊúâÊ†ñÂ∑ù Â§èËëâ": "#90e667",
            "Â§ßÂ¥é ÁîòÂ•à": "#f54275", "Â§ßÂ¥é ÁîúËä±": "#e75bec", "Ê°ëÂ±± ÂçÉÈõ™": "#fafafa",
            "ËäπÊ≤¢ „ÅÇ„Åï„Å≤": "#f30100", "Èªõ ÂÜ¨ÂÑ™Â≠ê": "#5aff19", "ÂíåÊ≥â ÊÑõ‰æù": "#ff00ff",
            "ÊµÖÂÄâ ÈÄè": "#50d0d0", "Ê®ãÂè£ ÂÜÜÈ¶ô": "#be1e3e", "Á¶è‰∏∏ Â∞èÁ≥∏": "#7967c3", "Â∏ÇÂ∑ù ÈõõËèú": "#ffc639",
            "‰∏ÉËçâ „Å´„Å°„Åã": "#a6cdb6", "Á∑ãÁî∞ ÁæéÁê¥": "#760f10",
            "ÊñëÈ≥© „É´„Ç´": "#23120c", "‰∏ÉËçâ „ÅØ„Å•„Åç": "#8adfff",
        },
        "„Éñ„É´„Éº„Ç¢„Éº„Ç´„Ç§„Éñ": {
            "ÂÖàÁîü": "#555555",
            "Ê†óÊùë „Ç¢„Ç§„É™": "#4f423e", "È∞êÊ∏ï „Ç¢„Ç´„É™": "#faf1b8", "Â§©Èõ® „Ç¢„Ç≥": "#a9bfdf", "Â§©Á´• „Ç¢„É™„Çπ": "#414a61", "Èô∏ÂÖ´È≠î „Ç¢„É´": "#f9b5ca",
            "ÁôΩÁü≥ „Ç¶„Çø„Éè": "#e9d0f3",
            "ÊùèÂ±± „Ç´„Ç∫„Çµ": "#2a2839",
            "‰∏≠Âãô „Ç≠„É™„Éé": "#fbf8f4",
            "Á©∫‰∫ï „Çµ„Ç≠": "#a491ad", "Ê≠å‰Ωè „Çµ„ÇØ„É©„Ç≥": "#f1ece8",
            "Á†ÇÁãº „Ç∑„É≠„Ç≥": "#c5c4c8",
            "Ê°êËó§ „Éä„ÇÆ„Çµ": "#e4dad1", "ÊüöÈ≥• „Éä„ÉÑ": "#fdebe4",
            "ÁîüÂ°© „Éé„Ç¢": "#e8eff3",
            "‰ºäËçâ „Éè„É´„Ç´": "#534665", "ÈªíËàò „Éè„É´„Éä": "#ccc9d5",
            "ÊßåÊ∞∏ „Éí„É®„É™": "#d5f5e9",
            "ÊÑõÊ∏Ö „Éï„Ç¶„Ç´": "#423856",
            "Â∞èÈ≥•ÈÅä „Éõ„Ç∑„Éé": "#fbe0e7",
            "Â∞èÂ°ó „Éû„Ç≠": "#d05a5b", "‰ºäËêΩ „Éû„É™„Éº": "#fcd9a3",
            "ËÅñÂúí „Éü„Ç´": "#ffebf3", "ÊâçÁæΩ „Éü„Éâ„É™": "#fdd8a3", "ËøëË°õ „Éü„Éä": "#bfc6c4", "ËíºÊ£Æ „Éü„Éç": "#c7ddf7", "ÊúàÈõ™ „Éü„É§„Ç≥": "#f1f5f9", "ÈúûÊ≤¢ „Éü„É¶": "#787285",
            "ÊµÖÈªÑ „É†„ÉÑ„Ç≠": "#f0eeed",
            "È¢®ÂÄâ „É¢„Ç®": "#beadab", "ÊâçÁæΩ „É¢„É¢„Ç§": "#fddca6",
            "Êó©ÁÄ¨ „É¶„Ç¶„Ç´": "#615b96", "Ëä±Â≤° „É¶„Ç∫": "#fc7c82",
            "ÂÆáÊ≤¢ „É¨„Ç§„Çµ": "#fef3fb",
        }
    }

    _db = {} # same as _db0, but with better saturation and lightness, and uses rgb() format. also last name and name with spaces stripped as keys
    for series in _db0:
        _db[series] = {}
        for name in _db0[series]:
            color1   = _db0[series][name]
            match    = re.match(r"#(..)(..)(..)", re.sub(r"#(.)(.)(.)$", r"\1\1\2\2\3\3", color1))
            h,l,s    = colorsys.rgb_to_hls(*[int(x, 16) / 256 for x in match.groups()])
            r,g,b    = colorsys.hls_to_rgb(*[h, _lightness, _saturation] if s > 0.01 else [h, l, s])
            color2   = "rgb(%s)" % ",".join([str(int(x * 256)) for x in [r, g, b]])
            lastname = re.search(r"\S*$", name).group(0)
            nospname = re.sub(r"\s", "", name)
            _db[series][lastname] = color2
            _db[series][nospname] = color2

    @classmethod
    def colorHTML(self, html):
        # color character names starting a serifu (e.g. Â§™ÈÉé in "Â§™ÈÉé„Äå„Åì„Çì„Å´„Å°„ÅØ„Äç")

        # regex: 1 = line beginning, 2 = name, 3 = open paren etc.
        #         (1   )(2  )(3                   )
        regex = r"(^\s*)(.*?)([^\S\r\n]*[(Ôºà„Äå„ÄéÔΩ¢])" # ) dummy parent to fix indent

        # Find what series is this html (different serieses may have same name charas with different colors)
        # list of characters found in html (with multiplicity, excluding bad patterns)
        def isCharaName(s): return len(s) > 0 and (not s.startswith("‚Äï"))
        charaList = [x[1] for x in re.findall(regex, html, flags=re.MULTILINE) if isCharaName(x[1])]
        # get series with most matching names in charaList
        series = max(self._db.keys(), key = lambda series: len([s for s in charaList if s in self._db[series].keys()]))

        # If at most 1/2 of charaList will get colored, it's likely that series is incorrect
        # We don't have a db for the correct series
        if 0.5 * len(charaList) > len([s for s in charaList if s in self._db[series].keys()]):
            return html

        # Wrap with <span>
        def nosp(s): return s.replace(" ", "")
        def decorHTML(name):
            if not nosp(name) in self._db[series]: return name
            return f"<span class='name name_{nosp(name)}'>{name}</span>"

        # CSS
        style = "<style>\n.name { font-weight: bold }\n" + "\n".join([".name_%s { color: %s; }" % (nosp(name), color) for (name, color) in self._db[series].items() if name in charaList]) + "\n</style>\n"

        # HTML (modified)
        html = re.sub(regex, lambda m: m.group(1) + decorHTML(m.group(2)) + m.group(3), html, flags=re.MULTILINE)

        return style + html


### Misc mini functions

class HttpGet:

    def __init__(self):
        self.times = {}

    def tryDecode(self, data):
        for enc in ["utf-8", "shift-jis", "euc-jp", "cp932"]:
            try:
                return data.decode(enc)
            except UnicodeDecodeError:
                pass
        assert False, "Could not decode data"

    def __call__(self, url, fmt="str", headers={}):
        # fmt: "str" (default), "json", "bytes"
        # headers: dict or list of dicts (later element har priority)

        # %-encode chars not allowed in URI, see https://en.wikipedia.org/wiki/Percent-encoding
        regex = r"""[^][!"#$&'()*+,/:;=?@A-Za-z0-9_.~-]+"""
        for m in re.findall(regex, url):
            url = url.replace(m, urllib.parse.quote(m))

        # Rate limiting per domain:port
        loc = urllib.parse.urlparse(url).netloc
        y = max(self.times.setdefault(loc, 0), (x := time.time())) # send request at y
        self.times[loc] = y + 0.5 + min(3, y - x) # exponential wait time, at most 3.5 between requests
        # Sleep if needed
        if y - x > 0.1:
            logging.info(f"HttpGet: requests to the same domain {loc} in a short period, sleeping for {y-x}")
            time.sleep(y - x)

        # Merge headers
        if isinstance(headers, list):
            headers = dict(sum((list(x.items()) for x in headers), []))

        # Actually send request and catch error
        req = urllib.request.Request(url, headers=headers)
        try:
            res = urllib.request.urlopen(req)
        except (urllib.error.HTTPError, urllib.error.URLError, Exception) as e:
            raise e

        if res.status != 200:
            raise Exception(f"http non-200: {res.status}")

        # Decompress, decode, and optionally parse json (and html?)
        data = res.read()
        if "gzip" == ("Content-Encoding" in res.headers and res.headers["Content-Encoding"]):
            data = gzip.decompress(data) # may miss deflate and brotli
        if fmt == "bytes":
            return data
        data = self.tryDecode(data)
        if fmt == "json":
            return json.loads(data)
        elif fmt == "str":
            return data
        else:
            assert False, f"Invalid fmt: {fmt}"

httpGet = HttpGet()

def withFileCache(name, getDefault, expiry=600):
    # note: when cache is expired and getDefault fails, returns old cache
    # expiry is in seconds
    # getDefault should return json-serializable data
    if not cachedir:
        return getDefault()
    if not re.match(r"^[a-zA-Z0-9-._]*$", name):
        raise Exception("Invalid cache name", name)
    if not os.path.isdir(cachedir):
        os.makedirs(cachedir, exist_ok=True)
    file = cachedir + os.sep + name
    def updateCache():
        value = getDefault()
        with open(file, "w") as f:
            json.dump(value, f, ensure_ascii=False, separators=(",", ":"))
        return value
    def readCache():
        with open(file, "r") as f:
            return json.load(f)
    try:
        if datetime.datetime.now().timestamp() - os.stat(file).st_mtime > expiry:
            try:
                value = updateCache()
                logging.debug("cache updating " + name)
            except:
                value = readCache()
                logging.debug("cache failed updating, using old " + name)
        else:
            value = readCache()
            logging.debug("cache is used " + name)
    except FileNotFoundError:
        value = updateCache()
        logging.debug("cache new item " + name)
    return value

def replaceLinks(desc, addTag=False): # replace novel/xxxxx links and user/xxxxx links
    f1 = lambda m: mkurl("user", id=m[1])
    f2 = lambda m: mkurl("novel", id=m[1])
    g1 = lambda m: f'<a href="{mkurl("user", id=m[1])}">user/{m[1]}</a>'
    g2 = lambda m: f'<a href="{mkurl("novel", id=m[1])}">novel/{m[1]}</a>'
    for (regex, rep) in [
        (r"https://www.pixiv.net/users/([0-9]*)",              g1 if addTag else f1),
        (r"https://www.pixiv.net/novel/show.php\?id=([0-9]*)", g2 if addTag else f2),
    ]:
        desc = re.sub(regex, rep, desc)
    return desc

def getRSign(xRestrict):
    return ["", "R ", "G "][int(xRestrict)]

def percentEncode(word):
    return urllib.parse.quote_plus(word, encoding="utf-8")

def mkurl(*args, **kwargs):
    return "/" + "/".join(percentEncode(str(v)) for v in args if v) + ("?" if kwargs else "") + "&".join(f"{k}={percentEncode(str(v))}" for k, v in kwargs.items() if v)

def addMissingCloseTags(html, tags=["b", "s", "u", "strong"]):

    # "aaa<b"  ==>  "aaa"
    m = re.search("<[^>]*$", html)
    if m:
        html = html[:m.start(0)]

    # Count opened counts for each tag in `tags`
    counts = {}
    for m in re.finditer(r"<\s*(/?)\s*(" + "|".join(tags) + r")\s*>", html):
        t = m.group(2)
        if m.group(1) == "/":
            counts[t] = (counts[t] - 1) if t in counts else -1
        else:
            counts[t] = (counts[t] + 1) if t in counts else 1

    # For each tag, if opened count > closed count then add close tags
    for t in counts:
        if counts[t] > 0:
            html += ("</" + t + ">") * counts[t]

    return html

def readCookiestxtAsHTTPCookieHeader(cookiestxt, domain):
    # Read Netscape HTTP Cookie File and return string for urllib request header
    #   urllib.request.Request(url, headers={"Cookie": ...})
    # Only matching domains will be extracted (if domain=="a.com" then www.a.com, .a.com etc will match)

    try:
        with open(cookiestxt) as f:
            results = []

            for line in f:
                # Skip empty or comment lines
                if re.match(r"^\s*$", line) or re.match(r"^# ", line):
                    continue
                fields = line[:-1].split('\t')
                # Each line must have 7 fields and has the specified
                if len(fields) == 7 and (not domain or fields[0].find(domain) != -1):
                    results += [ f"{fields[5]}={fields[6]}" ]

            logging.info(f"Loaded {len(results)} cookies for {domain} from {cookiestxt}")
            return "; ".join(results) # something like "name=val; name=val"

    except OSError as e:
        logging.warning("Could not read cookies.txt; R-18 search results will be omitted!")
        return False

def openInBrowser(url):
    if shutil.which("termux-open-url"):
        subprocess.run(["termux-open-url", url])
    else:
        webbrowser.open(url)

def yesterday():
    return datetime.date.today() - datetime.timedelta(days = 1)

def saveFile(name, text, maxLenBytes=os.pathconf('/', 'PC_NAME_MAX'), prefix="", suffix=""):
    def fsSafeChars(s):
        return re.sub(r'[\\/*<>|]', " ", s).replace('"', "‚Äù").replace(":", "Ôºö").replace("?", "Ôºü")
    def trunc(s, lenBytes, suffix=""):
        # Tuncate s so that (s+suffix).encode("utf-8") has at most lenBytes bytes, and return s+suffix
        # Will not chop in the middle of byte-sequence representing single unicode character
        lenS       = len(s)
        # lenSU      = len(s.encode("utf-8"))
        lenSuffixU = len(suffix.encode("utf-8"))
        i          = lenS
        while len(s[:i].encode("utf-8")) + lenSuffixU > lenBytes:
            i -= 1
        return s[:i] + suffix
    outfile = trunc(fsSafeChars(prefix + name), maxLenBytes, fsSafeChars(suffix))
    if not os.path.isdir(savedir): os.makedirs(savedir, exist_ok=True)
    with open(savedir + os.sep + outfile, "w") as f: f.write(text)
    return outfile


### test

def test():
    import subprocess as sp, time, urllib.error as ue, urllib.parse as up, urllib.request as ur
    port = 8001
    proc = sp.Popen(["python", "pixiv-novel.py", "-p", str(port), "-c", ""])

    def test(path): # test http status and response length
        time.sleep(1)
        try:
            res = ur.urlopen(f"http://localhost:{port}{path}")
        except ue.HTTPError as e:
            return print(f"FAIL {path}   ", e)
        data = res.read()
        if len(data) < 1000:
            return print(f"FAIL {path}   ", "response too short", len(data))
        print(f"OK   {path}")

    try:
        test("/")
        test("/novel?id=15898879")
        test("/search?q=" + up.quote("Ëëó‰ΩúÊ®©„Éï„É™„Éº"))
    finally:
        proc.kill()


### Main

def main():

    global cachedir, color, savedir

    ## Parse args
    parser = argparse.ArgumentParser(description="Start web server to view pixiv novels.")
    A = parser.add_argument
    D1 = " (default: %(default)s)"
    D2 = " (default: %(default)s; set '' to disable)"
    A("-b", "--bind",     metavar="ADDR", default="0.0.0.0", help="Bind to this address" + D1)
    A("-c", "--cachedir", metavar="DIR", default="_cache",   help="Cache directory" + D2)
    A("-d", "--download", metavar="URL",                     help="Download a novel and exit")
    A("-k", "--cookie",   default="cookies.txt",             help="Path of cookies.txt" + D2)
    A("-p", "--port",     type=int, default=8080,            help="Port number" + D1)
    A("-s", "--savedir",  metavar="DIR", default="_save",    help="Auto save novels in this directory" + D2)
    A("-v", "--verbose",  action="store_true",               help="Verbose mode")
    A("--browser", action="store_true", help="Open in browser")
    A("--nocolor", action="store_true", help="Disable character name colors")
    A("--sslcert", help="HTTPS cert file")
    A("--sslkey",  help="HTTPS key file")
    A("--test",    action="store_true")
    # parser.add_argument("--nor18", action="store_true", help="Disable R18.")
    args = parser.parse_args()

    # Save some options on global
    cachedir = args.cachedir
    color    = not args.nocolor
    savedir  = args.savedir

    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(name)s:%(thread)d:%(message)s', level=logging.DEBUG if args.verbose else logging.ERROR)

    # Readme
    # - Check out: cool reader (android app)
    # - cookies.txt
    # - how to visit server

    # Download novel and exit.
    if args.download:
        novelID = re.match(r"[0-9]*", args.download).group()
        if not novelID:
            logging.error("Invalid url")
        else:
            f = Fetch(novelID)
            outfile = f.save()
            print("Written to " + outfile, flush=True)
        exit()

    # Try read cookies.txt
    # To obtain cookies.txt, use https://addons.mozilla.org/ja/firefox/addon/cookies-txt/ or https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid or type document.cookie in devtool
    if args.cookie:
        Resources.Pixiv.cookie = readCookiestxtAsHTTPCookieHeader(args.cookie, "pixiv.net")

    # Test code
    if args.test:
        test()
        quit()

    # Check HTTPS support
    if args.sslcert and args.sslkey:
        serverHttps = True
        serverCert  = args.sslcert
        serverKey   = args.sslkey
    elif args.sslcert or args.sslkey:
        raise Exception("Specify both --sslcert and --sslkey for HTTPS support.")
    else:
        serverHttps = serverCert = serverKey = None

    # Run server
    serverHost = args.bind
    serverPort = args.port

    serverThread = threading.Thread(
        None,
        target=run_threaded_https_server,
        args=(MyRequestHandler, serverHost, serverPort, serverHttps, serverCert, serverKey),
        daemon=True)
    serverThread.start()

    serverUrl = f"http{'s' if serverHttps else ''}://{serverHost}:{serverPort}"
    print(f"Serving at {serverUrl}", flush=True)

    # Open in browser
    if args.browser:
        openInBrowser(serverUrl)

    try:
        serverThread.join()
    except KeyboardInterrupt:
        pass

if __name__ == "__main__":
    main()
